{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43638, 23)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filepath = \"/Users/eshwarchandrasekharan/Desktop/repo/predict-pop/code/\"\n",
    "post_type = 'link'\n",
    "links_meta = \"link_metadata_2016_2017.csv\"\n",
    "links_1hr_stats = \"link_1_hour_stats_2016_2017.csv\"\n",
    "links_48hr_stats = \"link_48_hour_stats_2016_2017.csv\"\n",
    "\n",
    "# df = pd.read_csv(\"../code/\" + post_type + '_metadata_06_2016.csv')\n",
    "meta_df = pd.read_csv(filepath + links_meta)\n",
    "stats_df = pd.read_csv(filepath + links_1hr_stats)\n",
    "stats_df = stats_df.drop('created_at', 1)\n",
    "\n",
    "train = pd.merge(meta_df, stats_df, on = ['external_id'], how = 'right')\n",
    "\n",
    "###get 2 day stats\n",
    "links_48hr_stats = \"link_48_hour_stats_2016_2017.csv\"\n",
    "day2_stats_df = pd.read_csv(filepath + links_48hr_stats)\n",
    "day2_stats_df = day2_stats_df[['external_id', 'stats_date', 'consumptions_by_type__link_clicks', 'stories_by_action_type__share']]\n",
    "new_cols = ['external_id', 'twoday_stats_date', 'twoday_consumptions_by_type__link_clicks', 'twoday_stories_by_action_type__share']\n",
    "day2_stats_df.columns = new_cols\n",
    "train = pd.merge(train, day2_stats_df, on = ['external_id'], how = 'right')\n",
    "\n",
    "###\n",
    "temp = train\n",
    "train_grouped = train.groupby(['buzz_id']).agg({'created_at':'min'})\n",
    "train_grouped = train_grouped.reset_index()\n",
    "train_grouped = train_grouped.rename(columns={'created_at':'first_post'})\n",
    "df = pd.merge(train, train_grouped, how='left', on=['buzz_id'])\n",
    "df = df[df['created_at'] == df['first_post']]\n",
    "train = temp\n",
    "###\n",
    "df = df.drop('parent_external_id', 1)\n",
    "df = df.dropna()\n",
    "#get 48 hour stats\n",
    "\n",
    "df = df.sort_values('stats_date', ascending = False).drop_duplicates(subset=['external_id'], keep = 'last')\n",
    "df = df.sort_values('twoday_stats_date', ascending = False).drop_duplicates(subset=['external_id'], keep = 'last')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((207497, 8), (161352, 13), (43638, 23))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv('radshift_links_2016_2017.csv', index = None)\n",
    "meta_df.shape, stats_df.shape, df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('radshift_links_2016_2017.csv')\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((207497,), (207497,))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_df['external_id'].shape, meta_df['external_id'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['external_id', 'created_at', 'page', 'post_type', 'buzz_id', 'uri',\n",
       "       'title', 'stats_ts', 'stats_date', 'consumptions_by_type__link_clicks',\n",
       "       ...\n",
       "       'Buzzed Feed', 'The Try Guys', 'BuzzFeed San Francisco',\n",
       "       'BuzzFeed Sweaty', 'Proper Tasty', 'BuzzFeed BFF', 'BuzzFeed College',\n",
       "       'BuzzReads', 'BuzzFeed Football', 'BuzzFeed Eugene'],\n",
       "      dtype='object', length=107)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###get cross-posting info for videos!!!! \n",
    "import math \n",
    "\n",
    "filepath = \"/Users/eshwarchandrasekharan/Desktop/repo/predict-pop/code/\"\n",
    "# train = pd.read_csv('../code/jan_may_2017_links_cross_posting_info.csv')\n",
    "train = pd.read_csv('radshift_links_2016_2017.csv')\n",
    "\n",
    "df = train.sort_values('created_at', ascending = False).drop_duplicates(subset=['buzz_id', 'external_id'], keep = 'last')\n",
    "\n",
    "all_pages = list(df.page.unique())\n",
    "print(\"Total pages considered: \", len(all_pages))\n",
    "\n",
    "def get_cross_posted_pages(x):\n",
    "        t = df[df.buzz_id == x].sort_values('created_at', ascending = 1)\n",
    "        return t.page.unique()\n",
    "\n",
    "df['cross_promotion_pages'] = df['buzz_id'].apply(get_cross_posted_pages)\n",
    "\n",
    "def generate_cat_pages(x, y):\n",
    "        if(y in x):\n",
    "            return list(x).index(y) + 1\n",
    "        else:\n",
    "            return math.inf\n",
    "\n",
    "###categorical features for pages where the link was shared\n",
    "for pag in all_pages:\n",
    "#     print(\"Getting for \", pag)\n",
    "    df[pag] = df['cross_promotion_pages'].apply(generate_cat_pages, y = pag)\n",
    "\n",
    "df.to_csv('radshift_links_cross_posting_info.csv', index = False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###get the previous page the video was posted on!\n",
    "train = df\n",
    "\n",
    "def get_previous(X):\n",
    "#     print(\"Start\")\n",
    "    buzzid = train[train['external_id'] == X]['buzz_id'].values[0]\n",
    "    cross_posts = list(train[train['buzz_id'] == buzzid].sort_values(['created_at'], ascending = True)['external_id']) \n",
    "    print(X, \":\", buzzid, \":\", len(cross_posts))\n",
    "#     print(cross_posts)\n",
    "    index = cross_posts.index(X)\n",
    "    if index == 0:\n",
    "        return -1\n",
    "#         return \"-1\"\n",
    "    else:\n",
    "        return cross_posts[index - 1]\n",
    "\n",
    "df['previous_external_id'] = df['external_id'].apply(get_previous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>external_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>page</th>\n",
       "      <th>post_type</th>\n",
       "      <th>buzz_id</th>\n",
       "      <th>uri</th>\n",
       "      <th>title</th>\n",
       "      <th>stats_ts</th>\n",
       "      <th>stats_date</th>\n",
       "      <th>consumptions_by_type__link_clicks</th>\n",
       "      <th>...</th>\n",
       "      <th>The Try Guys</th>\n",
       "      <th>BuzzFeed San Francisco</th>\n",
       "      <th>BuzzFeed Sweaty</th>\n",
       "      <th>Proper Tasty</th>\n",
       "      <th>BuzzFeed BFF</th>\n",
       "      <th>BuzzFeed College</th>\n",
       "      <th>BuzzReads</th>\n",
       "      <th>BuzzFeed Football</th>\n",
       "      <th>BuzzFeed Eugene</th>\n",
       "      <th>previous_external_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [external_id, created_at, page, post_type, buzz_id, uri, title, stats_ts, stats_date, consumptions_by_type__link_clicks, consumptions_by_type__other_clicks, reactions_like_total, stories_by_action_type__comment, stories_by_action_type__like, stories_by_action_type__share, engaged_fan, fan_reach, impressions, impressions_fan, twoday_stats_date, twoday_consumptions_by_type__link_clicks, twoday_stories_by_action_type__share, first_post, cross_promotion_pages, LOLA, BuzzFeed Australia, BuzzFeed India, BuzzFeed España, BuzzFeed DIY, BuzzFeed Deutschland, BuzzFeed Brasil, BuzzFeed UK News, BuzzFeed Video, BuzzFeed Oz News, BuzzFeed Oz Politics, BuzzFeed, BuzzFeed Japan News, BuzzFeed Celeb, BuzzFeed Style, BuzzFeed IRL, BuzzFeed Quiz, BuzzFeed News, Obsessed by BuzzFeed, BuzzFeed World, BuzzFeed Animals, BuzzFeed Community, BuzzFeed México, Buy Me That, SOML, BuzzFeed News BR, BuzzFeed Books, BuzzFeed Food, BuzzFeed Geeky, BuzzFeed Parents, BuzzFeed LGBT, BuzzFeed France, BuzzFeed UK, BuzzFeed Weddings, BuzzFeed Canada, BuzzFeed Japan, BuzzFeed Unsolved, BuzzFeed Español, BuzzFeed Entertainment, BuzzFeed Philippines, BuzzFeed Scotland, BuzzFeed Rewind, BuzzFeed Health, Quizzes En Español, Cheeky, Cocoa Butter, BuzzFeed Ladylike, BuzzFeed Partner, BuzzFeed UK Politics, BuzzFeed Politics, BuzzFeed Science, Reasons to Smile, Tasty Demais, Tasty Miam, BuzzFeed France News, Bien Tasty, BuzzFeed Tech, Pero Like, Tasty, Einfach Tasty, BuzzFeed Pink, BuzzFeed Reader, Another Round, Nifty, Tasty Japan, Kristin Chirico, Adam Ellis, Top Knot, See Something Say Something, Tasty One-Pot, Oh Great, More Politics, BuzzFeed Steven L., Nifty Science, Buzzed Feed, The Try Guys, BuzzFeed San Francisco, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 108 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['BuzzFeed'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###get cross-posting info and previous-next page stats!!!! \n",
    "import math \n",
    "\n",
    "filepath = \"/Users/eshwarchandrasekharan/Desktop/repo/predict-pop/code/\"\n",
    "# train = pd.read_csv('../code/jan_may_2017_links_cross_posting_info.csv')\n",
    "train = pd.read_csv('radshift_links_2016_2017.csv')\n",
    "\n",
    "df = train.sort_values('created_at', ascending = False).drop_duplicates(subset=['buzz_id', 'external_id'], keep = 'last')\n",
    "\n",
    "all_pages = list(df.page.unique())\n",
    "print(\"Total pages considered: \", len(all_pages))\n",
    "\n",
    "def get_cross_posted_pages(x):\n",
    "        t = df[df.buzz_id == x].sort_values('created_at', ascending = 1)\n",
    "        return t.page.unique()\n",
    "\n",
    "df['cross_promotion_pages'] = df['buzz_id'].apply(get_cross_posted_pages)\n",
    "\n",
    "def generate_cat_pages(x, y):\n",
    "        if(y in x):\n",
    "            return list(x).index(y) + 1\n",
    "        else:\n",
    "            return math.inf\n",
    "\n",
    "###categorical features for pages where the link was shared\n",
    "for pag in all_pages:\n",
    "#     print(\"Getting for \", pag)\n",
    "    df[pag] = df['cross_promotion_pages'].apply(generate_cat_pages, y = pag)\n",
    "\n",
    "df.to_csv('radshift_links_cross_posting_info.csv', index = False)\n",
    "df\n",
    "##################################\n",
    "###get the previous page the video was posted on!\n",
    "train = pd.read_csv('radshift_links_cross_posting_info.csv')\n",
    "\n",
    "def get_previous(X):\n",
    "#     print(\"Start\")\n",
    "    buzzid = train[train['external_id'] == X]['buzz_id'].values[0]\n",
    "    cross_posts = list(train[train['buzz_id'] == buzzid].sort_values(['created_at'], ascending = True)['external_id']) \n",
    "    print(X, \":\", buzzid, \":\", len(cross_posts))\n",
    "#     print(cross_posts)\n",
    "    index = cross_posts.index(X)\n",
    "    if index == 0:\n",
    "        return -1\n",
    "#         return \"-1\"\n",
    "    else:\n",
    "        return cross_posts[index - 1]\n",
    "\n",
    "df['previous_external_id'] = df['external_id'].apply(get_previous)\n",
    "df.to_csv('radshift_links_previous_page_info.csv', index = False)\n",
    "\n",
    "##################################\n",
    "\n",
    "all_df = pd.read_csv('radshift_links_cross_posting_info.csv')\n",
    "all_pages = all_df['page'].unique()\n",
    "\n",
    "alt_inf = len(all_pages) + 1\n",
    "all_df = all_df.replace(math.inf, alt_inf)\n",
    "\n",
    "pre_df = pd.read_csv('radshift_links_previous_page_info.csv')\n",
    "\n",
    "pre_df = pre_df[~(pre_df['previous_external_id'] == \"-1\")]\n",
    "pre_df = pre_df[['external_id','previous_external_id']]\n",
    "pre_df.columns = ['next_external_id', 'external_id']\n",
    "pre_df = pd.merge(pre_df, all_df, on=\"external_id\", how = \"inner\")\n",
    "\n",
    "features = [\n",
    "       'next_external_id', \n",
    "       'external_id', \n",
    "       'page',\n",
    "       'title',\n",
    "        'buzz_id',\n",
    "        'created_at',\n",
    "       'stats_date',\n",
    "       'consumptions_by_type__link_clicks',\n",
    "       'consumptions_by_type__other_clicks',\n",
    "       'reactions_like_total',\n",
    "       'stories_by_action_type__comment',\n",
    "       'stories_by_action_type__like',\n",
    "       'stories_by_action_type__share', \n",
    "        'engaged_fan',\n",
    "       'fan_reach', \n",
    "    'impressions',\n",
    "       'impressions_fan',\n",
    "        'twoday_stats_date',\n",
    "       'twoday_consumptions_by_type__link_clicks',\n",
    "       'twoday_stories_by_action_type__share',\n",
    "        ]\n",
    "\n",
    "for pg in all_pages:\n",
    "    features.append(pg)\n",
    "\n",
    "pre_df = pre_df[features]\n",
    "\n",
    "post_df = all_df[['external_id', 'page', 'consumptions_by_type__link_clicks', 'stories_by_action_type__share']]\n",
    "post_df.columns = ['next_external_id', 'next_page', 'next_clicks', 'next_shares']\n",
    "\n",
    "train_df = pd.merge(pre_df, post_df, on=\"next_external_id\", how = \"inner\")\n",
    "\n",
    "train_df.to_csv('radshift-links-previous-to-next-page-info.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###The code in the cell above is what you need!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource 'corpora/words' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/Users/eshwarchandrasekharan/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/Users/eshwarchandrasekharan/anaconda3/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/eshwarchandrasekharan/anaconda3/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/words.zip/words/' not found.  Please use the\n  NLTK Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/Users/eshwarchandrasekharan/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-448f62e071c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# string = \"It's Been 10 Years Since \\\"The Mysterious Ticking Noise\\\" And We're All Closer To Death\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0meng_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/eshwarchandrasekharan/anaconda3/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/eshwarchandrasekharan/anaconda3/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/eshwarchandrasekharan/anaconda3/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/eshwarchandrasekharan/anaconda3/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/words' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/Users/eshwarchandrasekharan/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "ts = \"25 Cosas que SÍ SON REALES sobre los saqueos y las protestas ante el gasolinazo\"\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "\n",
    "string = ts\n",
    "# string = \"21 Trucos para cuando estás tratando de comer más saludable\"\n",
    "# string = \"It's Been 10 Years Since \\\"The Mysterious Ticking Noise\\\" And We're All Closer To Death\"\n",
    "tokens = nltk.word_tokenize(string)\n",
    "eng_dict = words.words()\n",
    "\n",
    "tot = 0\n",
    "hits = 0\n",
    "\n",
    "for word in tokens:\n",
    "    tot += 1\n",
    "    if word in eng_dict:\n",
    "        hits += 1\n",
    "print(hits, tot, hits/tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
