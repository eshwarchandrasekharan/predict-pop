{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eshwarchandrasekharan/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "###predict where the link will be posted given just the title!\n",
    "import pandas as pd\n",
    "from pandas.stats.api import ols\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import math\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "\n",
    "train = pd.read_csv('../code/jan_may_2017_links_cross_posting_info.csv')\n",
    "all_pages = list(train.buzz_account_display_name.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['buzz_external_id', 'buzz_post_parent_external_id',\n",
       "       'buzz_post_created_at', 'buzz_account_display_name',\n",
       "       'buzz_post_type_name', 'buzz_post_buzz_id', 'buzz_campaign_uri',\n",
       "       'buzz_campaign_name', 'ext_table1_stats_date', 'ext_external_id',\n",
       "       ...\n",
       "       'Tasty One-Pot', 'BuzzFeed Sweaty', 'Tasty Junior',\n",
       "       'Oh Great, More Politics', 'Einfach Tasty', 'BuzzFeed Steven L.',\n",
       "       'The Try Guys', 'Ohmygod Yaaa', 'BuzzFeed Chloe', 'Nifty Science'],\n",
       "      dtype='object', length=113)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "starting_features = ['buzz_external_id', 'buzz_account_display_name',\n",
    "       'buzz_post_type_name', 'buzz_post_buzz_id', 'buzz_campaign_uri',\n",
    "       'buzz_campaign_name',]\n",
    "df = train[starting_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###for each buzz_id, get the external ID which was posted FIRST!!!\n",
    "train_grouped = train.groupby(['buzz_post_buzz_id']).agg({'buzz_post_created_at':'min'})\n",
    "\n",
    "train_grouped = train_grouped.reset_index()\n",
    "\n",
    "train_grouped = train_grouped.rename(columns={'buzz_post_created_at':'first_post'})\n",
    "\n",
    "df = pd.merge(train, train_grouped, how='left', on=['buzz_post_buzz_id'])\n",
    "\n",
    "df = df[df['buzz_post_created_at'] == df['first_post']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###predict where the link will be posted given just the title!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page:  BuzzFeed Quiz #links:  1316 Accuracy:  0.875759016016 Precision:  0.898844144799 Recall:  0.875758561026 Fscore: 0.873606419703\n",
      "Page:  BuzzFeed Japan #links:  966 Accuracy:  0.820404892901 Precision:  0.887943532199 Recall:  0.831010073475 Fscore: 0.798715085114\n",
      "Page:  BuzzFeed UK #links:  560 Accuracy:  0.683928571429 Precision:  0.769137887508 Recall:  0.685801124998 Fscore: 0.655485054184\n",
      "Page:  BuzzFeed Brasil #links:  1022 Accuracy:  0.948098995696 Precision:  0.951898128436 Recall:  0.949638947117 Fscore: 0.947957577092\n",
      "Page:  BuzzFeed Japan News #links:  616 Accuracy:  0.769433516916 Precision:  0.863745075664 Recall:  0.78241129329 Fscore: 0.7336541182\n",
      "Page:  BuzzFeed Deutschland #links:  581 Accuracy:  0.975913645741 Precision:  0.97691039408 Recall:  0.976253651664 Fscore: 0.975870332137\n",
      "Page:  BuzzFeed News #links:  649 Accuracy:  0.82282647585 Precision:  0.835122811148 Recall:  0.825185187816 Fscore: 0.821032303328\n",
      "Page:  BuzzFeed #links:  978 Accuracy:  0.640096807954 Precision:  0.720550201522 Recall:  0.643142094842 Fscore: 0.602419016664\n",
      "Page:  BuzzFeed Canada #links:  251 Accuracy:  0.641607843137 Precision:  0.702197138726 Recall:  0.643988946378 Fscore: 0.611533625265\n",
      "Page:  BuzzFeed Video #links:  121 Accuracy:  0.6605 Precision:  0.720943197454 Recall:  0.6616754773 Fscore: 0.632333731969\n",
      "Page:  Cocoa Butter #links:  177 Accuracy:  0.691507936508 Precision:  0.767120347169 Recall:  0.695130288958 Fscore: 0.665743995084\n",
      "Page:  BuzzFeed Oz Politics #links:  212 Accuracy:  0.811240310078 Precision:  0.857294674015 Recall:  0.811898096691 Fscore: 0.799697443303\n",
      "Page:  Quizzes En Español #links:  195 Accuracy:  0.894871794872 Precision:  0.911679061679 Recall:  0.902766803372 Fscore: 0.893404661332\n",
      "Page:  Obsessed by BuzzFeed #links:  292 Accuracy:  0.739918176505 Precision:  0.790540850387 Recall:  0.747486674095 Fscore: 0.728012956045\n",
      "Page:  BuzzFeed News BR #links:  278 Accuracy:  0.9425 Precision:  0.94783935012 Recall:  0.943592942776 Fscore: 0.94199969881\n",
      "Page:  Buy Me That #links:  362 Accuracy:  0.834360730594 Precision:  0.864859800723 Recall:  0.83628496918 Fscore: 0.830677936146\n",
      "Page:  BuzzFeed UK Politics #links:  256 Accuracy:  0.795098039216 Precision:  0.843298645679 Recall:  0.800875780288 Fscore: 0.787187478334\n",
      "Page:  BuzzFeed Australia #links:  718 Accuracy:  0.683275058275 Precision:  0.752921956857 Recall:  0.685013266064 Fscore: 0.659083354897\n",
      "Page:  BuzzFeed Rewind #links:  220 Accuracy:  0.702272727273 Precision:  0.806029713315 Recall:  0.705297370705 Fscore: 0.670086706826\n",
      "Page:  BuzzFeed Books #links:  210 Accuracy:  0.719047619048 Precision:  0.798607622361 Recall:  0.722167812482 Fscore: 0.695981595529\n",
      "Page:  BuzzFeed DIY #links:  260 Accuracy:  0.759615384615 Precision:  0.799198464544 Recall:  0.763393249767 Fscore: 0.748365237685\n",
      "Page:  BuzzFeed México #links:  665 Accuracy:  0.924060150376 Precision:  0.934734609503 Recall:  0.925705948502 Fscore: 0.923639166433\n",
      "Page:  BuzzFeed Food #links:  545 Accuracy:  0.749541284404 Precision:  0.818423105262 Recall:  0.750190868263 Fscore: 0.734099652809\n",
      "Page:  BuzzFeed Celeb #links:  480 Accuracy:  0.740625 Precision:  0.782109206617 Recall:  0.742085873393 Fscore: 0.730397767594\n",
      "Page:  BuzzFeed Animals #links:  268 Accuracy:  0.755520614955 Precision:  0.830276735597 Recall:  0.75741730859 Fscore: 0.738924128644\n",
      "Page:  BuzzFeed France #links:  546 Accuracy:  0.94964970809 Precision:  0.95394839169 Recall:  0.950518988784 Fscore: 0.949174616364\n",
      "Page:  BuzzFeed Español #links:  386 Accuracy:  0.913153513154 Precision:  0.926471159304 Recall:  0.914582253198 Fscore: 0.912385069677\n",
      "Page:  BuzzFeed Style #links:  409 Accuracy:  0.700496838302 Precision:  0.790180751971 Recall:  0.70198447067 Fscore: 0.674760763982\n",
      "Page:  Cheeky #links:  364 Accuracy:  0.791400304414 Precision:  0.825373609415 Recall:  0.797622987439 Fscore: 0.786427845949\n",
      "Page:  BuzzFeed Community #links:  348 Accuracy:  0.715610766046 Precision:  0.774804340212 Recall:  0.719404588014 Fscore: 0.697490256864\n",
      "Page:  BuzzFeed Weddings #links:  112 Accuracy:  0.648814229249 Precision:  0.698213537409 Recall:  0.660596903097 Fscore: 0.622016848264\n",
      "Page:  BuzzFeed World #links:  243 Accuracy:  0.833120748299 Precision:  0.862623139037 Recall:  0.835162303075 Fscore: 0.826842538495\n",
      "Page:  BuzzFeed Entertainment #links:  344 Accuracy:  0.751683716965 Precision:  0.785529421923 Recall:  0.756509943329 Fscore: 0.743615865131\n",
      "Page:  BuzzFeed Geeky #links:  142 Accuracy:  0.676108374384 Precision:  0.758699275362 Recall:  0.680271774794 Fscore: 0.642490510184\n",
      "Page:  BuzzFeed Parents #links:  189 Accuracy:  0.711806543385 Precision:  0.802320693196 Recall:  0.716390124929 Fscore: 0.684083514151\n",
      "Page:  BuzzFeed Health #links:  298 Accuracy:  0.688079096045 Precision:  0.770447279884 Recall:  0.692833449731 Fscore: 0.66043404188\n",
      "Page:  BuzzFeed España #links:  357 Accuracy:  0.880712050078 Precision:  0.904033135161 Recall:  0.882573165736 Fscore: 0.878601153896\n",
      "Page:  BuzzFeed Politics #links:  378 Accuracy:  0.859947368421 Precision:  0.879215887433 Recall:  0.861935861713 Fscore: 0.857941313388\n",
      "Page:  Pero Like #links:  45 Accuracy:  0.588888888889 Precision:  0.65380952381 Recall:  0.600833333333 Fscore: 0.562072649573\n",
      "Page:  BuzzFeed UK News #links:  366 Accuracy:  0.770455386894 Precision:  0.802443901652 Recall:  0.770126906699 Fscore: 0.763048558874\n",
      "Page:  BuzzFeed India #links:  620 Accuracy:  0.741935483871 Precision:  0.812164255654 Recall:  0.744992344418 Fscore: 0.726514101887\n",
      "Page:  BuzzFeed France News #links:  217 Accuracy:  0.940169133192 Precision:  0.949757187257 Recall:  0.943204169019 Fscore: 0.939841706746\n",
      "Page:  BuzzFeed Scotland #links:  96 Accuracy:  0.693421052632 Precision:  0.762446311858 Recall:  0.693560606061 Fscore: 0.667568892382\n",
      "Page:  LOLA #links:  37 Accuracy:  0.8625 Precision:  0.853214285714 Recall:  0.853333333333 Fscore: 0.830079365079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eshwarchandrasekharan/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page:  BuzzFeed Philippines #links:  161 Accuracy:  0.711458333333 Precision:  0.772315763757 Recall:  0.71685665663 Fscore: 0.691592539693\n",
      "Page:  BuzzFeed Oz News #links:  179 Accuracy:  0.799126984127 Precision:  0.837008178164 Recall:  0.806845481946 Fscore: 0.789376760497\n",
      "Page:  BuzzFeed Reader #links:  46 Accuracy:  0.75 Precision:  0.774285714286 Recall:  0.759166666667 Fscore: 0.741789321789\n",
      "Page:  BuzzFeed Pink #links:  25 Accuracy:  0.94 Precision:  0.958333333333 Recall:  0.95 Fscore: 0.938333333333\n",
      "Page:  BuzzFeed LGBT #links:  114 Accuracy:  0.758695652174 Precision:  0.79562983177 Recall:  0.766225441225 Fscore: 0.75102783084\n",
      "Page:  BuzzFeed Partner #links:  89 Accuracy:  0.757189542484 Precision:  0.792532051282 Recall:  0.753564213564 Fscore: 0.74402681157\n",
      "Page:  BuzzFeed Tech #links:  211 Accuracy:  0.810022148394 Precision:  0.83764680098 Recall:  0.818969364829 Fscore: 0.804061083584\n",
      "Page:  BuzzFeed Science #links:  73 Accuracy:  0.660476190476 Precision:  0.683596403596 Recall:  0.668115079365 Fscore: 0.640148119854\n"
     ]
    }
   ],
   "source": [
    "###predict where the link will be posted given just the title!\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "pages = []\n",
    "num_links = []\n",
    "average_acc = []\n",
    "average_pre = []\n",
    "average_rec = []\n",
    "average_f1 = []\n",
    "\n",
    "cv_folds = 10\n",
    "results = pd.DataFrame()\n",
    "page_clf = []\n",
    "\n",
    "for test_page in all_pages:\n",
    "#     test_page = \"BuzzFeed\"\n",
    "    X_1 = df[df.buzz_account_display_name == test_page]\n",
    "    X_0 = df[~(df.buzz_account_display_name == test_page)].sample(n=len(X_1))\n",
    "    if len(X_1) < 2.5*cv_folds:\n",
    "        continue\n",
    "    X_1 = X_1[['buzz_campaign_name', 'buzz_account_display_name']]\n",
    "    X_0 = X_0[['buzz_campaign_name', 'buzz_account_display_name']]\n",
    "    X_all = pd.concat([X_1, X_0])\n",
    "\n",
    "    def get_class(X, page):\n",
    "        if X == page:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    X_all['class'] = X_all['buzz_account_display_name'].apply(get_class, page = test_page)\n",
    "    X_all = X_all.dropna()\n",
    "    text_clf = Pipeline([\n",
    "                         ('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "#                          ('featselect', SelectKBest(k = 100)),\n",
    "                         ('clf', MultinomialNB()),\n",
    "#                          ('clf', LinearSVC()),\n",
    "#                          ('clf', RandomForestClassifier()),\n",
    "#                          ('clf', LogisticRegression()),\n",
    "    ])\n",
    "    fold = 0\n",
    "    kf = KFold(n_splits = cv_folds, shuffle = True)\n",
    "\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    fscore = []\n",
    "\n",
    "    X = X_all['buzz_campaign_name']\n",
    "    y = X_all['class']\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "#         print(\"Fold = \", fold)\n",
    "        fold+= 1\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        text_clf.fit(X_train, y_train)\n",
    "        y_pred = text_clf.predict(X_test)\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        prec, rec, fs, sup = precision_recall_fscore_support(y_test, y_pred)\n",
    "        accuracy.append(acc)\n",
    "        precision.append(prec)\n",
    "        recall.append(rec)\n",
    "        fscore.append(fs)\n",
    "    print(\"Page: \", test_page, \"#links: \", len(X_1), \"Accuracy: \", np.mean(accuracy), \"Precision: \", np.mean(precision), \"Recall: \", np.mean(recall), \"Fscore:\", np.mean(fscore))\n",
    "    num_links.append(len(X_1))\n",
    "    average_acc.append(np.mean(accuracy))\n",
    "    average_pre.append(np.mean(precision))\n",
    "    average_rec.append(np.mean(recall))\n",
    "    average_f1.append(np.mean(fscore))\n",
    "    pages.append(test_page)\n",
    "    page_clf.append(text_clf)\n",
    "results['page'] = pages\n",
    "results['num_links'] = num_links\n",
    "results['accuracy'] = average_acc\n",
    "results['precision'] = average_pre\n",
    "results['recall'] = average_rec\n",
    "results['fscore'] = average_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>num_links</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>fscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BuzzFeed Quiz</td>\n",
       "      <td>1316</td>\n",
       "      <td>0.875759</td>\n",
       "      <td>0.898844</td>\n",
       "      <td>0.875759</td>\n",
       "      <td>0.873606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BuzzFeed Japan</td>\n",
       "      <td>966</td>\n",
       "      <td>0.820405</td>\n",
       "      <td>0.887944</td>\n",
       "      <td>0.831010</td>\n",
       "      <td>0.798715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BuzzFeed UK</td>\n",
       "      <td>560</td>\n",
       "      <td>0.683929</td>\n",
       "      <td>0.769138</td>\n",
       "      <td>0.685801</td>\n",
       "      <td>0.655485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BuzzFeed Brasil</td>\n",
       "      <td>1022</td>\n",
       "      <td>0.948099</td>\n",
       "      <td>0.951898</td>\n",
       "      <td>0.949639</td>\n",
       "      <td>0.947958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BuzzFeed Japan News</td>\n",
       "      <td>616</td>\n",
       "      <td>0.769434</td>\n",
       "      <td>0.863745</td>\n",
       "      <td>0.782411</td>\n",
       "      <td>0.733654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BuzzFeed Deutschland</td>\n",
       "      <td>581</td>\n",
       "      <td>0.975914</td>\n",
       "      <td>0.976910</td>\n",
       "      <td>0.976254</td>\n",
       "      <td>0.975870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BuzzFeed News</td>\n",
       "      <td>649</td>\n",
       "      <td>0.822826</td>\n",
       "      <td>0.835123</td>\n",
       "      <td>0.825185</td>\n",
       "      <td>0.821032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BuzzFeed</td>\n",
       "      <td>978</td>\n",
       "      <td>0.640097</td>\n",
       "      <td>0.720550</td>\n",
       "      <td>0.643142</td>\n",
       "      <td>0.602419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BuzzFeed Canada</td>\n",
       "      <td>251</td>\n",
       "      <td>0.641608</td>\n",
       "      <td>0.702197</td>\n",
       "      <td>0.643989</td>\n",
       "      <td>0.611534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BuzzFeed Video</td>\n",
       "      <td>121</td>\n",
       "      <td>0.660500</td>\n",
       "      <td>0.720943</td>\n",
       "      <td>0.661675</td>\n",
       "      <td>0.632334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Cocoa Butter</td>\n",
       "      <td>177</td>\n",
       "      <td>0.691508</td>\n",
       "      <td>0.767120</td>\n",
       "      <td>0.695130</td>\n",
       "      <td>0.665744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BuzzFeed Oz Politics</td>\n",
       "      <td>212</td>\n",
       "      <td>0.811240</td>\n",
       "      <td>0.857295</td>\n",
       "      <td>0.811898</td>\n",
       "      <td>0.799697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Quizzes En Español</td>\n",
       "      <td>195</td>\n",
       "      <td>0.894872</td>\n",
       "      <td>0.911679</td>\n",
       "      <td>0.902767</td>\n",
       "      <td>0.893405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Obsessed by BuzzFeed</td>\n",
       "      <td>292</td>\n",
       "      <td>0.739918</td>\n",
       "      <td>0.790541</td>\n",
       "      <td>0.747487</td>\n",
       "      <td>0.728013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>BuzzFeed News BR</td>\n",
       "      <td>278</td>\n",
       "      <td>0.942500</td>\n",
       "      <td>0.947839</td>\n",
       "      <td>0.943593</td>\n",
       "      <td>0.942000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Buy Me That</td>\n",
       "      <td>362</td>\n",
       "      <td>0.834361</td>\n",
       "      <td>0.864860</td>\n",
       "      <td>0.836285</td>\n",
       "      <td>0.830678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>BuzzFeed UK Politics</td>\n",
       "      <td>256</td>\n",
       "      <td>0.795098</td>\n",
       "      <td>0.843299</td>\n",
       "      <td>0.800876</td>\n",
       "      <td>0.787187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BuzzFeed Australia</td>\n",
       "      <td>718</td>\n",
       "      <td>0.683275</td>\n",
       "      <td>0.752922</td>\n",
       "      <td>0.685013</td>\n",
       "      <td>0.659083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BuzzFeed Rewind</td>\n",
       "      <td>220</td>\n",
       "      <td>0.702273</td>\n",
       "      <td>0.806030</td>\n",
       "      <td>0.705297</td>\n",
       "      <td>0.670087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>BuzzFeed Books</td>\n",
       "      <td>210</td>\n",
       "      <td>0.719048</td>\n",
       "      <td>0.798608</td>\n",
       "      <td>0.722168</td>\n",
       "      <td>0.695982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>BuzzFeed DIY</td>\n",
       "      <td>260</td>\n",
       "      <td>0.759615</td>\n",
       "      <td>0.799198</td>\n",
       "      <td>0.763393</td>\n",
       "      <td>0.748365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>BuzzFeed México</td>\n",
       "      <td>665</td>\n",
       "      <td>0.924060</td>\n",
       "      <td>0.934735</td>\n",
       "      <td>0.925706</td>\n",
       "      <td>0.923639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>BuzzFeed Food</td>\n",
       "      <td>545</td>\n",
       "      <td>0.749541</td>\n",
       "      <td>0.818423</td>\n",
       "      <td>0.750191</td>\n",
       "      <td>0.734100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>BuzzFeed Celeb</td>\n",
       "      <td>480</td>\n",
       "      <td>0.740625</td>\n",
       "      <td>0.782109</td>\n",
       "      <td>0.742086</td>\n",
       "      <td>0.730398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>BuzzFeed Animals</td>\n",
       "      <td>268</td>\n",
       "      <td>0.755521</td>\n",
       "      <td>0.830277</td>\n",
       "      <td>0.757417</td>\n",
       "      <td>0.738924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>BuzzFeed France</td>\n",
       "      <td>546</td>\n",
       "      <td>0.949650</td>\n",
       "      <td>0.953948</td>\n",
       "      <td>0.950519</td>\n",
       "      <td>0.949175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>BuzzFeed Español</td>\n",
       "      <td>386</td>\n",
       "      <td>0.913154</td>\n",
       "      <td>0.926471</td>\n",
       "      <td>0.914582</td>\n",
       "      <td>0.912385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>BuzzFeed Style</td>\n",
       "      <td>409</td>\n",
       "      <td>0.700497</td>\n",
       "      <td>0.790181</td>\n",
       "      <td>0.701984</td>\n",
       "      <td>0.674761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Cheeky</td>\n",
       "      <td>364</td>\n",
       "      <td>0.791400</td>\n",
       "      <td>0.825374</td>\n",
       "      <td>0.797623</td>\n",
       "      <td>0.786428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>BuzzFeed Community</td>\n",
       "      <td>348</td>\n",
       "      <td>0.715611</td>\n",
       "      <td>0.774804</td>\n",
       "      <td>0.719405</td>\n",
       "      <td>0.697490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>BuzzFeed Weddings</td>\n",
       "      <td>112</td>\n",
       "      <td>0.648814</td>\n",
       "      <td>0.698214</td>\n",
       "      <td>0.660597</td>\n",
       "      <td>0.622017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>BuzzFeed World</td>\n",
       "      <td>243</td>\n",
       "      <td>0.833121</td>\n",
       "      <td>0.862623</td>\n",
       "      <td>0.835162</td>\n",
       "      <td>0.826843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>BuzzFeed Entertainment</td>\n",
       "      <td>344</td>\n",
       "      <td>0.751684</td>\n",
       "      <td>0.785529</td>\n",
       "      <td>0.756510</td>\n",
       "      <td>0.743616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>BuzzFeed Geeky</td>\n",
       "      <td>142</td>\n",
       "      <td>0.676108</td>\n",
       "      <td>0.758699</td>\n",
       "      <td>0.680272</td>\n",
       "      <td>0.642491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>BuzzFeed Parents</td>\n",
       "      <td>189</td>\n",
       "      <td>0.711807</td>\n",
       "      <td>0.802321</td>\n",
       "      <td>0.716390</td>\n",
       "      <td>0.684084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>BuzzFeed Health</td>\n",
       "      <td>298</td>\n",
       "      <td>0.688079</td>\n",
       "      <td>0.770447</td>\n",
       "      <td>0.692833</td>\n",
       "      <td>0.660434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>BuzzFeed España</td>\n",
       "      <td>357</td>\n",
       "      <td>0.880712</td>\n",
       "      <td>0.904033</td>\n",
       "      <td>0.882573</td>\n",
       "      <td>0.878601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>BuzzFeed Politics</td>\n",
       "      <td>378</td>\n",
       "      <td>0.859947</td>\n",
       "      <td>0.879216</td>\n",
       "      <td>0.861936</td>\n",
       "      <td>0.857941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Pero Like</td>\n",
       "      <td>45</td>\n",
       "      <td>0.588889</td>\n",
       "      <td>0.653810</td>\n",
       "      <td>0.600833</td>\n",
       "      <td>0.562073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>BuzzFeed UK News</td>\n",
       "      <td>366</td>\n",
       "      <td>0.770455</td>\n",
       "      <td>0.802444</td>\n",
       "      <td>0.770127</td>\n",
       "      <td>0.763049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>BuzzFeed India</td>\n",
       "      <td>620</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.812164</td>\n",
       "      <td>0.744992</td>\n",
       "      <td>0.726514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>BuzzFeed France News</td>\n",
       "      <td>217</td>\n",
       "      <td>0.940169</td>\n",
       "      <td>0.949757</td>\n",
       "      <td>0.943204</td>\n",
       "      <td>0.939842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>BuzzFeed Scotland</td>\n",
       "      <td>96</td>\n",
       "      <td>0.693421</td>\n",
       "      <td>0.762446</td>\n",
       "      <td>0.693561</td>\n",
       "      <td>0.667569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>LOLA</td>\n",
       "      <td>37</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>0.853214</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>0.830079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>BuzzFeed Philippines</td>\n",
       "      <td>161</td>\n",
       "      <td>0.711458</td>\n",
       "      <td>0.772316</td>\n",
       "      <td>0.716857</td>\n",
       "      <td>0.691593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>BuzzFeed Oz News</td>\n",
       "      <td>179</td>\n",
       "      <td>0.799127</td>\n",
       "      <td>0.837008</td>\n",
       "      <td>0.806845</td>\n",
       "      <td>0.789377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>BuzzFeed Reader</td>\n",
       "      <td>46</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.774286</td>\n",
       "      <td>0.759167</td>\n",
       "      <td>0.741789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>BuzzFeed Pink</td>\n",
       "      <td>25</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.938333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>BuzzFeed LGBT</td>\n",
       "      <td>114</td>\n",
       "      <td>0.758696</td>\n",
       "      <td>0.795630</td>\n",
       "      <td>0.766225</td>\n",
       "      <td>0.751028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>BuzzFeed Partner</td>\n",
       "      <td>89</td>\n",
       "      <td>0.757190</td>\n",
       "      <td>0.792532</td>\n",
       "      <td>0.753564</td>\n",
       "      <td>0.744027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>BuzzFeed Tech</td>\n",
       "      <td>211</td>\n",
       "      <td>0.810022</td>\n",
       "      <td>0.837647</td>\n",
       "      <td>0.818969</td>\n",
       "      <td>0.804061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>BuzzFeed Science</td>\n",
       "      <td>73</td>\n",
       "      <td>0.660476</td>\n",
       "      <td>0.683596</td>\n",
       "      <td>0.668115</td>\n",
       "      <td>0.640148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      page  num_links  accuracy  precision    recall    fscore\n",
       "0            BuzzFeed Quiz       1316  0.875759   0.898844  0.875759  0.873606\n",
       "1           BuzzFeed Japan        966  0.820405   0.887944  0.831010  0.798715\n",
       "2              BuzzFeed UK        560  0.683929   0.769138  0.685801  0.655485\n",
       "3          BuzzFeed Brasil       1022  0.948099   0.951898  0.949639  0.947958\n",
       "4      BuzzFeed Japan News        616  0.769434   0.863745  0.782411  0.733654\n",
       "5     BuzzFeed Deutschland        581  0.975914   0.976910  0.976254  0.975870\n",
       "6            BuzzFeed News        649  0.822826   0.835123  0.825185  0.821032\n",
       "7                 BuzzFeed        978  0.640097   0.720550  0.643142  0.602419\n",
       "8          BuzzFeed Canada        251  0.641608   0.702197  0.643989  0.611534\n",
       "9           BuzzFeed Video        121  0.660500   0.720943  0.661675  0.632334\n",
       "10            Cocoa Butter        177  0.691508   0.767120  0.695130  0.665744\n",
       "11    BuzzFeed Oz Politics        212  0.811240   0.857295  0.811898  0.799697\n",
       "12      Quizzes En Español        195  0.894872   0.911679  0.902767  0.893405\n",
       "13    Obsessed by BuzzFeed        292  0.739918   0.790541  0.747487  0.728013\n",
       "14        BuzzFeed News BR        278  0.942500   0.947839  0.943593  0.942000\n",
       "15             Buy Me That        362  0.834361   0.864860  0.836285  0.830678\n",
       "16    BuzzFeed UK Politics        256  0.795098   0.843299  0.800876  0.787187\n",
       "17      BuzzFeed Australia        718  0.683275   0.752922  0.685013  0.659083\n",
       "18         BuzzFeed Rewind        220  0.702273   0.806030  0.705297  0.670087\n",
       "19          BuzzFeed Books        210  0.719048   0.798608  0.722168  0.695982\n",
       "20            BuzzFeed DIY        260  0.759615   0.799198  0.763393  0.748365\n",
       "21         BuzzFeed México        665  0.924060   0.934735  0.925706  0.923639\n",
       "22           BuzzFeed Food        545  0.749541   0.818423  0.750191  0.734100\n",
       "23          BuzzFeed Celeb        480  0.740625   0.782109  0.742086  0.730398\n",
       "24        BuzzFeed Animals        268  0.755521   0.830277  0.757417  0.738924\n",
       "25         BuzzFeed France        546  0.949650   0.953948  0.950519  0.949175\n",
       "26        BuzzFeed Español        386  0.913154   0.926471  0.914582  0.912385\n",
       "27          BuzzFeed Style        409  0.700497   0.790181  0.701984  0.674761\n",
       "28                  Cheeky        364  0.791400   0.825374  0.797623  0.786428\n",
       "29      BuzzFeed Community        348  0.715611   0.774804  0.719405  0.697490\n",
       "30       BuzzFeed Weddings        112  0.648814   0.698214  0.660597  0.622017\n",
       "31          BuzzFeed World        243  0.833121   0.862623  0.835162  0.826843\n",
       "32  BuzzFeed Entertainment        344  0.751684   0.785529  0.756510  0.743616\n",
       "33          BuzzFeed Geeky        142  0.676108   0.758699  0.680272  0.642491\n",
       "34        BuzzFeed Parents        189  0.711807   0.802321  0.716390  0.684084\n",
       "35         BuzzFeed Health        298  0.688079   0.770447  0.692833  0.660434\n",
       "36         BuzzFeed España        357  0.880712   0.904033  0.882573  0.878601\n",
       "37       BuzzFeed Politics        378  0.859947   0.879216  0.861936  0.857941\n",
       "38               Pero Like         45  0.588889   0.653810  0.600833  0.562073\n",
       "39        BuzzFeed UK News        366  0.770455   0.802444  0.770127  0.763049\n",
       "40          BuzzFeed India        620  0.741935   0.812164  0.744992  0.726514\n",
       "41    BuzzFeed France News        217  0.940169   0.949757  0.943204  0.939842\n",
       "42       BuzzFeed Scotland         96  0.693421   0.762446  0.693561  0.667569\n",
       "43                    LOLA         37  0.862500   0.853214  0.853333  0.830079\n",
       "44    BuzzFeed Philippines        161  0.711458   0.772316  0.716857  0.691593\n",
       "45        BuzzFeed Oz News        179  0.799127   0.837008  0.806845  0.789377\n",
       "46         BuzzFeed Reader         46  0.750000   0.774286  0.759167  0.741789\n",
       "47           BuzzFeed Pink         25  0.940000   0.958333  0.950000  0.938333\n",
       "48           BuzzFeed LGBT        114  0.758696   0.795630  0.766225  0.751028\n",
       "49        BuzzFeed Partner         89  0.757190   0.792532  0.753564  0.744027\n",
       "50           BuzzFeed Tech        211  0.810022   0.837647  0.818969  0.804061\n",
       "51        BuzzFeed Science         73  0.660476   0.683596  0.668115  0.640148"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "num_links    357.557692\n",
       "accuracy       0.778595\n",
       "precision      0.824024\n",
       "recall         0.782314\n",
       "fscore         0.763955\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best first page for item titled:  ['Find Out What your spirit animal is?']\n",
      "BuzzFeed Quiz 0.857346901535\n",
      "BuzzFeed Japan 0.0151556639795\n",
      "BuzzFeed UK 0.603574725765\n",
      "BuzzFeed Brasil 0.0127997197183\n",
      "BuzzFeed Japan News 0.0387170315684\n",
      "BuzzFeed Deutschland 0.0597336895542\n",
      "BuzzFeed News 0.381261782623\n",
      "BuzzFeed 0.650399031633\n",
      "BuzzFeed Canada 0.641803568741\n",
      "BuzzFeed Video 0.354736716206\n",
      "Best fit for this link would be:  BuzzFeed Quiz\n"
     ]
    }
   ],
   "source": [
    "#####Query by page and title of link - Should a link with this title be posted on this BF page?\n",
    "q_page = \"BuzzFeed Quiz\"\n",
    "q_test = [\"Find Out What your spirit animal is?\"]  ###list of titles you wanna query\n",
    "\n",
    "max_prob = 0\n",
    "max_page = \"\"\n",
    "\n",
    "print(\"Finding best first page for item titled: \", q_test)\n",
    "\n",
    "for pg in all_pages[:10]:\n",
    "    try:\n",
    "        clf = page_clf[pages.index(pg)]\n",
    "    except:\n",
    "#         print(\"Woops, the model for page \", pg, \" is still in the works!\")\n",
    "        continue\n",
    "        \n",
    "    prob = clf.predict_proba(q_test)[0][1]\n",
    "    print(pg, prob)\n",
    "    if prob > max_prob:\n",
    "        max_prob = prob\n",
    "        max_page = pg\n",
    "    \n",
    "print(\"Best fit for this link would be: \", max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('BuzzFeed Quiz', 0.85734690153508308)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_page, max_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buzz_campaign_name</th>\n",
       "      <th>buzz_account_display_name</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1474</th>\n",
       "      <td>Caltech Professor Who Harassed Women Was Also ...</td>\n",
       "      <td>BuzzFeed Science</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1631</th>\n",
       "      <td>The CEO Of A Huge Chicken Company Denied That ...</td>\n",
       "      <td>BuzzFeed Science</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1670</th>\n",
       "      <td>A NASA Spacecraft Has Found Northern Lights An...</td>\n",
       "      <td>BuzzFeed Science</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1782</th>\n",
       "      <td>Scientists Tell Navajo Farmers Their Water Is ...</td>\n",
       "      <td>BuzzFeed Science</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2218</th>\n",
       "      <td>Why Americans Are So Damn Unhealthy, In 4 Shoc...</td>\n",
       "      <td>BuzzFeed Science</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3606</th>\n",
       "      <td>This Is Why You Should Always Shower Before Sw...</td>\n",
       "      <td>BuzzFeed Science</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4187</th>\n",
       "      <td>A Photographer Captured A Jaw-Dropping Weather...</td>\n",
       "      <td>BuzzFeed Science</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7874</th>\n",
       "      <td>A Shocking 51-49 Senate Vote Just Kept Obama's...</td>\n",
       "      <td>BuzzFeed Science</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7937</th>\n",
       "      <td>A Scientist Is Leaving Google For A Mental Hea...</td>\n",
       "      <td>BuzzFeed Science</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8190</th>\n",
       "      <td>Obama Warns Inequality Could Derail Climate Ch...</td>\n",
       "      <td>BuzzFeed Science</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8202</th>\n",
       "      <td>There's A Storm On Saturn And People Think It ...</td>\n",
       "      <td>BuzzFeed Science</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10508</th>\n",
       "      <td>This Study Adds Even More Weight To Scientists...</td>\n",
       "      <td>BuzzFeed Science</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10613</th>\n",
       "      <td>This Is Why Everyone Got Confused About The &amp;q...</td>\n",
       "      <td>BuzzFeed Science</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11477</th>\n",
       "      <td>Despite Donald Trump, Medical Science Just Got...</td>\n",
       "      <td>BuzzFeed Science</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12057</th>\n",
       "      <td>Just A Bunch Of Truly Awesome Signs From The P...</td>\n",
       "      <td>BuzzFeed Science</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13357</th>\n",
       "      <td>Don’t Believe The Big Story About Humans Roami...</td>\n",
       "      <td>BuzzFeed Science</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13671</th>\n",
       "      <td>This Guy Gorged On Fish For A Year To See If O...</td>\n",
       "      <td>BuzzFeed Science</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13794</th>\n",
       "      <td>Trump Is Ordering A Review Of All National Mon...</td>\n",
       "      <td>BuzzFeed Science</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13875</th>\n",
       "      <td>The Science March Was Political. Its Organizer...</td>\n",
       "      <td>BuzzFeed Science</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14549</th>\n",
       "      <td>These Native American Scholars Marched For Ind...</td>\n",
       "      <td>BuzzFeed Science</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14628</th>\n",
       "      <td>15 Climate Change Facts That You'll Know Are T...</td>\n",
       "      <td>BuzzFeed Science</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14674</th>\n",
       "      <td>These March For Science Signs Are So Wonderful...</td>\n",
       "      <td>BuzzFeed Science</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15077</th>\n",
       "      <td>This Is What The World Looks Like Without Water</td>\n",
       "      <td>BuzzFeed Science</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15654</th>\n",
       "      <td>The Opioid Epidemic Is Actually Two Epidemics</td>\n",
       "      <td>BuzzFeed Science</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15883</th>\n",
       "      <td>This Planet Around A Small, Cool Star May Be O...</td>\n",
       "      <td>BuzzFeed Science</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15946</th>\n",
       "      <td>26 Science Tweets That Will Leave You Shook</td>\n",
       "      <td>BuzzFeed Science</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17920</th>\n",
       "      <td>These Takeout Robots Won't Wipe Out Delivery J...</td>\n",
       "      <td>BuzzFeed Science</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18024</th>\n",
       "      <td>California Shows The Rest Of The Country How T...</td>\n",
       "      <td>BuzzFeed Science</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19318</th>\n",
       "      <td>This Sky Pool Lets You Swim Suspended 500 Feet...</td>\n",
       "      <td>BuzzFeed Science</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20881</th>\n",
       "      <td>17 Cheat Sheets That Will Actually Help You Sl...</td>\n",
       "      <td>BuzzFeed Science</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10528</th>\n",
       "      <td>Le résumé pas chiant du débat Macron-Le Pen</td>\n",
       "      <td>BuzzFeed France News</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28562</th>\n",
       "      <td>12 situações em que as pessoas dramáticas semp...</td>\n",
       "      <td>BuzzFeed Brasil</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29330</th>\n",
       "      <td>27 Things We Learned On Set With Dua Lipa</td>\n",
       "      <td>BuzzFeed Celeb</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25063</th>\n",
       "      <td>18 blagues que tous les fans d'Harry Potter ap...</td>\n",
       "      <td>BuzzFeed France</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32851</th>\n",
       "      <td>9 Interesting Theories About Who The \"Riverdal...</td>\n",
       "      <td>BuzzFeed Australia</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27804</th>\n",
       "      <td>Les différences entre les universités français...</td>\n",
       "      <td>BuzzFeed France</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17655</th>\n",
       "      <td>The Ultimate &amp;quot;RuPaul's Drag Race&amp;quot; Se...</td>\n",
       "      <td>BuzzFeed LGBT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32112</th>\n",
       "      <td>Here's Why The World's Biggest Brands Are Blac...</td>\n",
       "      <td>BuzzFeed Politics</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26576</th>\n",
       "      <td>21 Secretos que los blogueros de comida nunca ...</td>\n",
       "      <td>BuzzFeed Español</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27641</th>\n",
       "      <td>We Know The Exact Date You'll Be Abducted By A...</td>\n",
       "      <td>BuzzFeed Quiz</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19539</th>\n",
       "      <td>Este quiz de comida te dirá si te sobra o te f...</td>\n",
       "      <td>Quizzes En Español</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27902</th>\n",
       "      <td>Here Is The Winner Of The \"One Book, One New Y...</td>\n",
       "      <td>BuzzFeed Reader</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31617</th>\n",
       "      <td>Theresa May Has Asked MPs To Help End Violence...</td>\n",
       "      <td>BuzzFeed UK Politics</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15893</th>\n",
       "      <td>Leave It To Tyler, The Creator And Bill Nye To...</td>\n",
       "      <td>Obsessed by BuzzFeed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20548</th>\n",
       "      <td>11 Struggles That Kinda Suck When Trying To Ge...</td>\n",
       "      <td>BuzzFeed Australia</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28526</th>\n",
       "      <td>Si vous ne portez que des vêtements noirs, vou...</td>\n",
       "      <td>BuzzFeed France</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30676</th>\n",
       "      <td>18 Personas súper inteligentes que merecen tod...</td>\n",
       "      <td>BuzzFeed México</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37161</th>\n",
       "      <td>Salut nazi, «extermination» des Arabes: à Lyon...</td>\n",
       "      <td>BuzzFeed France News</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5522</th>\n",
       "      <td>「上野学園大学」が経営陣を批判した教授を相次ぎ解雇　「無効だ」と裁判に……</td>\n",
       "      <td>BuzzFeed Japan News</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19773</th>\n",
       "      <td>佐々木希が結婚報告でおのろけ、おやすみのチューは「ん〜たまにするね」</td>\n",
       "      <td>BuzzFeed Japan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>En pleno siglo XXI, el gobernador de Nuevo Leó...</td>\n",
       "      <td>BuzzFeed México</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2812</th>\n",
       "      <td>Mr. Modi And Mr. Jaitley, I'm Pretty Sure I Ne...</td>\n",
       "      <td>BuzzFeed India</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2810</th>\n",
       "      <td>19 Bilder, von denen du Albträume bekommst, we...</td>\n",
       "      <td>BuzzFeed Deutschland</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16429</th>\n",
       "      <td>Un collectif demande la vente de la pilule san...</td>\n",
       "      <td>BuzzFeed France News</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10952</th>\n",
       "      <td>韓国人俳優に「ファッキン・コリア」　京都のヘイトスピーチを韓国メディアも報道</td>\n",
       "      <td>BuzzFeed Japan News</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31501</th>\n",
       "      <td>You Truly Are 100% Emo If You Can Finish These...</td>\n",
       "      <td>BuzzFeed Quiz</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29119</th>\n",
       "      <td>Contesta 5 preguntas y te diremos qué rol sexu...</td>\n",
       "      <td>BuzzFeed Español</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>Este detalle en 'Unbreakable Kimmy Schmidt' pr...</td>\n",
       "      <td>BuzzFeed México</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19066</th>\n",
       "      <td>34 Memes, die du nur verstehst, wenn Katzen de...</td>\n",
       "      <td>BuzzFeed Deutschland</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36296</th>\n",
       "      <td>言葉をイラストに変換してくれるサイトがおもしろい！</td>\n",
       "      <td>BuzzFeed Japan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>146 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      buzz_campaign_name  \\\n",
       "1474   Caltech Professor Who Harassed Women Was Also ...   \n",
       "1631   The CEO Of A Huge Chicken Company Denied That ...   \n",
       "1670   A NASA Spacecraft Has Found Northern Lights An...   \n",
       "1782   Scientists Tell Navajo Farmers Their Water Is ...   \n",
       "2218   Why Americans Are So Damn Unhealthy, In 4 Shoc...   \n",
       "3606   This Is Why You Should Always Shower Before Sw...   \n",
       "4187   A Photographer Captured A Jaw-Dropping Weather...   \n",
       "7874   A Shocking 51-49 Senate Vote Just Kept Obama's...   \n",
       "7937   A Scientist Is Leaving Google For A Mental Hea...   \n",
       "8190   Obama Warns Inequality Could Derail Climate Ch...   \n",
       "8202   There's A Storm On Saturn And People Think It ...   \n",
       "10508  This Study Adds Even More Weight To Scientists...   \n",
       "10613  This Is Why Everyone Got Confused About The &q...   \n",
       "11477  Despite Donald Trump, Medical Science Just Got...   \n",
       "12057  Just A Bunch Of Truly Awesome Signs From The P...   \n",
       "13357  Don’t Believe The Big Story About Humans Roami...   \n",
       "13671  This Guy Gorged On Fish For A Year To See If O...   \n",
       "13794  Trump Is Ordering A Review Of All National Mon...   \n",
       "13875  The Science March Was Political. Its Organizer...   \n",
       "14549  These Native American Scholars Marched For Ind...   \n",
       "14628  15 Climate Change Facts That You'll Know Are T...   \n",
       "14674  These March For Science Signs Are So Wonderful...   \n",
       "15077    This Is What The World Looks Like Without Water   \n",
       "15654      The Opioid Epidemic Is Actually Two Epidemics   \n",
       "15883  This Planet Around A Small, Cool Star May Be O...   \n",
       "15946        26 Science Tweets That Will Leave You Shook   \n",
       "17920  These Takeout Robots Won't Wipe Out Delivery J...   \n",
       "18024  California Shows The Rest Of The Country How T...   \n",
       "19318  This Sky Pool Lets You Swim Suspended 500 Feet...   \n",
       "20881  17 Cheat Sheets That Will Actually Help You Sl...   \n",
       "...                                                  ...   \n",
       "10528        Le résumé pas chiant du débat Macron-Le Pen   \n",
       "28562  12 situações em que as pessoas dramáticas semp...   \n",
       "29330          27 Things We Learned On Set With Dua Lipa   \n",
       "25063  18 blagues que tous les fans d'Harry Potter ap...   \n",
       "32851  9 Interesting Theories About Who The \"Riverdal...   \n",
       "27804  Les différences entre les universités français...   \n",
       "17655  The Ultimate &quot;RuPaul's Drag Race&quot; Se...   \n",
       "32112  Here's Why The World's Biggest Brands Are Blac...   \n",
       "26576  21 Secretos que los blogueros de comida nunca ...   \n",
       "27641  We Know The Exact Date You'll Be Abducted By A...   \n",
       "19539  Este quiz de comida te dirá si te sobra o te f...   \n",
       "27902  Here Is The Winner Of The \"One Book, One New Y...   \n",
       "31617  Theresa May Has Asked MPs To Help End Violence...   \n",
       "15893  Leave It To Tyler, The Creator And Bill Nye To...   \n",
       "20548  11 Struggles That Kinda Suck When Trying To Ge...   \n",
       "28526  Si vous ne portez que des vêtements noirs, vou...   \n",
       "30676  18 Personas súper inteligentes que merecen tod...   \n",
       "37161  Salut nazi, «extermination» des Arabes: à Lyon...   \n",
       "5522               「上野学園大学」が経営陣を批判した教授を相次ぎ解雇　「無効だ」と裁判に……   \n",
       "19773                 佐々木希が結婚報告でおのろけ、おやすみのチューは「ん〜たまにするね」   \n",
       "1097   En pleno siglo XXI, el gobernador de Nuevo Leó...   \n",
       "2812   Mr. Modi And Mr. Jaitley, I'm Pretty Sure I Ne...   \n",
       "2810   19 Bilder, von denen du Albträume bekommst, we...   \n",
       "16429  Un collectif demande la vente de la pilule san...   \n",
       "10952             韓国人俳優に「ファッキン・コリア」　京都のヘイトスピーチを韓国メディアも報道   \n",
       "31501  You Truly Are 100% Emo If You Can Finish These...   \n",
       "29119  Contesta 5 preguntas y te diremos qué rol sexu...   \n",
       "2499   Este detalle en 'Unbreakable Kimmy Schmidt' pr...   \n",
       "19066  34 Memes, die du nur verstehst, wenn Katzen de...   \n",
       "36296                          言葉をイラストに変換してくれるサイトがおもしろい！   \n",
       "\n",
       "      buzz_account_display_name  class  \n",
       "1474           BuzzFeed Science      1  \n",
       "1631           BuzzFeed Science      1  \n",
       "1670           BuzzFeed Science      1  \n",
       "1782           BuzzFeed Science      1  \n",
       "2218           BuzzFeed Science      1  \n",
       "3606           BuzzFeed Science      1  \n",
       "4187           BuzzFeed Science      1  \n",
       "7874           BuzzFeed Science      1  \n",
       "7937           BuzzFeed Science      1  \n",
       "8190           BuzzFeed Science      1  \n",
       "8202           BuzzFeed Science      1  \n",
       "10508          BuzzFeed Science      1  \n",
       "10613          BuzzFeed Science      1  \n",
       "11477          BuzzFeed Science      1  \n",
       "12057          BuzzFeed Science      1  \n",
       "13357          BuzzFeed Science      1  \n",
       "13671          BuzzFeed Science      1  \n",
       "13794          BuzzFeed Science      1  \n",
       "13875          BuzzFeed Science      1  \n",
       "14549          BuzzFeed Science      1  \n",
       "14628          BuzzFeed Science      1  \n",
       "14674          BuzzFeed Science      1  \n",
       "15077          BuzzFeed Science      1  \n",
       "15654          BuzzFeed Science      1  \n",
       "15883          BuzzFeed Science      1  \n",
       "15946          BuzzFeed Science      1  \n",
       "17920          BuzzFeed Science      1  \n",
       "18024          BuzzFeed Science      1  \n",
       "19318          BuzzFeed Science      1  \n",
       "20881          BuzzFeed Science      1  \n",
       "...                         ...    ...  \n",
       "10528      BuzzFeed France News      0  \n",
       "28562           BuzzFeed Brasil      0  \n",
       "29330            BuzzFeed Celeb      0  \n",
       "25063           BuzzFeed France      0  \n",
       "32851        BuzzFeed Australia      0  \n",
       "27804           BuzzFeed France      0  \n",
       "17655             BuzzFeed LGBT      0  \n",
       "32112         BuzzFeed Politics      0  \n",
       "26576          BuzzFeed Español      0  \n",
       "27641             BuzzFeed Quiz      0  \n",
       "19539        Quizzes En Español      0  \n",
       "27902           BuzzFeed Reader      0  \n",
       "31617      BuzzFeed UK Politics      0  \n",
       "15893      Obsessed by BuzzFeed      0  \n",
       "20548        BuzzFeed Australia      0  \n",
       "28526           BuzzFeed France      0  \n",
       "30676           BuzzFeed México      0  \n",
       "37161      BuzzFeed France News      0  \n",
       "5522        BuzzFeed Japan News      0  \n",
       "19773            BuzzFeed Japan      0  \n",
       "1097            BuzzFeed México      0  \n",
       "2812             BuzzFeed India      0  \n",
       "2810       BuzzFeed Deutschland      0  \n",
       "16429      BuzzFeed France News      0  \n",
       "10952       BuzzFeed Japan News      0  \n",
       "31501             BuzzFeed Quiz      0  \n",
       "29119          BuzzFeed Español      0  \n",
       "2499            BuzzFeed México      0  \n",
       "19066      BuzzFeed Deutschland      0  \n",
       "36296            BuzzFeed Japan      0  \n",
       "\n",
       "[146 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dr = train[train['buzz_post_buzz_id'] == 4456887].sort_values('buzz_post_created_at')\n",
    "def get_previous(X):\n",
    "    prev = -1\n",
    "    buzzid = train[train['buzz_external_id'] == X]['buzz_post_buzz_id'].values[0]    \n",
    "    created_at = train[train['buzz_external_id'] == X]['buzz_post_created_at'].values[0]    \n",
    "    tf = train[(train['buzz_post_buzz_id'] == buzzid)&(train['buzz_post_created_at'] < created_at)]\n",
    "    try:\n",
    "        prev = tf[-1:]['buzz_external_id'].values[0]\n",
    "        return prev\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "dr['previous_external_id'] = dr['buzz_external_id'].apply(get_previous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>previous_external_id</th>\n",
       "      <th>buzz_external_id</th>\n",
       "      <th>buzz_post_created_at</th>\n",
       "      <th>buzz_post_buzz_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37197</th>\n",
       "      <td>-1</td>\n",
       "      <td>560066014088988_1228152523946997</td>\n",
       "      <td>2017-02-06 12:48:13.398595</td>\n",
       "      <td>4456887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36967</th>\n",
       "      <td>560066014088988_1228152523946997</td>\n",
       "      <td>491452930867938_1550064698340084</td>\n",
       "      <td>2017-02-06 18:26:53.590087</td>\n",
       "      <td>4456887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   previous_external_id                  buzz_external_id  \\\n",
       "37197                                -1  560066014088988_1228152523946997   \n",
       "36967  560066014088988_1228152523946997  491452930867938_1550064698340084   \n",
       "\n",
       "             buzz_post_created_at  buzz_post_buzz_id  \n",
       "37197  2017-02-06 12:48:13.398595            4456887  \n",
       "36967  2017-02-06 18:26:53.590087            4456887  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dr[['previous_external_id', 'buzz_external_id', 'buzz_post_created_at', \"buzz_post_buzz_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###parameter tuning using grid search\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "text_clf = Pipeline([\n",
    "                     ('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),\n",
    "#                      ('clf', LinearSVC()),\n",
    "#                      ('clf', RandomForestClassifier()),\n",
    "#                      ('clf', LogisticRegression()),\n",
    "                    ])\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2), (1,3)], \n",
    "              'vect__max_features': [10,100,1000, 10000],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'tfidf__norm': ('l1', 'l2', None),\n",
    "              'clf': [MultinomialNB(), LinearSVC(), RandomForestClassifier(), LogisticRegression(),],\n",
    "#               'clf__alpha': (1e-2, 1e-3),\n",
    "             }\n",
    "\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(X_all['buzz_campaign_name'], X_all['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf': LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "      intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "      multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "      verbose=0),\n",
       " 'tfidf__norm': 'l1',\n",
       " 'tfidf__use_idf': True,\n",
       " 'vect__max_features': 1000,\n",
       " 'vect__ngram_range': (1, 3)}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76712328767123283"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a2ecca9a2d46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_all' is not defined"
     ]
    }
   ],
   "source": [
    "X_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
