{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eshwarchandrasekharan/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas.stats.api import ols\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import math\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "\n",
    "filepath = '/Users/eshwarchandrasekharan/Desktop/repo/predict-pop/models-links/'\n",
    "train_df = pd.read_csv(filepath + 'radshift-links-previous-to-next-page-info.csv')\n",
    "\n",
    "one_hour_features = [\n",
    "       'consumptions_by_type__link_clicks',\n",
    "       'consumptions_by_type__other_clicks',\n",
    "       'reactions_like_total',\n",
    "       'stories_by_action_type__comment',\n",
    "       'stories_by_action_type__like',\n",
    "       'stories_by_action_type__share', \n",
    "        'engaged_fan',\n",
    "       'fan_reach', \n",
    "    'impressions',\n",
    "       'impressions_fan',\n",
    "                ]\n",
    "\n",
    "two_day_features = [\n",
    "       'twoday_consumptions_by_type__link_clicks',\n",
    "       'twoday_stories_by_action_type__share',\n",
    "]\n",
    "\n",
    "all_pages = train_df['page'].unique()\n",
    "cross_promotion_features = all_pages\n",
    "\n",
    "# train_features = one_hour_features\n",
    "# print(\"ONLY 1 HOUR PERFORMANCE IN PREVIOUS PAGE\")\n",
    "# print('SHARES:')\n",
    "# res = ols(y = train_df['next_shares'], x = train_df[train_features])\n",
    "# print(res)\n",
    "# print('CLICKS:')\n",
    "# res = ols(y = train_df['next_clicks'], x = train_df[train_features])\n",
    "# print(res)\n",
    "# train_features = one_hour_features + page_infos# + cross_promotion_features\n",
    "# for feat in cross_promotion_features:\n",
    "#     train_features.append(feat)\n",
    "# train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TREE REGRESSION!\n",
      "Performance stats with page names, and also cross-promotion order included!\n",
      "No. of data-points =  38769\n",
      "Raw values!\n",
      "Shares performance: Accuracy =  0.795540873589  ; Error (/100) =  0.564984989545\n",
      "Clicks performance: Accuracy =  0.802176445372  ; Error (/100) =  0.42019896698\n"
     ]
    }
   ],
   "source": [
    "# from sklearn import linear_model\n",
    "# clf = linear_model.LinearRegression()\n",
    "# print(\"LINEAR REGRESSION!\")\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "clf = DecisionTreeRegressor(max_depth = 10)\n",
    "print(\"TREE REGRESSION!\")\n",
    "\n",
    "# train_df = pd.read_csv('radshift-links-previous-to-next-page-info.csv')\n",
    "# all_pages = list(train_df.page.unique())\n",
    "\n",
    "print(\"Performance stats with page names, and also cross-promotion order included!\")\n",
    "# print(\"WITH cross-promotion features\")\n",
    "\n",
    "page_infos = ['page', 'next_page']\n",
    "\n",
    "# train_features = one_hour_features\n",
    "# train_features = one_hour_features + page_infos + cross_promotion_features\n",
    "train_features = one_hour_features + page_infos\n",
    "# train_features = cross_promotion_features + one_hour_features\n",
    "\n",
    "# log_scale = 1\n",
    "log_scale = 0\n",
    "\n",
    "if log_scale == 1:\n",
    "    for feat in one_hour_features:\n",
    "        train_df[feat] = np.log(train_df[feat] + 1)\n",
    "\n",
    "# cross_promote = 0\n",
    "cross_promote = 1\n",
    "\n",
    "if cross_promote == 1:\n",
    "    for feat in cross_promotion_features:\n",
    "        train_features.append(feat)\n",
    "        \n",
    "# train_features = two_day_features + one_hour_features + cross_promotion_features\n",
    "\n",
    "# for feats in (one_hour_features):\n",
    "#     train_df[feats] = np.log(train_df[feats] + 1)\n",
    "\n",
    "# lm = linear_model.LinearRegression(fit_intercept=True, normalize=True)\n",
    "cv = 10\n",
    "\n",
    "train_df['clicks_bucket'] = np.log(train_df['next_clicks'] + 1)\n",
    "train_df['share_bucket'] = np.log(train_df['next_shares']+1)\n",
    "# train_df['clicks_bucket'] = np.log(train_df['next_clicks'] + 1).astype(int)\n",
    "# train_df['share_bucket'] = np.log(train_df['next_shares']+1).astype(int)\n",
    "\n",
    "print(\"No. of data-points = \", len(train_df))\n",
    "\n",
    "if log_scale == 0:\n",
    "    print(\"Raw values!\")\n",
    "    y_clicks = train_df['next_clicks']\n",
    "    y_share = train_df['next_shares']\n",
    "else:\n",
    "    print(\"Log scaled!\")\n",
    "    y_clicks = train_df['clicks_bucket']\n",
    "    y_share = train_df['share_bucket']\n",
    "\n",
    "###generate DFs for analysis - X and Y\n",
    "X = train_df[train_features]\n",
    "####convert page_infos into dummies\n",
    "X = pd.get_dummies(data = X, columns=['page', 'next_page'])\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "fold = 0\n",
    "cv_folds = 10\n",
    "kf = KFold(n_splits = cv_folds, shuffle = True)\n",
    "\n",
    "clicks_accuracy = []\n",
    "clicks_error = []\n",
    "shares_accuracy = []\n",
    "shares_error = []\n",
    "\n",
    "share_cv_y_test = []\n",
    "share_cv_y_pred = []\n",
    "clicks_cv_y_test = []\n",
    "clicks_cv_y_pred = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "#     print(\"Fold = \", fold)\n",
    "#     print(\"Shares: Fold = \", fold)\n",
    "    y = y_share\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    clf.fit(X, y)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    ####\n",
    "    for pt in y_test:\n",
    "        share_cv_y_test.append(pt)\n",
    "    for pt in y_pred:\n",
    "        share_cv_y_pred.append(pt)\n",
    "        \n",
    "    accuracy = metrics.r2_score(y_test, y_pred)\n",
    "#     print(\"Cross-Predicted Accuracy (R2):\", accuracy)\n",
    "    shares_accuracy.append(accuracy)\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    # print(\"Mean Absolute Error: \", mean_absolute_error(y, predictions))\n",
    "    error_percent = mean_absolute_error(y_test, y_pred)/y.mean()\n",
    "#     print(\"Mean values (share): \", y_test.mean(), \" | percent error: \",  error_percent)\n",
    "    shares_error.append(error_percent)\n",
    "    \n",
    "#     print(\"Clicks: Fold = \", fold)\n",
    "    y = y_clicks\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    clf.fit(X, y)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    ####\n",
    "    for pt in y_test:\n",
    "        clicks_cv_y_test.append(pt)\n",
    "    for pt in y_pred:\n",
    "        clicks_cv_y_pred.append(pt)\n",
    "\n",
    "    accuracy = metrics.r2_score(y_test, y_pred)\n",
    "#     print(\"Cross-Predicted Accuracy (R2):\", accuracy)\n",
    "    clicks_accuracy.append(accuracy)\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    # print(\"Mean Absolute Error: \", mean_absolute_error(y, predictions))\n",
    "    error_percent = mean_absolute_error(y_test, y_pred)/y.mean()\n",
    "#     print(\"Mean values (share): \", y_test.mean(), \" | percent error: \",  error_percent)\n",
    "    clicks_error.append(error_percent)\n",
    "    fold += 1\n",
    "print(\"Shares performance: Accuracy = \", np.mean(shares_accuracy), \" ; Error (/100) = \", np.mean(shares_error))\n",
    "print(\"Clicks performance: Accuracy = \", np.mean(clicks_accuracy), \" ; Error (/100) = \", np.mean(clicks_error))\n",
    "\n",
    "if log_scale == 1:\n",
    "    ##share\n",
    "    print(\"Converting log-scale values back to exponents!\")\n",
    "    print('Shares!')\n",
    "    accuracy = metrics.r2_score(np.exp(share_cv_y_test)-1, np.exp(share_cv_y_pred)-1)\n",
    "    print(\"R2: \", accuracy)\n",
    "    error_percent = mean_absolute_error(np.exp(share_cv_y_test)-1, np.exp(share_cv_y_pred)-1)/np.mean(np.exp(share_cv_y_test)-1)\n",
    "    print(\"Error (\\\\100): \", error_percent)\n",
    "    print(\"Mean: \", np.mean(np.exp(share_cv_y_test)-1))\n",
    "#     plt.scatter(share_cv_y_test, share_cv_y_pred)\n",
    "\n",
    "    print('Clicks!')\n",
    "    accuracy = metrics.r2_score(np.exp(clicks_cv_y_test)-1, np.exp(clicks_cv_y_pred)-1)\n",
    "    print(\"R2: \", accuracy)\n",
    "    error_percent = mean_absolute_error(np.exp(clicks_cv_y_test)-1, np.exp(clicks_cv_y_pred)-1)/np.mean(np.exp(clicks_cv_y_test)-1)\n",
    "    print(\"Error (\\\\100): \", error_percent)\n",
    "    print(\"Mean: \", np.mean(np.exp(clicks_cv_y_test)-1))\n",
    "#     plt.scatter(clicks_cv_y_test, clicks_cv_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Task 3: Routing!\n",
    "# 1) \n",
    "# TREE REGRESSION!\n",
    "# Performance stats with page names, and also cross-promotion order included!\n",
    "# No. of data-points =  38769\n",
    "# Raw values!\n",
    "# Shares performance: Accuracy =  0.793776564986  ; Error (/100) =  0.564984135071\n",
    "# Clicks performance: Accuracy =  0.803486044908  ; Error (/100) =  0.420198927612\n",
    "    \n",
    "# ----\n",
    "# 2)\n",
    "# TREE REGRESSION!\n",
    "# Performance stats with page names, and also cross-promotion order included!\n",
    "# No. of data-points =  38769\n",
    "# Log scaled!\n",
    "# Shares performance: Accuracy =  0.6575787588  ; Error (/100) =  0.212635134964\n",
    "# Clicks performance: Accuracy =  0.721635998508  ; Error (/100) =  0.0910451690249\n",
    "    \n",
    "# 2.1) Convert log-scale predictions/test to exponents and compute R2/errors\n",
    "\n",
    "# Shares!\n",
    "# R2:  0.779841664501\n",
    "# Error (\\100):  0.516329728148\n",
    "# Mean:  118.348990173\n",
    "# Clicks!\n",
    "# R2:  0.779658862269\n",
    "# Error (\\100):  0.398451498565\n",
    "# Mean:  10139.2730016\n",
    "# ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Get all links where \"next_page\" == test_page, and then sort links based on predicted shares/clicks. \n",
    "###Verify how many of the sorted links are in the right place, wrt to actual values of shares/clicks!\n",
    "test_page = 'Tasty'\n",
    "test_df = train_df[train_df.next_page == test_page]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_hour_features = [\n",
    "       'consumptions_by_type__link_clicks',\n",
    "       'consumptions_by_type__other_clicks',\n",
    "       'reactions_like_total',\n",
    "       'stories_by_action_type__comment',\n",
    "       'stories_by_action_type__like',\n",
    "       'stories_by_action_type__share', \n",
    "        'engaged_fan',\n",
    "       'fan_reach', \n",
    "    'impressions',\n",
    "       'impressions_fan',\n",
    "                ]\n",
    "\n",
    "two_day_features = [\n",
    "       'twoday_consumptions_by_type__link_clicks',\n",
    "       'twoday_stories_by_action_type__share',\n",
    "]\n",
    "\n",
    "all_pages = train_df['page'].unique()\n",
    "cross_promotion_features = all_pages\n",
    "\n",
    "page_infos = ['page', 'next_page']\n",
    "\n",
    "# train_features = one_hour_features\n",
    "# train_features = one_hour_features + page_infos + cross_promotion_features\n",
    "test_features = one_hour_features + page_infos\n",
    "# train_features = cross_promotion_features + one_hour_features\n",
    "\n",
    "# cross_promote = 0\n",
    "cross_promote = 1\n",
    "\n",
    "if cross_promote == 1:\n",
    "    for feat in cross_promotion_features:\n",
    "        test_features.append(feat)\n",
    "\n",
    "# log_scale = 1\n",
    "log_scale = 0\n",
    "\n",
    "if log_scale == 1:\n",
    "    for feat in one_hour_features:\n",
    "        test_df[feat] = np.log(test_df[feat] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['next_page'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shares!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eshwarchandrasekharan/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test page: BuzzFeed Food ; Accuracy:  0.7542754275427542 ; Total:  9999 ; Hits:  7542 ; Miss:  2457\n",
      "Total links:  922 ; Top K:  20 ; Hits:  11 ; hit-rate:  0.55\n",
      "Test page: BuzzFeed ; Accuracy:  0.7726772677267727 ; Total:  9999 ; Hits:  7726 ; Miss:  2273\n",
      "Total links:  7547 ; Top K:  20 ; Hits:  16 ; hit-rate:  0.8\n",
      "Test page: BuzzFeed Video ; Accuracy:  0.8030803080308031 ; Total:  9999 ; Hits:  8030 ; Miss:  1969\n",
      "Total links:  2160 ; Top K:  20 ; Hits:  13 ; hit-rate:  0.65\n",
      "Test page: Buy Me That ; Accuracy:  0.6707670767076708 ; Total:  9999 ; Hits:  6707 ; Miss:  3292\n",
      "Total links:  595 ; Top K:  20 ; Hits:  4 ; hit-rate:  0.2\n",
      "Test page: BuzzFeed Parents ; Accuracy:  0.7075707570757076 ; Total:  9999 ; Hits:  7075 ; Miss:  2924\n",
      "Total links:  459 ; Top K:  20 ; Hits:  5 ; hit-rate:  0.25\n",
      "Test page: BuzzFeed Australia ; Accuracy:  0.6788678867886788 ; Total:  9999 ; Hits:  6788 ; Miss:  3211\n",
      "Total links:  3141 ; Top K:  20 ; Hits:  2 ; hit-rate:  0.1\n",
      "Test page: BuzzFeed News ; Accuracy:  0.693969396939694 ; Total:  9999 ; Hits:  6939 ; Miss:  3060\n",
      "Total links:  2234 ; Top K:  20 ; Hits:  6 ; hit-rate:  0.3\n",
      "Test page: BuzzFeed Entertainment ; Accuracy:  0.7248724872487249 ; Total:  9999 ; Hits:  7248 ; Miss:  2751\n",
      "Total links:  1505 ; Top K:  20 ; Hits:  7 ; hit-rate:  0.35\n",
      "Test page: BuzzFeed UK News ; Accuracy:  0.6548654865486548 ; Total:  9999 ; Hits:  6548 ; Miss:  3451\n",
      "Total links:  320 ; Top K:  20 ; Hits:  6 ; hit-rate:  0.3\n",
      "Test page: BuzzFeed Celeb ; Accuracy:  0.716971697169717 ; Total:  9999 ; Hits:  7169 ; Miss:  2830\n",
      "Total links:  1096 ; Top K:  20 ; Hits:  7 ; hit-rate:  0.35\n",
      "Test page: Tasty ; Accuracy:  0.8422842284228422 ; Total:  9999 ; Hits:  8422 ; Miss:  1577\n",
      "Total links:  866 ; Top K:  20 ; Hits:  19 ; hit-rate:  0.95\n",
      "Test page: BuzzFeed Geeky ; Accuracy:  0.7443744374437443 ; Total:  9999 ; Hits:  7443 ; Miss:  2556\n",
      "Total links:  261 ; Top K:  20 ; Hits:  4 ; hit-rate:  0.2\n",
      "Test page: BuzzFeed Health ; Accuracy:  0.7283728372837284 ; Total:  9999 ; Hits:  7283 ; Miss:  2716\n",
      "Total links:  852 ; Top K:  20 ; Hits:  3 ; hit-rate:  0.15\n",
      "Test page: BuzzFeed DIY ; Accuracy:  0.7381738173817382 ; Total:  9999 ; Hits:  7381 ; Miss:  2618\n",
      "Total links:  807 ; Top K:  20 ; Hits:  7 ; hit-rate:  0.35\n",
      "Test page: SOML ; Accuracy:  0.6311631163116311 ; Total:  9999 ; Hits:  6311 ; Miss:  3688\n",
      "Total links:  588 ; Top K:  20 ; Hits:  2 ; hit-rate:  0.1\n",
      "Test page: BuzzFeed Books ; Accuracy:  0.6786678667866787 ; Total:  9999 ; Hits:  6786 ; Miss:  3213\n",
      "Total links:  1135 ; Top K:  20 ; Hits:  1 ; hit-rate:  0.05\n",
      "Test page: BuzzFeed Style ; Accuracy:  0.7377737773777377 ; Total:  9999 ; Hits:  7377 ; Miss:  2622\n",
      "Total links:  1194 ; Top K:  20 ; Hits:  7 ; hit-rate:  0.35\n",
      "Test page: BuzzFeed UK ; Accuracy:  0.7097709770977098 ; Total:  9999 ; Hits:  7097 ; Miss:  2902\n",
      "Total links:  2263 ; Top K:  20 ; Hits:  3 ; hit-rate:  0.15\n",
      "Test page: Obsessed by BuzzFeed ; Accuracy:  0.7163716371637163 ; Total:  9999 ; Hits:  7163 ; Miss:  2836\n",
      "Total links:  825 ; Top K:  20 ; Hits:  6 ; hit-rate:  0.3\n",
      "Test page: BuzzFeed Animals ; Accuracy:  0.6374637463746374 ; Total:  9999 ; Hits:  6374 ; Miss:  3625\n",
      "Total links:  929 ; Top K:  20 ; Hits:  2 ; hit-rate:  0.1\n",
      "Test page: Cocoa Butter ; Accuracy:  0.6405640564056405 ; Total:  9999 ; Hits:  6405 ; Miss:  3594\n",
      "Total links:  584 ; Top K:  20 ; Hits:  2 ; hit-rate:  0.1\n",
      "Test page: BuzzFeed Unsolved ; Accuracy:  0.6831683168316832 ; Total:  9999 ; Hits:  6831 ; Miss:  3168\n",
      "Total links:  19 ; Top K:  20 ; Hits:  19 ; hit-rate:  0.95\n",
      "Test page: BuzzFeed Science ; Accuracy:  0.6343634363436343 ; Total:  9999 ; Hits:  6343 ; Miss:  3656\n",
      "Total links:  85 ; Top K:  20 ; Hits:  5 ; hit-rate:  0.25\n",
      "Test page: Cheeky ; Accuracy:  0.6903690369036903 ; Total:  9999 ; Hits:  6903 ; Miss:  3096\n",
      "Total links:  892 ; Top K:  20 ; Hits:  7 ; hit-rate:  0.35\n",
      "Test page: BuzzFeed Quiz ; Accuracy:  0.5744574457445745 ; Total:  9999 ; Hits:  5744 ; Miss:  4255\n",
      "Total links:  691 ; Top K:  20 ; Hits:  7 ; hit-rate:  0.35\n",
      "Test page: BuzzFeed Politics ; Accuracy:  0.7001700170017001 ; Total:  9999 ; Hits:  7001 ; Miss:  2998\n",
      "Total links:  189 ; Top K:  20 ; Hits:  11 ; hit-rate:  0.55\n",
      "Test page: BuzzFeed LGBT ; Accuracy:  0.6744674467446745 ; Total:  9999 ; Hits:  6744 ; Miss:  3255\n",
      "Total links:  169 ; Top K:  20 ; Hits:  9 ; hit-rate:  0.45\n",
      "Test page: BuzzFeed Weddings ; Accuracy:  0.6993699369936993 ; Total:  9999 ; Hits:  6993 ; Miss:  3006\n",
      "Total links:  275 ; Top K:  20 ; Hits:  8 ; hit-rate:  0.4\n",
      "Test page: Bien Tasty ; Accuracy:  0.6601660166016602 ; Total:  9999 ; Hits:  6601 ; Miss:  3398\n",
      "Total links:  35 ; Top K:  20 ; Hits:  12 ; hit-rate:  0.6\n",
      "Skipping... Cos < 5 links on  Top Knot\n",
      "Test page: BuzzFeed India ; Accuracy:  0.6411641164116412 ; Total:  9999 ; Hits:  6411 ; Miss:  3588\n",
      "Total links:  572 ; Top K:  20 ; Hits:  1 ; hit-rate:  0.05\n",
      "Test page: Reasons to Smile ; Accuracy:  0.9488948894889488 ; Total:  9999 ; Hits:  9488 ; Miss:  511\n",
      "Total links:  5 ; Top K:  20 ; Hits:  5 ; hit-rate:  0.25\n",
      "Test page: BuzzFeed World ; Accuracy:  0.6810681068106811 ; Total:  9999 ; Hits:  6810 ; Miss:  3189\n",
      "Total links:  241 ; Top K:  20 ; Hits:  9 ; hit-rate:  0.45\n",
      "Test page: LOLA ; Accuracy:  0.5927592759275928 ; Total:  9999 ; Hits:  5927 ; Miss:  4072\n",
      "Total links:  39 ; Top K:  20 ; Hits:  12 ; hit-rate:  0.6\n",
      "Test page: Proper Tasty ; Accuracy:  0.6993699369936993 ; Total:  9999 ; Hits:  6993 ; Miss:  3006\n",
      "Total links:  24 ; Top K:  20 ; Hits:  20 ; hit-rate:  1.0\n",
      "Test page: Nifty ; Accuracy:  0.5878587858785879 ; Total:  9999 ; Hits:  5878 ; Miss:  4121\n",
      "Total links:  20 ; Top K:  20 ; Hits:  20 ; hit-rate:  1.0\n",
      "Test page: BuzzFeed Tech ; Accuracy:  0.7141714171417142 ; Total:  9999 ; Hits:  7141 ; Miss:  2858\n",
      "Total links:  22 ; Top K:  20 ; Hits:  20 ; hit-rate:  1.0\n",
      "Skipping... Cos < 5 links on  Tasty One-Pot\n",
      "Skipping... Cos < 5 links on  BuzzFeed Sweaty\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "#####train on all, test per page! \n",
    "\n",
    "pages = []\n",
    "accuracy = []\n",
    "topK_hit_rate = []\n",
    "num_links = []\n",
    "\n",
    "print(\"Shares!\")\n",
    "# test_page = 'Tasty'\n",
    "\n",
    "test_pages = [\n",
    "'BuzzFeed Food',\n",
    "'BuzzFeed',\n",
    "'BuzzFeed Video',\n",
    "'Buy Me That',\n",
    "'BuzzFeed Parents',\n",
    "'BuzzFeed Australia',\n",
    "'BuzzFeed News',\n",
    "'BuzzFeed Entertainment',\n",
    "'BuzzFeed UK News',\n",
    "'BuzzFeed Celeb',\n",
    "'Tasty',\n",
    "'BuzzFeed Geeky',\n",
    "'BuzzFeed Health',\n",
    "'BuzzFeed DIY',\n",
    "'SOML',\n",
    "'BuzzFeed Books',\n",
    "'BuzzFeed Style',\n",
    "'BuzzFeed UK',\n",
    "'Obsessed by BuzzFeed',\n",
    "'BuzzFeed Animals',\n",
    "'Cocoa Butter',\n",
    "'BuzzFeed Unsolved',\n",
    "'BuzzFeed Science',\n",
    "'Cheeky',\n",
    "'BuzzFeed Quiz',\n",
    "'BuzzFeed Politics',\n",
    "'BuzzFeed LGBT',\n",
    "'BuzzFeed Weddings',\n",
    "'Bien Tasty',\n",
    "'Top Knot',\n",
    "'BuzzFeed India',\n",
    "'Reasons to Smile',\n",
    "'BuzzFeed World',\n",
    "'LOLA',\n",
    "'Proper Tasty',\n",
    "'Nifty',\n",
    "'BuzzFeed Tech',\n",
    "'Tasty One-Pot',\n",
    "'BuzzFeed Sweaty',\n",
    "]\n",
    "\n",
    "for test_page in test_pages:\n",
    "# for test_page in train_df['next_page'].unique():\n",
    "# for test_page in ['Tasty', 'SOML', 'BuzzFeed News', 'Nifty', 'BuzzFeed Animals']:\n",
    "#     print(\"Test page: \", test_page)\n",
    "    X = train_df\n",
    "    # X = train_df[~(train_df.next_page == test_page)]\n",
    "    y_train = X['next_shares']\n",
    "    X = X[test_features]\n",
    "    X['test_split'] = 0\n",
    "\n",
    "    test_df = train_df[train_df.next_page == test_page]\n",
    "    if len(test_df) < 5:\n",
    "        print(\"Skipping... Cos < 5 links on \", test_page)\n",
    "        continue\n",
    "    num_links.append(len(test_df))\n",
    "    y_test = test_df[['next_shares', 'external_id']]\n",
    "    test_df = test_df[test_features]\n",
    "    test_df['test_split'] = 1\n",
    "\n",
    "    X = pd.concat([X,test_df])\n",
    "\n",
    "    ####convert page_infos into dummies\n",
    "    X = pd.get_dummies(data = X, columns=['page', 'next_page'])\n",
    "\n",
    "    X_train = X[X.test_split == 0]\n",
    "    X_test = X[X.test_split == 1]\n",
    "\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    clf = DecisionTreeRegressor(max_depth = 10)\n",
    "#     print(\"TREE REGRESSION!\")\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "#     accuracy = metrics.r2_score(y_test['next_shares'], y_pred)\n",
    "#     print(\"Cross-Predicted Accuracy (R2):\", accuracy)\n",
    "#     from sklearn.metrics import mean_absolute_error\n",
    "#     print(\"Mean Absolute Error: \", mean_absolute_error(y_test['next_shares'], y_pred))\n",
    "#     error_percent = mean_absolute_error(y_test['next_shares'], y_pred)/y_test['next_shares'].mean()\n",
    "#     print(\"Mean values (share): \", y_test['next_shares'].mean(), \" | percent error: \",  error_percent)\n",
    "\n",
    "    ###sort and see how many pair-wise predictions are correct!?\n",
    "    order = pd.DataFrame()\n",
    "    order = y_test\n",
    "    # order['actual_shares'] = y_test\n",
    "    order['pred_shares'] = y_pred\n",
    "\n",
    "    order['actual_rank'] = order['next_shares'].rank(ascending=0)\n",
    "    order['predicted_rank'] = order['pred_shares'].rank(ascending=0)\n",
    "\n",
    "    ###randomly pick 2 external_ids and make predictions on which will do better than the other:\n",
    "    limit = 10000\n",
    "    hits = 0\n",
    "    miss = 0 \n",
    "    total = 0\n",
    "    \n",
    "    for i in range(1,limit):\n",
    "        total += 1\n",
    "        trial = order.sample(n=2)\n",
    "    #     print(trial)\n",
    "        act1 = trial[:1].next_shares.values[0]\n",
    "        act2 = trial[1:2].next_shares.values[0]\n",
    "        pred1 = trial[:1].pred_shares.values[0]\n",
    "        pred2 = trial[1:2].pred_shares.values[0]\n",
    "\n",
    "        if act1 >= act2:\n",
    "            if pred1 >= pred2:\n",
    "                hits += 1\n",
    "            else:\n",
    "                miss += 1\n",
    "        else:\n",
    "            if pred1 < pred2:\n",
    "                hits += 1\n",
    "            else:\n",
    "                miss += 1\n",
    "    # test = test_df.sample(n = 2)\n",
    "    print(\"Test page:\", test_page, \"; Accuracy: \", hits/total, \"; Total: \", total, \"; Hits: \", hits, \"; Miss: \", miss)\n",
    "#     print(\"Test page: \", test_page, \"; Accuracy: \", hits/total)\n",
    "    pages.append(test_page)\n",
    "    accuracy.append(hits/total)\n",
    "    ###top K predictions\n",
    "    top_k = 20\n",
    "    temp = order.sort_values('actual_rank', ascending = True)[:top_k]\n",
    "    hits = len(temp[temp['predicted_rank'] <= top_k])\n",
    "    print('Total links: ', len(test_df), '; Top K: ', top_k, '; Hits: ', hits, \"; hit-rate: \", hits/top_k)\n",
    "    topK_hit_rate.append(hits/top_k)\n",
    "\n",
    "simulations = pd.DataFrame()\n",
    "simulations['page'] = pages\n",
    "simulations['accuracy'] = accuracy\n",
    "simulations['topK_hit_rate'] = topK_hit_rate\n",
    "simulations['total_links'] = num_links\n",
    "\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>topK_hit_rate</th>\n",
       "      <th>total_links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BuzzFeed Food</td>\n",
       "      <td>0.762676</td>\n",
       "      <td>0.55</td>\n",
       "      <td>922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BuzzFeed</td>\n",
       "      <td>0.770477</td>\n",
       "      <td>0.80</td>\n",
       "      <td>7547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BuzzFeed Video</td>\n",
       "      <td>0.804280</td>\n",
       "      <td>0.65</td>\n",
       "      <td>2160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Buy Me That</td>\n",
       "      <td>0.670367</td>\n",
       "      <td>0.20</td>\n",
       "      <td>595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BuzzFeed Parents</td>\n",
       "      <td>0.704870</td>\n",
       "      <td>0.25</td>\n",
       "      <td>459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BuzzFeed Australia</td>\n",
       "      <td>0.687269</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BuzzFeed News</td>\n",
       "      <td>0.689869</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BuzzFeed Entertainment</td>\n",
       "      <td>0.717572</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BuzzFeed UK News</td>\n",
       "      <td>0.657966</td>\n",
       "      <td>0.30</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BuzzFeed Celeb</td>\n",
       "      <td>0.708071</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Tasty</td>\n",
       "      <td>0.836684</td>\n",
       "      <td>0.95</td>\n",
       "      <td>866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BuzzFeed Geeky</td>\n",
       "      <td>0.744274</td>\n",
       "      <td>0.20</td>\n",
       "      <td>261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>BuzzFeed Health</td>\n",
       "      <td>0.722272</td>\n",
       "      <td>0.15</td>\n",
       "      <td>852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>BuzzFeed DIY</td>\n",
       "      <td>0.739174</td>\n",
       "      <td>0.35</td>\n",
       "      <td>807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SOML</td>\n",
       "      <td>0.629763</td>\n",
       "      <td>0.10</td>\n",
       "      <td>588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>BuzzFeed Books</td>\n",
       "      <td>0.681268</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>BuzzFeed Style</td>\n",
       "      <td>0.735074</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BuzzFeed UK</td>\n",
       "      <td>0.722172</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Obsessed by BuzzFeed</td>\n",
       "      <td>0.716472</td>\n",
       "      <td>0.30</td>\n",
       "      <td>825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>BuzzFeed Animals</td>\n",
       "      <td>0.642864</td>\n",
       "      <td>0.10</td>\n",
       "      <td>929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Cocoa Butter</td>\n",
       "      <td>0.643364</td>\n",
       "      <td>0.10</td>\n",
       "      <td>584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>BuzzFeed Unsolved</td>\n",
       "      <td>0.676468</td>\n",
       "      <td>0.95</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>BuzzFeed Science</td>\n",
       "      <td>0.629263</td>\n",
       "      <td>0.25</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Cheeky</td>\n",
       "      <td>0.691469</td>\n",
       "      <td>0.35</td>\n",
       "      <td>892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>BuzzFeed Quiz</td>\n",
       "      <td>0.581658</td>\n",
       "      <td>0.35</td>\n",
       "      <td>691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>BuzzFeed Politics</td>\n",
       "      <td>0.699670</td>\n",
       "      <td>0.55</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>BuzzFeed LGBT</td>\n",
       "      <td>0.683868</td>\n",
       "      <td>0.45</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>BuzzFeed Weddings</td>\n",
       "      <td>0.699170</td>\n",
       "      <td>0.40</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Bien Tasty</td>\n",
       "      <td>0.661966</td>\n",
       "      <td>0.60</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>BuzzFeed India</td>\n",
       "      <td>0.630263</td>\n",
       "      <td>0.05</td>\n",
       "      <td>572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Reasons to Smile</td>\n",
       "      <td>0.949895</td>\n",
       "      <td>0.25</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>BuzzFeed World</td>\n",
       "      <td>0.688769</td>\n",
       "      <td>0.45</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>LOLA</td>\n",
       "      <td>0.592559</td>\n",
       "      <td>0.60</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Proper Tasty</td>\n",
       "      <td>0.703170</td>\n",
       "      <td>1.00</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Nifty</td>\n",
       "      <td>0.596360</td>\n",
       "      <td>1.00</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>BuzzFeed Tech</td>\n",
       "      <td>0.713271</td>\n",
       "      <td>1.00</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      page  accuracy  topK_hit_rate  total_links\n",
       "0            BuzzFeed Food  0.762676           0.55          922\n",
       "1                 BuzzFeed  0.770477           0.80         7547\n",
       "2           BuzzFeed Video  0.804280           0.65         2160\n",
       "3              Buy Me That  0.670367           0.20          595\n",
       "4         BuzzFeed Parents  0.704870           0.25          459\n",
       "5       BuzzFeed Australia  0.687269           0.10         3141\n",
       "6            BuzzFeed News  0.689869           0.30         2234\n",
       "7   BuzzFeed Entertainment  0.717572           0.35         1505\n",
       "8         BuzzFeed UK News  0.657966           0.30          320\n",
       "9           BuzzFeed Celeb  0.708071           0.35         1096\n",
       "10                   Tasty  0.836684           0.95          866\n",
       "11          BuzzFeed Geeky  0.744274           0.20          261\n",
       "12         BuzzFeed Health  0.722272           0.15          852\n",
       "13            BuzzFeed DIY  0.739174           0.35          807\n",
       "14                    SOML  0.629763           0.10          588\n",
       "15          BuzzFeed Books  0.681268           0.05         1135\n",
       "16          BuzzFeed Style  0.735074           0.35         1194\n",
       "17             BuzzFeed UK  0.722172           0.15         2263\n",
       "18    Obsessed by BuzzFeed  0.716472           0.30          825\n",
       "19        BuzzFeed Animals  0.642864           0.10          929\n",
       "20            Cocoa Butter  0.643364           0.10          584\n",
       "21       BuzzFeed Unsolved  0.676468           0.95           19\n",
       "22        BuzzFeed Science  0.629263           0.25           85\n",
       "23                  Cheeky  0.691469           0.35          892\n",
       "24           BuzzFeed Quiz  0.581658           0.35          691\n",
       "25       BuzzFeed Politics  0.699670           0.55          189\n",
       "26           BuzzFeed LGBT  0.683868           0.45          169\n",
       "27       BuzzFeed Weddings  0.699170           0.40          275\n",
       "28              Bien Tasty  0.661966           0.60           35\n",
       "29          BuzzFeed India  0.630263           0.05          572\n",
       "30        Reasons to Smile  0.949895           0.25            5\n",
       "31          BuzzFeed World  0.688769           0.45          241\n",
       "32                    LOLA  0.592559           0.60           39\n",
       "33            Proper Tasty  0.703170           1.00           24\n",
       "34                   Nifty  0.596360           1.00           20\n",
       "35           BuzzFeed Tech  0.713271           1.00           22"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulations.to_csv('task3-simulations.csv', index = False)\n",
    "simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:  9999 ; Hits:  8330 ; Miss:  1669\n",
      "Accuracy:  0.8330833083308331\n"
     ]
    }
   ],
   "source": [
    "###randomly pick 2 external_ids and make predictions on which will do better than the other:\n",
    "\n",
    "limit =10000\n",
    "hits = 0\n",
    "miss = 0 \n",
    "total = 0\n",
    "\n",
    "for i in range(1,limit):\n",
    "    total += 1\n",
    "    trial = order[:100].sample(n=2)\n",
    "#     print(trial)\n",
    "    act1 = trial[:1].next_shares.values[0]\n",
    "    act2 = trial[1:2].next_shares.values[0]\n",
    "    pred1 = trial[:1].pred_shares.values[0]\n",
    "    pred2 = trial[1:2].pred_shares.values[0]\n",
    "    \n",
    "    if act1 >= act2:\n",
    "        if pred1 >= pred2:\n",
    "            hits += 1\n",
    "        else:\n",
    "            miss += 1\n",
    "    else:\n",
    "        if pred1 < pred2:\n",
    "            hits += 1\n",
    "        else:\n",
    "            miss += 1\n",
    "# test = test_df.sample(n = 2)\n",
    "print(\"Total: \", total, \"; Hits: \", hits, \"; Miss: \", miss)\n",
    "print(\"Accuracy: \", hits/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Randomly pick 2 external_ids and make predictions on which will do better than the other:\n",
    "# Total:  9999 ; Hits:  8462 ; Miss:  1537\n",
    "# Accuracy: 0.8462846284628462"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:  99 ; Hits:  86 ; Miss:  13\n",
      "Accuracy:  0.8686868686868687\n"
     ]
    }
   ],
   "source": [
    "###randomly pick 2 external_ids from TOP 100 and make predictions on which will do better than the other:\n",
    "\n",
    "limit = 100\n",
    "hits = 0\n",
    "miss = 0 \n",
    "total = 0\n",
    "\n",
    "for i in range(1,limit):\n",
    "    total += 1\n",
    "    trial = order[:100].sample(n=2)\n",
    "#     print(trial)\n",
    "    act1 = trial[:1].next_shares.values[0]\n",
    "    act2 = trial[1:2].next_shares.values[0]\n",
    "    pred1 = trial[:1].pred_shares.values[0]\n",
    "    pred2 = trial[1:2].pred_shares.values[0]\n",
    "    \n",
    "    if act1 >= act2:\n",
    "        if pred1 >= pred2:\n",
    "            hits += 1\n",
    "        else:\n",
    "            miss += 1\n",
    "    else:\n",
    "        if pred1 < pred2:\n",
    "            hits += 1\n",
    "        else:\n",
    "            miss += 1\n",
    "# test = test_df.sample(n = 2)\n",
    "print(\"Total: \", total, \"; Hits: \", hits, \"; Miss: \", miss)\n",
    "print(\"Accuracy: \", hits/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###CLICKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLICKS!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eshwarchandrasekharan/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TREE REGRESSION!\n",
      "Cross-Predicted Accuracy (R2): 0.0451870042936\n",
      "Mean Absolute Error:  3746.0814707\n",
      "Mean values (share):  4882.68537414966  | percent error:  0.767217459994\n"
     ]
    }
   ],
   "source": [
    "#####train on all, test per page! \n",
    "\n",
    "print(\"CLICKS!\")\n",
    "X = train_df[test_features]\n",
    "y_train = train_df['next_clicks']\n",
    "\n",
    "X['test_split'] = 0\n",
    "\n",
    "test_page = 'SOML'\n",
    "# test_page = 'Tasty'\n",
    "test_df = train_df[train_df.next_page == test_page]\n",
    "y_test = test_df[['next_clicks', 'external_id']]\n",
    "test_df = test_df[test_features]\n",
    "test_df['test_split'] = 1\n",
    "\n",
    "X = pd.concat([X,test_df])\n",
    "\n",
    "####convert page_infos into dummies\n",
    "X = pd.get_dummies(data = X, columns=['page', 'next_page'])\n",
    "\n",
    "X_train = X[X.test_split == 0]\n",
    "X_test = X[X.test_split == 1]\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "clf = DecisionTreeRegressor(max_depth = 10)\n",
    "print(\"TREE REGRESSION!\")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = metrics.r2_score(y_test['next_clicks'], y_pred)\n",
    "print(\"Cross-Predicted Accuracy (R2):\", accuracy)\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"Mean Absolute Error: \", mean_absolute_error(y_test['next_clicks'], y_pred))\n",
    "error_percent = mean_absolute_error(y_test['next_clicks'], y_pred)/y_test['next_clicks'].mean()\n",
    "print(\"Mean values (share): \", y_test['next_clicks'].mean(), \" | percent error: \",  error_percent)\n",
    "\n",
    "###sort and see how many pair-wise predictions are correct!?\n",
    "order = pd.DataFrame()\n",
    "order = y_test\n",
    "# order['actual_shares'] = y_test\n",
    "order['pred_clicks'] = y_pred\n",
    "\n",
    "order['actual_rank'] = order['next_clicks'].rank(ascending=0)\n",
    "order['predicted_rank'] = order['pred_clicks'].rank(ascending=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:  99999 ; Hits:  79901 ; Miss:  20098\n",
      "Accuracy:  0.7990179901799018\n"
     ]
    }
   ],
   "source": [
    "###randomly pick 2 external_ids and make predictions on which will do better than the other: wrt CLICKS\n",
    "\n",
    "limit =100000\n",
    "hits = 0\n",
    "miss = 0 \n",
    "total = 0\n",
    "\n",
    "for i in range(1,limit):\n",
    "    total += 1\n",
    "    trial = order[:100].sample(n=2)\n",
    "#     print(trial)\n",
    "    act1 = trial[:1].next_clicks.values[0]\n",
    "    act2 = trial[1:2].next_clicks.values[0]\n",
    "    pred1 = trial[:1].pred_clicks.values[0]\n",
    "    pred2 = trial[1:2].pred_clicks.values[0]\n",
    "    \n",
    "    if act1 >= act2:\n",
    "        if pred1 >= pred2:\n",
    "            hits += 1\n",
    "        else:\n",
    "            miss += 1\n",
    "    else:\n",
    "        if pred1 < pred2:\n",
    "            hits += 1\n",
    "        else:\n",
    "            miss += 1\n",
    "# test = test_df.sample(n = 2)\n",
    "print(\"Total: \", total, \"; Hits: \", hits, \"; Miss: \", miss)\n",
    "print(\"Accuracy: \", hits/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:  99 ; Hits:  72 ; Miss:  27\n",
      "Accuracy:  0.7272727272727273\n"
     ]
    }
   ],
   "source": [
    "###randomly pick 2 external_ids from TOP 100 and make predictions on which will do better than the other:\n",
    "\n",
    "limit = 100\n",
    "hits = 0\n",
    "miss = 0 \n",
    "total = 0\n",
    "\n",
    "for i in range(1,limit):\n",
    "    total += 1\n",
    "    trial = order[:100].sample(n=2)\n",
    "#     print(trial)\n",
    "    act1 = trial[:1].next_clicks.values[0]\n",
    "    act2 = trial[1:2].next_clicks.values[0]\n",
    "    pred1 = trial[:1].pred_clicks.values[0]\n",
    "    pred2 = trial[1:2].pred_clicks.values[0]\n",
    "    \n",
    "    if act1 >= act2:\n",
    "        if pred1 >= pred2:\n",
    "            hits += 1\n",
    "        else:\n",
    "            miss += 1\n",
    "    else:\n",
    "        if pred1 < pred2:\n",
    "            hits += 1\n",
    "        else:\n",
    "            miss += 1\n",
    "# test = test_df.sample(n = 2)\n",
    "print(\"Total: \", total, \"; Hits: \", hits, \"; Miss: \", miss)\n",
    "print(\"Accuracy: \", hits/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eshwarchandrasekharan/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TREE REGRESSION!\n",
      "Cross-Predicted Accuracy (R2): 0.0800118692307\n",
      "Mean Absolute Error:  65.2914646141\n",
      "Mean values (share):  81.15646258503402  | percent error:  0.804513436569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eshwarchandrasekharan/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/eshwarchandrasekharan/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/eshwarchandrasekharan/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:68: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "######### INCLUDE TITLE FEATURES ALSO!!!\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import scipy.sparse\n",
    "\n",
    "# tfidf = Pipeline([\n",
    "#     ('vectorizer', HashingVectorizer(ngram_range=(1,4), non_negative=True)),\n",
    "#     ('tfidf', TfidfTransformer()),\n",
    "# ])\n",
    "\n",
    "title_vect = Pipeline([\n",
    "                     ('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ])\n",
    "\n",
    "X = train_df\n",
    "y_train = train_df['next_shares']\n",
    "X['test_split'] = 0\n",
    "\n",
    "test_page = 'SOML'\n",
    "# test_page = 'Nifty'\n",
    "# test_page = 'Tasty'\n",
    "test_df = train_df[train_df.next_page == test_page]\n",
    "y_test = test_df[['next_shares', 'external_id']]\n",
    "test_df['test_split'] = 1\n",
    "\n",
    "X = pd.concat([X,test_df])\n",
    "X = X[test_features + ['title', 'test_split']]\n",
    "####convert page_infos into dummies\n",
    "X = pd.get_dummies(data = X, columns=['page', 'next_page'])\n",
    "\n",
    "X_train = X[X.test_split == 0]\n",
    "X_test = X[X.test_split == 1]\n",
    "\n",
    "###train\n",
    "X_tfidf = title_vect.fit_transform(X_train['title'])\n",
    "X_other = X_train.drop(['title', 'test_split'], axis = 1)\n",
    "X_train = scipy.sparse.hstack([X_tfidf, X_other])\n",
    "\n",
    "###test\n",
    "X_tfidf = title_vect.transform(X_test['title'])\n",
    "X_other = X_test.drop(['title', 'test_split'], axis = 1)\n",
    "X_test = scipy.sparse.hstack([X_tfidf, X_other])\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "clf = DecisionTreeRegressor(max_depth = 10)\n",
    "print(\"TREE REGRESSION!\")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = metrics.r2_score(y_test['next_shares'], y_pred)\n",
    "print(\"Cross-Predicted Accuracy (R2):\", accuracy)\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"Mean Absolute Error: \", mean_absolute_error(y_test['next_shares'], y_pred))\n",
    "error_percent = mean_absolute_error(y_test['next_shares'], y_pred)/y_test['next_shares'].mean()\n",
    "print(\"Mean values (share): \", y_test['next_shares'].mean(), \" | percent error: \",  error_percent)\n",
    "\n",
    "###sort and see how many pair-wise predictions are correct!?\n",
    "order = pd.DataFrame()\n",
    "order = y_test\n",
    "# order['actual_shares'] = y_test\n",
    "order['pred_shares'] = y_pred\n",
    "\n",
    "order['actual_rank'] = order['next_shares'].rank(ascending=0)\n",
    "order['predicted_rank'] = order['pred_shares'].rank(ascending=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:  9999 ; Hits:  7605 ; Miss:  2394\n",
      "Accuracy:  0.7605760576057605\n"
     ]
    }
   ],
   "source": [
    "###randomly pick 2 external_ids and make predictions on which will do better than the other:\n",
    "limit =10000\n",
    "hits = 0\n",
    "miss = 0 \n",
    "total = 0\n",
    "\n",
    "for i in range(1,limit):\n",
    "    total += 1\n",
    "    trial = order[:100].sample(n=2)\n",
    "#     print(trial)\n",
    "    act1 = trial[:1].next_shares.values[0]\n",
    "    act2 = trial[1:2].next_shares.values[0]\n",
    "    pred1 = trial[:1].pred_shares.values[0]\n",
    "    pred2 = trial[1:2].pred_shares.values[0]\n",
    "    \n",
    "    if act1 >= act2:\n",
    "        if pred1 >= pred2:\n",
    "            hits += 1\n",
    "        else:\n",
    "            miss += 1\n",
    "    else:\n",
    "        if pred1 < pred2:\n",
    "            hits += 1\n",
    "        else:\n",
    "            miss += 1\n",
    "# test = test_df.sample(n = 2)\n",
    "print(\"Total: \", total, \"; Hits: \", hits, \"; Miss: \", miss)\n",
    "print(\"Accuracy: \", hits/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(588, 111)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_page = 'SOML'\n",
    "# test_page = 'Nifty'\n",
    "# test_page = 'Tasty'\n",
    "train_df[train_df.next_page == test_page].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BuzzFeed Food', 'BuzzFeed', 'BuzzFeed Video', 'Buy Me That',\n",
       "       'BuzzFeed Philippines', 'BuzzFeed Parents', 'BuzzFeed Australia',\n",
       "       'BuzzFeed News', 'BuzzFeed Scotland', 'BuzzFeed Entertainment',\n",
       "       'BuzzFeed UK News', 'Quizzes En Espaol', 'BuzzFeed Celeb', 'Tasty',\n",
       "       'BuzzFeed Geeky', 'BuzzFeed Health', 'BuzzFeed DIY', 'SOML',\n",
       "       'BuzzFeed Rewind', 'BuzzFeed Books', 'BuzzFeed Style',\n",
       "       'BuzzFeed UK', 'Obsessed by BuzzFeed', 'BuzzFeed Animals',\n",
       "       'Cocoa Butter', 'BuzzFeed Unsolved', 'BuzzFeed Science', 'Cheeky',\n",
       "       'BuzzFeed Mxico', 'BuzzFeed Ladylike', 'BuzzFeed Espaa',\n",
       "       'BuzzFeed Quiz', 'BuzzFeed Brasil', 'BuzzFeed Espaol',\n",
       "       'BuzzFeed Politics', 'BuzzFeed Oz Politics', 'BuzzFeed Japan',\n",
       "       'Another Round', 'Pero Like', 'BuzzFeed Community',\n",
       "       'BuzzFeed Canada', 'BuzzFeed Oz News', 'BuzzFeed LGBT',\n",
       "       'BuzzFeed Weddings', 'BuzzFeed France', 'Bien Tasty',\n",
       "       'BuzzFeed IRL', 'BuzzFeed UK Politics', 'Tasty Demais',\n",
       "       'Tasty Miam', 'Top Knot', 'BuzzFeed India', 'BuzzFeed Deutschland',\n",
       "       'Reasons to Smile', 'BuzzFeed World', 'BuzzFeed News BR', 'LOLA',\n",
       "       'BuzzFeed France News', 'Proper Tasty', 'BuzzFeed San Francisco',\n",
       "       'Nifty', 'BuzzFeed Japan News', 'BuzzFeed Tech', 'Buzzed Feed',\n",
       "       'BuzzFeed Partner', 'Tasty One-Pot', 'BuzzFeed Sweaty',\n",
       "       'Tasty Junior', 'BuzzFeed BFF', 'Oh Great, More Politics',\n",
       "       'Einfach Tasty', 'Ohmygod Yaaa', 'BuzzFeed Reader',\n",
       "       'Kristin Chirico', 'BuzzReads'], dtype=object)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.next_page.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####mode-report testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINEAR REGRESSION!\n",
      "Performance stats with page names, and also cross-promotion order included!\n",
      "No. of data-points =  38769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######FIT THE MODEL ON HISTORICAL DATA!!!\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.stats.api import ols\n",
    "from sklearn import datasets, linear_model\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "import math\n",
    "from sklearn import metrics\n",
    "\n",
    "filepath = '/Users/eshwarchandrasekharan/Desktop/repo/predict-pop/models-links/'\n",
    "train_df = pd.read_csv(filepath + 'radshift-links-previous-to-next-page-info.csv')\n",
    "# train_df = build_df\n",
    "train_df = train_df.fillna(0)\n",
    "\n",
    "one_hour_features = [\n",
    "#        'consumptions_by_type__link_clicks',\n",
    "#       'consumptions_by_type__other_clicks',\n",
    "#       'reactions_like_total',\n",
    "#       'stories_by_action_type__comment',\n",
    "#       'stories_by_action_type__like',\n",
    "       'stories_by_action_type__share', \n",
    "#         'engaged_fan',\n",
    "#       'fan_reach', \n",
    "#     'impressions',\n",
    "#       'impressions_fan',\n",
    "                ]\n",
    "\n",
    "cross_promotion_features = all_pages\n",
    "\n",
    "from sklearn import linear_model\n",
    "clf = linear_model.LinearRegression()\n",
    "print(\"LINEAR REGRESSION!\")\n",
    "\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# clf = DecisionTreeRegressor(max_depth = 5)\n",
    "# print(\"TREE REGRESSION!\")\n",
    "\n",
    "print(\"Performance stats with page names, and also cross-promotion order included!\")\n",
    "# print(\"WITH cross-promotion features\")\n",
    "\n",
    "page_infos = ['page', 'next_page']\n",
    "# page_infos = ['display_name', 'next_page']   ###mode report variables\n",
    "\n",
    "train_features = one_hour_features\n",
    "\n",
    "# include_page_info = 0\n",
    "include_page_info = 1\n",
    "if include_page_info == 1:\n",
    "  train_features = one_hour_features + page_infos\n",
    "# train_features = cross_promotion_features + one_hour_features\n",
    "\n",
    "log_scale = 1\n",
    "# log_scale = 0\n",
    "\n",
    "if log_scale == 1:\n",
    "    for feat in one_hour_features:\n",
    "        train_df[feat] = np.log(train_df[feat] + 1)\n",
    "\n",
    "cross_promote = 0\n",
    "# cross_promote = 1\n",
    "\n",
    "if cross_promote == 1:\n",
    "    for feat in cross_promotion_features:\n",
    "        train_features.append(feat)\n",
    "\n",
    "train_df['clicks_bucket'] = np.log(train_df['next_clicks'] + 1)\n",
    "train_df['share_bucket'] = np.log(train_df['next_shares']+1)\n",
    "\n",
    "print(\"No. of data-points = \", len(train_df))\n",
    "\n",
    "###generate DFs for analysis - X and Y\n",
    "X = train_df[train_features]\n",
    "y = train_df['share_bucket']\n",
    "# y = train_df['next_shares']\n",
    "####convert page_infos into dummies\n",
    "if include_page_info == 1:\n",
    "#   X = pd.get_dummies(data = X, columns=['display_name', 'next_page']) ###mode report variables\n",
    "  X = pd.get_dummies(data = X, columns=['page', 'next_page']) \n",
    "\n",
    "###test vector to be in this dimension space too!\n",
    "test_features = X.columns\n",
    "\n",
    "clf = clf.fit(X, y)\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>title</th>\n",
       "      <th>buzz_id</th>\n",
       "      <th>predicted_shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29785</th>\n",
       "      <td>Cheeky</td>\n",
       "      <td>Thousands Of Americans Crashed Canada's Immigr...</td>\n",
       "      <td>4393851.0</td>\n",
       "      <td>4505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18311</th>\n",
       "      <td>BuzzFeed Canada</td>\n",
       "      <td>This Is Already The Most Iconic Photo Of The T...</td>\n",
       "      <td>4465035.0</td>\n",
       "      <td>3776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31004</th>\n",
       "      <td>BuzzFeed Philippines</td>\n",
       "      <td>Anthony Bourdain's Manila Episode Of &amp;quot;Par...</td>\n",
       "      <td>4240567.0</td>\n",
       "      <td>3259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26826</th>\n",
       "      <td>BuzzFeed News</td>\n",
       "      <td>Dakota Access Pipeline Will Be Rerouted In A V...</td>\n",
       "      <td>4412034.0</td>\n",
       "      <td>3150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26479</th>\n",
       "      <td>BuzzFeed News</td>\n",
       "      <td>Ohio Passes Bill That Bans Abortion After A Mo...</td>\n",
       "      <td>4414644.0</td>\n",
       "      <td>2650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32705</th>\n",
       "      <td>BuzzFeed UK News</td>\n",
       "      <td>Angelina Jolie Has Filed For Divorce From Brad...</td>\n",
       "      <td>4359699.0</td>\n",
       "      <td>2463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21471</th>\n",
       "      <td>BuzzFeed Philippines</td>\n",
       "      <td>17 Questions Filipino Catholic Schools Need To...</td>\n",
       "      <td>3795841.0</td>\n",
       "      <td>2288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38682</th>\n",
       "      <td>Cheeky</td>\n",
       "      <td>People Are Loving This Woman's Mirror Finish C...</td>\n",
       "      <td>4244831.0</td>\n",
       "      <td>2193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38014</th>\n",
       "      <td>Cheeky</td>\n",
       "      <td>The \"Cheerio Challenge\" Is Proof We Are Living...</td>\n",
       "      <td>4281568.0</td>\n",
       "      <td>2034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18073</th>\n",
       "      <td>BuzzFeed Philippines</td>\n",
       "      <td>15 Times Filipinas Refused To Take Shit From T...</td>\n",
       "      <td>4439404.0</td>\n",
       "      <td>2027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       page  \\\n",
       "29785                Cheeky   \n",
       "18311       BuzzFeed Canada   \n",
       "31004  BuzzFeed Philippines   \n",
       "26826         BuzzFeed News   \n",
       "26479         BuzzFeed News   \n",
       "32705      BuzzFeed UK News   \n",
       "21471  BuzzFeed Philippines   \n",
       "38682                Cheeky   \n",
       "38014                Cheeky   \n",
       "18073  BuzzFeed Philippines   \n",
       "\n",
       "                                                   title    buzz_id  \\\n",
       "29785  Thousands Of Americans Crashed Canada's Immigr...  4393851.0   \n",
       "18311  This Is Already The Most Iconic Photo Of The T...  4465035.0   \n",
       "31004  Anthony Bourdain's Manila Episode Of &quot;Par...  4240567.0   \n",
       "26826  Dakota Access Pipeline Will Be Rerouted In A V...  4412034.0   \n",
       "26479  Ohio Passes Bill That Bans Abortion After A Mo...  4414644.0   \n",
       "32705  Angelina Jolie Has Filed For Divorce From Brad...  4359699.0   \n",
       "21471  17 Questions Filipino Catholic Schools Need To...  3795841.0   \n",
       "38682  People Are Loving This Woman's Mirror Finish C...  4244831.0   \n",
       "38014  The \"Cheerio Challenge\" Is Proof We Are Living...  4281568.0   \n",
       "18073  15 Times Filipinas Refused To Take Shit From T...  4439404.0   \n",
       "\n",
       "       predicted_shares  \n",
       "29785              4505  \n",
       "18311              3776  \n",
       "31004              3259  \n",
       "26826              3150  \n",
       "26479              2650  \n",
       "32705              2463  \n",
       "21471              2288  \n",
       "38682              2193  \n",
       "38014              2034  \n",
       "18073              2027  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####make predictions on performance if posted on the target page, given the perfoprmance in the previous page!\n",
    "# test_page = 'BuzzFeed Video'\n",
    "test_page = 'BuzzFeed'\n",
    "max_suggestions = 10\n",
    "test_df = train_df\n",
    "X_test = test_df[train_features]\n",
    "\n",
    "###save for later!\n",
    "# curr_page = X_test['page']\n",
    "\n",
    "if include_page_info == 1:\n",
    "#   X_test = pd.get_dummies(data = X_test, columns=['display_name', 'next_page'])\n",
    "  X_test = pd.get_dummies(data = X_test, columns=['page', 'next_page'])\n",
    "\n",
    "for page in test_df['next_page'].unique():\n",
    "  X_test['next_page_' + page] = 0\n",
    "\n",
    "X_test['next_page_' + test_page] = 1\n",
    "\n",
    "test_df['predicted_shares'] = clf.predict(X_test)\n",
    "test_df['predicted_shares'] = (np.exp(test_df['predicted_shares'])-1).astype(int)\n",
    "\n",
    "result_cols = ['page', 'title', 'buzz_id', 'predicted_shares']\n",
    "# result_cols = ['display_name', 'title', 'buzz_id', 'predicted_shares']\n",
    "test_df = test_df[~(test_df['page'] == test_page)]\n",
    "# test_df = test_df[~(test_df['display_name'] == test_page)]\n",
    "# ###only keep candidates that are from pages which have sourced test_page in the past!\n",
    "candidate_pages = test_df[test_df.next_page == test_page]['page'].unique()\n",
    "test_df = test_df[test_df['page'].isin(candidate_pages)]\n",
    "###\n",
    "test_df = test_df[result_cols].drop_duplicates(subset=['buzz_id'], keep = 'last')\n",
    "test_df.sort_values('predicted_shares', ascending = False)[:max_suggestions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using title features too!\n"
     ]
    }
   ],
   "source": [
    "print(\"Using title features too!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINEAR REGRESSION!\n",
      "Performance stats with page names, and also cross-promotion order included!\n",
      "No. of data-points =  38769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######FIT THE MODEL ON HISTORICAL DATA!!!\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.stats.api import ols\n",
    "from sklearn import datasets, linear_model\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "import math\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import scipy.sparse\n",
    "\n",
    "filepath = '/Users/eshwarchandrasekharan/Desktop/repo/predict-pop/models-links/'\n",
    "train_df = pd.read_csv(filepath + 'radshift-links-previous-to-next-page-info.csv')\n",
    "# train_df = build_df\n",
    "train_df = train_df.fillna(0)\n",
    "\n",
    "one_hour_features = [\n",
    "#        'consumptions_by_type__link_clicks',\n",
    "#       'consumptions_by_type__other_clicks',\n",
    "#       'reactions_like_total',\n",
    "#       'stories_by_action_type__comment',\n",
    "#       'stories_by_action_type__like',\n",
    "       'stories_by_action_type__share', \n",
    "#         'engaged_fan',\n",
    "#       'fan_reach', \n",
    "#     'impressions',\n",
    "#       'impressions_fan',\n",
    "                ]\n",
    "\n",
    "cross_promotion_features = all_pages\n",
    "\n",
    "from sklearn import linear_model\n",
    "clf = linear_model.LinearRegression()\n",
    "print(\"LINEAR REGRESSION!\")\n",
    "\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# clf = DecisionTreeRegressor(max_depth = 5)\n",
    "# print(\"TREE REGRESSION!\")\n",
    "\n",
    "print(\"Performance stats with page names, and also cross-promotion order included!\")\n",
    "# print(\"WITH cross-promotion features\")\n",
    "\n",
    "page_infos = ['page', 'next_page']\n",
    "# page_infos = ['display_name', 'next_page']   ###mode report variables\n",
    "\n",
    "train_features = one_hour_features\n",
    "\n",
    "# include_page_info = 0\n",
    "include_page_info = 1\n",
    "if include_page_info == 1:\n",
    "  train_features = one_hour_features + page_infos\n",
    "# train_features = cross_promotion_features + one_hour_features\n",
    "\n",
    "log_scale = 1\n",
    "# log_scale = 0\n",
    "\n",
    "if log_scale == 1:\n",
    "    for feat in one_hour_features:\n",
    "        train_df[feat] = np.log(train_df[feat] + 1)\n",
    "\n",
    "cross_promote = 0\n",
    "# cross_promote = 1\n",
    "\n",
    "if cross_promote == 1:\n",
    "    for feat in cross_promotion_features:\n",
    "        train_features.append(feat)\n",
    "\n",
    "train_df['clicks_bucket'] = np.log(train_df['next_clicks'] + 1)\n",
    "train_df['share_bucket'] = np.log(train_df['next_shares']+1)\n",
    "\n",
    "print(\"No. of data-points = \", len(train_df))\n",
    "\n",
    "###generate DFs for analysis - X and Y\n",
    "X = train_df[train_features + ['title']]\n",
    "y = train_df['share_bucket']\n",
    "# y = train_df['next_shares']\n",
    "####convert page_infos into dummies\n",
    "if include_page_info == 1:\n",
    "#   X = pd.get_dummies(data = X, columns=['display_name', 'next_page']) ###mode report variables\n",
    "  X = pd.get_dummies(data = X, columns=['page', 'next_page']) \n",
    "\n",
    "####include title features!\n",
    "title_vect = Pipeline([\n",
    "                     ('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ])\n",
    "\n",
    "##\n",
    "X_tfidf = title_vect.fit_transform(X['title'])\n",
    "X = X.drop(['title'], axis = 1)\n",
    "X = scipy.sparse.hstack([X_tfidf, X])\n",
    "\n",
    "clf = clf.fit(X, y)\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>title</th>\n",
       "      <th>buzz_id</th>\n",
       "      <th>predicted_shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29785</th>\n",
       "      <td>Cheeky</td>\n",
       "      <td>Thousands Of Americans Crashed Canada's Immigr...</td>\n",
       "      <td>4393851.0</td>\n",
       "      <td>10232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26826</th>\n",
       "      <td>BuzzFeed News</td>\n",
       "      <td>Dakota Access Pipeline Will Be Rerouted In A V...</td>\n",
       "      <td>4412034.0</td>\n",
       "      <td>7528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38733</th>\n",
       "      <td>BuzzFeed Health</td>\n",
       "      <td>24 Things Only People With Shit Eyesight Will ...</td>\n",
       "      <td>4239651.0</td>\n",
       "      <td>6594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23993</th>\n",
       "      <td>BuzzFeed Entertainment</td>\n",
       "      <td>Debbie Reynolds Dies One Day After Daughter Ca...</td>\n",
       "      <td>4430695.0</td>\n",
       "      <td>4699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29245</th>\n",
       "      <td>BuzzFeed World</td>\n",
       "      <td>Two People Dead After Magnitude-7.8 Earthquake...</td>\n",
       "      <td>4397061.0</td>\n",
       "      <td>4601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13921</th>\n",
       "      <td>BuzzFeed UK News</td>\n",
       "      <td>Sandi Toksvig And Noel Fielding Are The New Ho...</td>\n",
       "      <td>4489251.0</td>\n",
       "      <td>3992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38747</th>\n",
       "      <td>BuzzFeed Celeb</td>\n",
       "      <td>Kristen Bell And Mila Kunis Being Surprised By...</td>\n",
       "      <td>4240936.0</td>\n",
       "      <td>3826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24204</th>\n",
       "      <td>BuzzFeed News</td>\n",
       "      <td>&amp;quot;Star Wars&amp;quot; Icon Carrie Fisher Dies</td>\n",
       "      <td>4428606.0</td>\n",
       "      <td>3761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23006</th>\n",
       "      <td>BuzzFeed Philippines</td>\n",
       "      <td>20 Things You'll Only Understand If You're Sli...</td>\n",
       "      <td>4130135.0</td>\n",
       "      <td>3583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30008</th>\n",
       "      <td>BuzzFeed Oz Politics</td>\n",
       "      <td>The Senate Has Killed Off The Marriage Equalit...</td>\n",
       "      <td>4391870.0</td>\n",
       "      <td>3306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         page  \\\n",
       "29785                  Cheeky   \n",
       "26826           BuzzFeed News   \n",
       "38733         BuzzFeed Health   \n",
       "23993  BuzzFeed Entertainment   \n",
       "29245          BuzzFeed World   \n",
       "13921        BuzzFeed UK News   \n",
       "38747          BuzzFeed Celeb   \n",
       "24204           BuzzFeed News   \n",
       "23006    BuzzFeed Philippines   \n",
       "30008    BuzzFeed Oz Politics   \n",
       "\n",
       "                                                   title    buzz_id  \\\n",
       "29785  Thousands Of Americans Crashed Canada's Immigr...  4393851.0   \n",
       "26826  Dakota Access Pipeline Will Be Rerouted In A V...  4412034.0   \n",
       "38733  24 Things Only People With Shit Eyesight Will ...  4239651.0   \n",
       "23993  Debbie Reynolds Dies One Day After Daughter Ca...  4430695.0   \n",
       "29245  Two People Dead After Magnitude-7.8 Earthquake...  4397061.0   \n",
       "13921  Sandi Toksvig And Noel Fielding Are The New Ho...  4489251.0   \n",
       "38747  Kristen Bell And Mila Kunis Being Surprised By...  4240936.0   \n",
       "24204      &quot;Star Wars&quot; Icon Carrie Fisher Dies  4428606.0   \n",
       "23006  20 Things You'll Only Understand If You're Sli...  4130135.0   \n",
       "30008  The Senate Has Killed Off The Marriage Equalit...  4391870.0   \n",
       "\n",
       "       predicted_shares  \n",
       "29785             10232  \n",
       "26826              7528  \n",
       "38733              6594  \n",
       "23993              4699  \n",
       "29245              4601  \n",
       "13921              3992  \n",
       "38747              3826  \n",
       "24204              3761  \n",
       "23006              3583  \n",
       "30008              3306  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####make predictions on performance if posted on the target page, given the perfoprmance in the previous page!\n",
    "# test_page = 'BuzzFeed Video'\n",
    "test_page = 'BuzzFeed'\n",
    "max_suggestions = 10\n",
    "test_df = train_df\n",
    "X_test = test_df[train_features + ['title']]\n",
    "\n",
    "###save for later!\n",
    "# curr_page = X_test['page']\n",
    "\n",
    "if include_page_info == 1:\n",
    "#   X_test = pd.get_dummies(data = X_test, columns=['display_name', 'next_page'])\n",
    "  X_test = pd.get_dummies(data = X_test, columns=['page', 'next_page'])\n",
    "    \n",
    "for page in test_df['next_page'].unique():\n",
    "  X_test['next_page_' + page] = 0\n",
    "\n",
    "X_test['next_page_' + test_page] = 1\n",
    "\n",
    "##title features\n",
    "X_tfidf = title_vect.transform(X_test['title'])\n",
    "X_test = X_test.drop(['title'], axis = 1)\n",
    "X_test = scipy.sparse.hstack([X_tfidf, X_test])\n",
    "###\n",
    "\n",
    "test_df['predicted_shares'] = clf.predict(X_test)\n",
    "test_df['predicted_shares'] = (np.exp(test_df['predicted_shares'])-1).astype(int)\n",
    "\n",
    "result_cols = ['page', 'title', 'buzz_id', 'predicted_shares']\n",
    "# result_cols = ['display_name', 'title', 'buzz_id', 'predicted_shares']\n",
    "test_df = test_df[~(test_df['page'] == test_page)]\n",
    "# test_df = test_df[~(test_df['display_name'] == test_page)]\n",
    "# ###only keep candidates that are from pages which have sourced test_page in the past!\n",
    "candidate_pages = test_df[test_df.next_page == test_page]['page'].unique()\n",
    "test_df = test_df[test_df['page'].isin(candidate_pages)]\n",
    "###\n",
    "test_df = test_df[result_cols].drop_duplicates(subset=['buzz_id'], keep = 'last')\n",
    "test_df.sort_values('predicted_shares', ascending = False)[:max_suggestions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>title</th>\n",
       "      <th>buzz_id</th>\n",
       "      <th>predicted_shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29785</th>\n",
       "      <td>Cheeky</td>\n",
       "      <td>Thousands Of Americans Crashed Canada's Immigr...</td>\n",
       "      <td>4393851.0</td>\n",
       "      <td>3105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26826</th>\n",
       "      <td>BuzzFeed News</td>\n",
       "      <td>Dakota Access Pipeline Will Be Rerouted In A V...</td>\n",
       "      <td>4412034.0</td>\n",
       "      <td>2284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38733</th>\n",
       "      <td>BuzzFeed Health</td>\n",
       "      <td>24 Things Only People With Shit Eyesight Will ...</td>\n",
       "      <td>4239651.0</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23993</th>\n",
       "      <td>BuzzFeed Entertainment</td>\n",
       "      <td>Debbie Reynolds Dies One Day After Daughter Ca...</td>\n",
       "      <td>4430695.0</td>\n",
       "      <td>1425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29245</th>\n",
       "      <td>BuzzFeed World</td>\n",
       "      <td>Two People Dead After Magnitude-7.8 Earthquake...</td>\n",
       "      <td>4397061.0</td>\n",
       "      <td>1396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13921</th>\n",
       "      <td>BuzzFeed UK News</td>\n",
       "      <td>Sandi Toksvig And Noel Fielding Are The New Ho...</td>\n",
       "      <td>4489251.0</td>\n",
       "      <td>1211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38747</th>\n",
       "      <td>BuzzFeed Celeb</td>\n",
       "      <td>Kristen Bell And Mila Kunis Being Surprised By...</td>\n",
       "      <td>4240936.0</td>\n",
       "      <td>1160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24204</th>\n",
       "      <td>BuzzFeed News</td>\n",
       "      <td>&amp;quot;Star Wars&amp;quot; Icon Carrie Fisher Dies</td>\n",
       "      <td>4428606.0</td>\n",
       "      <td>1141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23006</th>\n",
       "      <td>BuzzFeed Philippines</td>\n",
       "      <td>20 Things You'll Only Understand If You're Sli...</td>\n",
       "      <td>4130135.0</td>\n",
       "      <td>1087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20725</th>\n",
       "      <td>BuzzFeed News</td>\n",
       "      <td>A Nationwide Ban On Abortions After A Month An...</td>\n",
       "      <td>4448375.0</td>\n",
       "      <td>1068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         page  \\\n",
       "29785                  Cheeky   \n",
       "26826           BuzzFeed News   \n",
       "38733         BuzzFeed Health   \n",
       "23993  BuzzFeed Entertainment   \n",
       "29245          BuzzFeed World   \n",
       "13921        BuzzFeed UK News   \n",
       "38747          BuzzFeed Celeb   \n",
       "24204           BuzzFeed News   \n",
       "23006    BuzzFeed Philippines   \n",
       "20725           BuzzFeed News   \n",
       "\n",
       "                                                   title    buzz_id  \\\n",
       "29785  Thousands Of Americans Crashed Canada's Immigr...  4393851.0   \n",
       "26826  Dakota Access Pipeline Will Be Rerouted In A V...  4412034.0   \n",
       "38733  24 Things Only People With Shit Eyesight Will ...  4239651.0   \n",
       "23993  Debbie Reynolds Dies One Day After Daughter Ca...  4430695.0   \n",
       "29245  Two People Dead After Magnitude-7.8 Earthquake...  4397061.0   \n",
       "13921  Sandi Toksvig And Noel Fielding Are The New Ho...  4489251.0   \n",
       "38747  Kristen Bell And Mila Kunis Being Surprised By...  4240936.0   \n",
       "24204      &quot;Star Wars&quot; Icon Carrie Fisher Dies  4428606.0   \n",
       "23006  20 Things You'll Only Understand If You're Sli...  4130135.0   \n",
       "20725  A Nationwide Ban On Abortions After A Month An...  4448375.0   \n",
       "\n",
       "       predicted_shares  \n",
       "29785              3105  \n",
       "26826              2284  \n",
       "38733              2001  \n",
       "23993              1425  \n",
       "29245              1396  \n",
       "13921              1211  \n",
       "38747              1160  \n",
       "24204              1141  \n",
       "23006              1087  \n",
       "20725              1068  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.sort_values('predicted_shares', ascending = False)[:max_suggestions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
