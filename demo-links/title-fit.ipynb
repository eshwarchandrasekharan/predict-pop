{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 links that are most relevant for FB  BuzzFeed Animals\n",
      "--------------\n",
      "This Quiz Will Determine If You Like Cats Or Dogs More\n",
      "17 Photos That’ll Make Every Pet Parent Say “Same\"\n",
      "This Boy And His Dog's Sweet Friendship Will Make You Feel All The Feels\n",
      "18 Puppies Who Are Up To No Good\n",
      "19 Things Only Cat Owners Know To Be True\n",
      "21 Puppies Who Won't Be Small For Long\n",
      "Which Puppy Should Give You A Pep Talk?\n",
      "Let These Puppies Decide What You Should Get For Lunch\n",
      "12 GIFs Of People Dancing Guaranteed To Make You Smile\n",
      "12 Motivational Posters That Are Way Too Real If You Own A Cat\n"
     ]
    }
   ],
   "source": [
    "######querying with Andrew's data - include error handling later (if clf doesn't exist for the page being queried)\n",
    "test_page = 'BuzzFeed Animals'\n",
    "ind = pages.index(test_page)\n",
    "clf = page_clf[ind]\n",
    "\n",
    "temp = lf_meta[~(lf_meta.page == test_page)]\n",
    "temp = temp[:1000]\n",
    "# temp = temp.sort_values('stats_date', ascending = False)[:1000]\n",
    "query_titles = temp['title'].unique()\n",
    "\n",
    "def get_title_fit(X, clf):\n",
    "     return clf.predict_proba(X)\n",
    "\n",
    "demo = pd.DataFrame()\n",
    "demo['title'] = query_titles\n",
    "temp = pd.DataFrame(clf.predict_proba(query_titles), columns=clf.classes_)\n",
    "demo['prob'] = temp[1]\n",
    "\n",
    "print(\"Top 10 links that are most relevant for FB \", test_page)\n",
    "print(\"--------------\")\n",
    "for nam in demo.sort_values('prob', ascending = False)[:10]['title']:\n",
    "    print(nam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 18874: expected 9 fields, saw 10\\nSkipping line 21002: expected 9 fields, saw 10\\nSkipping line 21355: expected 9 fields, saw 11\\n'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "num_links    3287.900000\n",
       "accuracy        0.770588\n",
       "precision       0.776932\n",
       "recall          0.770811\n",
       "fscore          0.768698\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###TASK 1 - LINKS (using Andrew's data)\n",
    "\n",
    "import pandas as pd\n",
    "# vf_stats = pd.read_csv('link_stats_jun20.csv')\n",
    "andrew_df = pd.read_csv('/Users/eshwarchandrasekharan/Downloads/post_metadata_7-31-17.csv', error_bad_lines = False)\n",
    "lf_meta = andrew_df\n",
    "lf_meta['title'] = lf_meta['web_app_post.name']\n",
    "lf_meta['page'] = lf_meta['display_name']\n",
    "####\n",
    "all_pages = lf_meta['page'].unique()\n",
    "\n",
    "###which page is best fit for a given just the title - \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "import numpy as np\n",
    "\n",
    "pages = []\n",
    "num_links = []\n",
    "average_acc = []\n",
    "average_pre = []\n",
    "average_rec = []\n",
    "average_f1 = []\n",
    "\n",
    "cv_folds = 10\n",
    "results = pd.DataFrame()\n",
    "page_clf = []\n",
    "\n",
    "df = lf_meta[['title', 'page']].dropna()\n",
    "\n",
    "for test_page in all_pages:\n",
    "#     test_page = \"BuzzFeed\"\n",
    "    X_1 = df[df.page == test_page]\n",
    "    X_0 = df[~(df.page == test_page)].sample(n=len(X_1))\n",
    "    if len(X_1) < 2.5*cv_folds:\n",
    "        continue\n",
    "    X_1 = X_1[['title', 'page']]\n",
    "    X_0 = X_0[['title', 'page']]\n",
    "    X_all = pd.concat([X_1, X_0])\n",
    "    \n",
    "#     print(len(X_1), \":\", len(X_0))\n",
    "    def get_class(X, page):\n",
    "        if X == page:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    X_all['class'] = X_all['page'].apply(get_class, page = test_page)\n",
    "#     X_all = X_all.dropna()\n",
    "    text_clf = Pipeline([\n",
    "                         ('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "#                          ('featselect', SelectKBest(k = 100)),\n",
    "                         ('clf', MultinomialNB()),\n",
    "#                          ('clf', LinearSVC()),\n",
    "#                          ('clf', RandomForestClassifier()),\n",
    "#                          ('clf', LogisticRegression()),\n",
    "    ])\n",
    "    fold = 0\n",
    "    kf = KFold(n_splits = cv_folds, shuffle = True)\n",
    "\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    fscore = []\n",
    "\n",
    "    X = X_all['title']\n",
    "    y = X_all['class']\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "#         print(\"Fold = \", fold)\n",
    "        fold+= 1\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        text_clf.fit(X_train, y_train)\n",
    "        y_pred = text_clf.predict(X_test)\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        prec, rec, fs, sup = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "        accuracy.append(acc)\n",
    "        precision.append(prec)\n",
    "        recall.append(rec)\n",
    "        fscore.append(fs)\n",
    "#     print(\"Page: \", test_page, \"#links: \", len(X_1), \"Accuracy: \", np.mean(accuracy), \"Precision: \", np.mean(precision), \"Recall: \", np.mean(recall), \"Fscore:\", np.mean(fscore))\n",
    "    num_links.append(len(X_1))\n",
    "    average_acc.append(np.mean(accuracy))\n",
    "    average_pre.append(np.mean(precision))\n",
    "    average_rec.append(np.mean(recall))\n",
    "    average_f1.append(np.mean(fscore))\n",
    "    pages.append(test_page)\n",
    "    page_clf.append(text_clf)\n",
    "results['page'] = pages\n",
    "results['num_links'] = num_links\n",
    "results['accuracy'] = average_acc\n",
    "results['precision'] = average_pre\n",
    "results['recall'] = average_rec\n",
    "results['fscore'] = average_f1\n",
    "\n",
    "results.mean()\n",
    "\n",
    "### First page promotion - previous dataset\n",
    "###Total data points: 85591\n",
    "# num_pages: 85\n",
    "# num_links    1448.000000\n",
    "# accuracy        0.789186\n",
    "# precision       0.811785\n",
    "# recall          0.792267\n",
    "# fscore          0.778326\n",
    "\n",
    "#####test with Andrew's data (98637 rows)\n",
    "# num_links    3287.900000\n",
    "# accuracy        0.768146\n",
    "# precision       0.774221\n",
    "# recall          0.768273\n",
    "# fscore          0.766238"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((98637, 11), (98637, 11))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "andrew_df.shape, lf_meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eshwarchandrasekharan/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "num_links    1448.000000\n",
       "accuracy        0.789857\n",
       "precision       0.815148\n",
       "recall          0.792735\n",
       "fscore          0.779524\n",
       "dtype: float64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###TASK 1 - LINKS\n",
    "import pandas as pd\n",
    "# vf_stats = pd.read_csv('link_stats_jun20.csv')\n",
    "lf_meta = pd.read_csv('../models-links/radshift_links_2016_2017.csv')\n",
    "\n",
    "all_pages = lf_meta['page'].unique()\n",
    "\n",
    "###which page is best fit for a given just the title - \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "import numpy as np\n",
    "\n",
    "pages = []\n",
    "num_links = []\n",
    "average_acc = []\n",
    "average_pre = []\n",
    "average_rec = []\n",
    "average_f1 = []\n",
    "\n",
    "cv_folds = 10\n",
    "results = pd.DataFrame()\n",
    "page_clf = []\n",
    "\n",
    "df = lf_meta[['title', 'page']].dropna()\n",
    "\n",
    "for test_page in all_pages:\n",
    "#     test_page = \"BuzzFeed\"\n",
    "    X_1 = df[df.page == test_page]\n",
    "    X_0 = df[~(df.page == test_page)].sample(n=len(X_1))\n",
    "    if len(X_1) < 2.5*cv_folds:\n",
    "        continue\n",
    "    X_1 = X_1[['title', 'page']]\n",
    "    X_0 = X_0[['title', 'page']]\n",
    "    X_all = pd.concat([X_1, X_0])\n",
    "    \n",
    "#     print(len(X_1), \":\", len(X_0))\n",
    "    def get_class(X, page):\n",
    "        if X == page:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    X_all['class'] = X_all['page'].apply(get_class, page = test_page)\n",
    "#     X_all = X_all.dropna()\n",
    "    text_clf = Pipeline([\n",
    "                         ('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "#                          ('featselect', SelectKBest(k = 100)),\n",
    "                         ('clf', MultinomialNB()),\n",
    "#                          ('clf', LinearSVC()),\n",
    "#                          ('clf', RandomForestClassifier()),\n",
    "#                          ('clf', LogisticRegression()),\n",
    "    ])\n",
    "    fold = 0\n",
    "    kf = KFold(n_splits = cv_folds, shuffle = True)\n",
    "\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    fscore = []\n",
    "\n",
    "    X = X_all['title']\n",
    "    y = X_all['class']\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "#         print(\"Fold = \", fold)\n",
    "        fold+= 1\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        text_clf.fit(X_train, y_train)\n",
    "        y_pred = text_clf.predict(X_test)\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        prec, rec, fs, sup = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "        accuracy.append(acc)\n",
    "        precision.append(prec)\n",
    "        recall.append(rec)\n",
    "        fscore.append(fs)\n",
    "#     print(\"Page: \", test_page, \"#links: \", len(X_1), \"Accuracy: \", np.mean(accuracy), \"Precision: \", np.mean(precision), \"Recall: \", np.mean(recall), \"Fscore:\", np.mean(fscore))\n",
    "    num_links.append(len(X_1))\n",
    "    average_acc.append(np.mean(accuracy))\n",
    "    average_pre.append(np.mean(precision))\n",
    "    average_rec.append(np.mean(recall))\n",
    "    average_f1.append(np.mean(fscore))\n",
    "    pages.append(test_page)\n",
    "    page_clf.append(text_clf)\n",
    "results['page'] = pages\n",
    "results['num_links'] = num_links\n",
    "results['accuracy'] = average_acc\n",
    "results['precision'] = average_pre\n",
    "results['recall'] = average_rec\n",
    "results['fscore'] = average_f1\n",
    "\n",
    "results.mean()\n",
    "\n",
    "### First page promotion\n",
    "###Total data points: 85591\n",
    "# num_pages: 85\n",
    "# num_links    1448.000000\n",
    "# accuracy        0.789186\n",
    "# precision       0.811785\n",
    "# recall          0.792267\n",
    "# fscore          0.778326\n",
    "\n",
    "#####test with Andrew's data (98637 rows)\n",
    "# num_links    3287.900000\n",
    "# accuracy        0.768146\n",
    "# precision       0.774221\n",
    "# recall          0.768273\n",
    "# fscore          0.766238"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best first page for item titled:  ['5 DIY Soap Ideas']\n",
      "BuzzFeed Partner 0.829435684612\n",
      "Cheeky 0.225256428384\n",
      "BuzzFeed DIY 0.888881222809\n",
      "BuzzFeed Entertainment 0.249794354286\n",
      "BuzzFeed World 0.24772764345\n",
      "BuzzFeed Rewind 0.236342542472\n",
      "BuzzFeed Books 0.417053300364\n",
      "Cocoa Butter 0.187122964563\n",
      "BuzzFeed Health 0.477480940329\n",
      "BuzzFeed 0.392046522428\n",
      "BuzzFeed Celeb 0.173027283558\n",
      "BuzzFeed Science 0.273454556059\n",
      "BuzzFeed News 0.0868352148482\n",
      "BuzzFeed Parents 0.631152267097\n",
      "BuzzFeed Australia 0.293100657078\n",
      "BuzzFeed Style 0.664991707289\n",
      "BuzzFeed Community 0.646891689936\n",
      "BuzzFeed Quiz 0.172516538375\n",
      "BuzzFeed Politics 0.142905670186\n",
      "BuzzFeed Animals 0.400222672277\n",
      "Buy Me That 0.396477915068\n",
      "BuzzFeed Food 0.714112699956\n",
      "BuzzFeed UK 0.491900620884\n",
      "SOML 0.585315545586\n",
      "BuzzFeed LGBT 0.387019351993\n",
      "BuzzFeed Tech 0.144868570324\n",
      "Obsessed by BuzzFeed 0.178198090614\n",
      "Tasty 0.591989285159\n",
      "BuzzFeed Video 0.216898516493\n",
      "BuzzFeed Canada 0.6614710383\n",
      "Best fit for this link would be:  BuzzFeed DIY ; prob =  0.888881222809\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#####Query by page and title of link - Should a link with this title be posted on this BF page?\n",
    "# q_page = \"LOLA\"\n",
    "q_page = \"BuzzFeed Quiz\"\n",
    "# q_page = \"BuzzFeed Japan\"\n",
    "\n",
    "q_test = [\"5 DIY Soap Ideas\"]\n",
    "# q_test = ['Camping Meal Prep Hacks']\n",
    "# q_test = [\"People Try Kardashian Beauty Secrets\"]\n",
    "# q_test = ['This Restaurant In Mumbai Is Serving Up Goth Tikka']  ###list of titles you wanna query\n",
    "# q_test = [\"Find Out What your spirit animal is?\"]  ###list of titles you wanna query\n",
    "\n",
    "max_prob = 0\n",
    "max_page = \"\"\n",
    "\n",
    "print(\"Finding best first page for item titled: \", q_test)\n",
    "\n",
    "for pg in all_pages:\n",
    "    try:\n",
    "        clf = page_clf[pages.index(pg)]\n",
    "    except:\n",
    "#         print(\"Woops, the model for page \", pg, \" is still in the works!\")\n",
    "        continue\n",
    "        \n",
    "    prob = clf.predict_proba(q_test)[0][1]\n",
    "    print(pg, prob)\n",
    "    if prob > max_prob:\n",
    "        max_prob = prob\n",
    "        max_page = pg\n",
    "    \n",
    "print(\"Best fit for this link would be: \", max_page, \"; prob = \", max_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 links that are most relevant for FB  Tasty\n",
      "--------------\n",
      "You Can Only Eat Pasta If You Get 12/15 In This Quiz\n",
      "7 Dinners Under $10 You Should Make This Week\n",
      "Only A True Pizza Expert Can Identify Which Restaurant These Pizzas Are From Based On A Picture\n",
      "7 Small Ways To Actually Get Your Home In Order\n",
      "How Much Do You Actually Know About Cooking Eggs?\n",
      "23 Cheap Products That'll Actually Keep Your Kitchen Organized\n",
      "We Know Which Job You Should Actually Have Based On Your Apartment Preferences\n",
      "Pick Seven Of Your Favorite Foods And We'll Reveal What Your Life Will Be Like In 10 Years\n",
      "Your Proposal Preferences Will Reveal When You'll Get Engaged\n",
      "Tell Us Eight Of Your Food Preferences And We'll Reveal Where You Should Move To\n"
     ]
    }
   ],
   "source": [
    "test_page = 'Tasty'\n",
    "ind = pages.index(test_page)\n",
    "clf = page_clf[ind]\n",
    "\n",
    "temp = lf_meta[~(lf_meta.page == test_page)]\n",
    "temp = temp.sort_values('stats_date', ascending = False)[:1000]\n",
    "query_titles = temp['title'].unique()\n",
    "\n",
    "def get_title_fit(X, clf):\n",
    "     return clf.predict_proba(X)\n",
    "\n",
    "demo = pd.DataFrame()\n",
    "demo['title'] = query_titles\n",
    "temp = pd.DataFrame(clf.predict_proba(query_titles), columns=clf.classes_)\n",
    "demo['prob'] = temp[1]\n",
    "\n",
    "print(\"Top 10 links that are most relevant for FB \", test_page)\n",
    "print(\"--------------\")\n",
    "for nam in demo.sort_values('prob', ascending = False)[:10]['title']:\n",
    "    print(nam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 links that are most relevant for FB  BuzzFeed Animals\n",
      "--------------\n",
      "This Quiz Will Determine If You Like Cats Or Dogs More\n",
      "18 Puppies Who Are Up To No Good\n",
      "17 Photos That’ll Make Every Pet Parent Say “Same\"\n",
      "19 Things Only Cat Owners Know To Be True\n",
      "The Starving Sea Lion Pup Found Sleeping In A Restaurant Has A Good Chance Of Surviving\n",
      "This Boy And His Dog's Sweet Friendship Will Make You Feel All The Feels\n",
      "18 Small Things That Will Make You Smile\n",
      "21 Puppies Who Won't Be Small For Long\n",
      "This Adorable House-Broken Bison Found Her Forever Home\n",
      "17 Photos That Will Make Any Broke Person Laugh Then Cry\n"
     ]
    }
   ],
   "source": [
    "test_page = 'BuzzFeed Animals'\n",
    "ind = pages.index(test_page)\n",
    "clf = page_clf[ind]\n",
    "\n",
    "temp = lf_meta[~(lf_meta.page == test_page)]\n",
    "temp = temp[:1000]\n",
    "temp = temp.sort_values('stats_date', ascending = False)[:1000]\n",
    "query_titles = temp['title'].unique()\n",
    "\n",
    "def get_title_fit(X, clf):\n",
    "     return clf.predict_proba(X)\n",
    "\n",
    "demo = pd.DataFrame()\n",
    "demo['title'] = query_titles\n",
    "temp = pd.DataFrame(clf.predict_proba(query_titles), columns=clf.classes_)\n",
    "demo['prob'] = temp[1]\n",
    "\n",
    "print(\"Top 10 links that are most relevant for FB \", test_page)\n",
    "print(\"--------------\")\n",
    "for nam in demo.sort_values('prob', ascending = False)[:10]['title']:\n",
    "    print(nam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>num_links</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>fscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BuzzFeed Rewind</td>\n",
       "      <td>2106</td>\n",
       "      <td>0.769468</td>\n",
       "      <td>0.804743</td>\n",
       "      <td>0.770652</td>\n",
       "      <td>0.762497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BuzzFeed UK</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.670780</td>\n",
       "      <td>0.704175</td>\n",
       "      <td>0.671220</td>\n",
       "      <td>0.656821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BuzzFeed</td>\n",
       "      <td>10435</td>\n",
       "      <td>0.617729</td>\n",
       "      <td>0.663590</td>\n",
       "      <td>0.617901</td>\n",
       "      <td>0.588998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BuzzFeed Food</td>\n",
       "      <td>2855</td>\n",
       "      <td>0.815061</td>\n",
       "      <td>0.837122</td>\n",
       "      <td>0.815317</td>\n",
       "      <td>0.811916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BuzzFeed Entertainment</td>\n",
       "      <td>2791</td>\n",
       "      <td>0.792731</td>\n",
       "      <td>0.807137</td>\n",
       "      <td>0.792996</td>\n",
       "      <td>0.790220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BuzzFeed Books</td>\n",
       "      <td>1978</td>\n",
       "      <td>0.747216</td>\n",
       "      <td>0.783619</td>\n",
       "      <td>0.747919</td>\n",
       "      <td>0.738632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BuzzFeed Science</td>\n",
       "      <td>269</td>\n",
       "      <td>0.778826</td>\n",
       "      <td>0.781032</td>\n",
       "      <td>0.781168</td>\n",
       "      <td>0.776306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BuzzFeed Parents</td>\n",
       "      <td>940</td>\n",
       "      <td>0.703191</td>\n",
       "      <td>0.764443</td>\n",
       "      <td>0.703677</td>\n",
       "      <td>0.684662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tasty</td>\n",
       "      <td>972</td>\n",
       "      <td>0.774716</td>\n",
       "      <td>0.813309</td>\n",
       "      <td>0.774685</td>\n",
       "      <td>0.766898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BuzzFeed Australia</td>\n",
       "      <td>5360</td>\n",
       "      <td>0.641978</td>\n",
       "      <td>0.679455</td>\n",
       "      <td>0.642291</td>\n",
       "      <td>0.622433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>BuzzFeed Animals</td>\n",
       "      <td>1758</td>\n",
       "      <td>0.805439</td>\n",
       "      <td>0.843739</td>\n",
       "      <td>0.805828</td>\n",
       "      <td>0.799731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BuzzFeed UK Politics</td>\n",
       "      <td>514</td>\n",
       "      <td>0.883286</td>\n",
       "      <td>0.894788</td>\n",
       "      <td>0.885061</td>\n",
       "      <td>0.881146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>BuzzFeed Quiz</td>\n",
       "      <td>3581</td>\n",
       "      <td>0.920971</td>\n",
       "      <td>0.926374</td>\n",
       "      <td>0.921143</td>\n",
       "      <td>0.920640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>BuzzFeed France</td>\n",
       "      <td>1564</td>\n",
       "      <td>0.984977</td>\n",
       "      <td>0.985295</td>\n",
       "      <td>0.985109</td>\n",
       "      <td>0.984930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>BuzzFeed DIY</td>\n",
       "      <td>1553</td>\n",
       "      <td>0.809408</td>\n",
       "      <td>0.822333</td>\n",
       "      <td>0.809895</td>\n",
       "      <td>0.807355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>BuzzFeed Brasil</td>\n",
       "      <td>4089</td>\n",
       "      <td>0.988506</td>\n",
       "      <td>0.988652</td>\n",
       "      <td>0.988598</td>\n",
       "      <td>0.988500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Tasty Demais</td>\n",
       "      <td>171</td>\n",
       "      <td>0.941513</td>\n",
       "      <td>0.947132</td>\n",
       "      <td>0.943001</td>\n",
       "      <td>0.940169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BuzzFeed Español</td>\n",
       "      <td>2076</td>\n",
       "      <td>0.967727</td>\n",
       "      <td>0.969461</td>\n",
       "      <td>0.968003</td>\n",
       "      <td>0.967638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BuzzFeed Philippines</td>\n",
       "      <td>946</td>\n",
       "      <td>0.675970</td>\n",
       "      <td>0.700548</td>\n",
       "      <td>0.676803</td>\n",
       "      <td>0.665959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>BuzzFeed Health</td>\n",
       "      <td>1732</td>\n",
       "      <td>0.734112</td>\n",
       "      <td>0.767647</td>\n",
       "      <td>0.735023</td>\n",
       "      <td>0.725676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Buy Me That</td>\n",
       "      <td>1210</td>\n",
       "      <td>0.848347</td>\n",
       "      <td>0.865806</td>\n",
       "      <td>0.848477</td>\n",
       "      <td>0.846454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>BuzzFeed India</td>\n",
       "      <td>2635</td>\n",
       "      <td>0.797723</td>\n",
       "      <td>0.815729</td>\n",
       "      <td>0.797772</td>\n",
       "      <td>0.794714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>BuzzFeed Ladylike</td>\n",
       "      <td>304</td>\n",
       "      <td>0.651148</td>\n",
       "      <td>0.699049</td>\n",
       "      <td>0.652910</td>\n",
       "      <td>0.627205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>BuzzFeed Community</td>\n",
       "      <td>1174</td>\n",
       "      <td>0.722319</td>\n",
       "      <td>0.764617</td>\n",
       "      <td>0.722482</td>\n",
       "      <td>0.709405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>BuzzFeed Deutschland</td>\n",
       "      <td>1435</td>\n",
       "      <td>0.983624</td>\n",
       "      <td>0.983666</td>\n",
       "      <td>0.983859</td>\n",
       "      <td>0.983551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>SOML</td>\n",
       "      <td>816</td>\n",
       "      <td>0.717526</td>\n",
       "      <td>0.752415</td>\n",
       "      <td>0.719081</td>\n",
       "      <td>0.706952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>BuzzFeed Scotland</td>\n",
       "      <td>253</td>\n",
       "      <td>0.697647</td>\n",
       "      <td>0.745131</td>\n",
       "      <td>0.696959</td>\n",
       "      <td>0.678102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>BuzzFeed News</td>\n",
       "      <td>4954</td>\n",
       "      <td>0.798447</td>\n",
       "      <td>0.804474</td>\n",
       "      <td>0.798428</td>\n",
       "      <td>0.797363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Quizzes En Español</td>\n",
       "      <td>460</td>\n",
       "      <td>0.948913</td>\n",
       "      <td>0.953396</td>\n",
       "      <td>0.948926</td>\n",
       "      <td>0.947980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>BuzzFeed Video</td>\n",
       "      <td>2764</td>\n",
       "      <td>0.624458</td>\n",
       "      <td>0.673181</td>\n",
       "      <td>0.624973</td>\n",
       "      <td>0.596196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Bien Tasty</td>\n",
       "      <td>63</td>\n",
       "      <td>0.960256</td>\n",
       "      <td>0.963750</td>\n",
       "      <td>0.960417</td>\n",
       "      <td>0.957325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Cocoa Butter</td>\n",
       "      <td>1118</td>\n",
       "      <td>0.733911</td>\n",
       "      <td>0.758114</td>\n",
       "      <td>0.734424</td>\n",
       "      <td>0.726842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>BuzzFeed Canada</td>\n",
       "      <td>884</td>\n",
       "      <td>0.635198</td>\n",
       "      <td>0.664674</td>\n",
       "      <td>0.637118</td>\n",
       "      <td>0.617649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>BuzzFeed España</td>\n",
       "      <td>763</td>\n",
       "      <td>0.948886</td>\n",
       "      <td>0.953090</td>\n",
       "      <td>0.949270</td>\n",
       "      <td>0.948358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>LOLA</td>\n",
       "      <td>86</td>\n",
       "      <td>0.935948</td>\n",
       "      <td>0.950216</td>\n",
       "      <td>0.927460</td>\n",
       "      <td>0.930758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>BuzzFeed Unsolved</td>\n",
       "      <td>45</td>\n",
       "      <td>0.744444</td>\n",
       "      <td>0.777262</td>\n",
       "      <td>0.724583</td>\n",
       "      <td>0.694624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>BuzzFeed México</td>\n",
       "      <td>2086</td>\n",
       "      <td>0.967877</td>\n",
       "      <td>0.969727</td>\n",
       "      <td>0.967890</td>\n",
       "      <td>0.967762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>BuzzFeed Celeb</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.772335</td>\n",
       "      <td>0.789112</td>\n",
       "      <td>0.772822</td>\n",
       "      <td>0.768506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>BuzzFeed Japan</td>\n",
       "      <td>430</td>\n",
       "      <td>0.768605</td>\n",
       "      <td>0.863731</td>\n",
       "      <td>0.787370</td>\n",
       "      <td>0.736412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>BuzzFeed Style</td>\n",
       "      <td>2702</td>\n",
       "      <td>0.752592</td>\n",
       "      <td>0.796655</td>\n",
       "      <td>0.753148</td>\n",
       "      <td>0.742988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>BuzzFeed Oz Politics</td>\n",
       "      <td>647</td>\n",
       "      <td>0.883298</td>\n",
       "      <td>0.893605</td>\n",
       "      <td>0.884491</td>\n",
       "      <td>0.881595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Obsessed by BuzzFeed</td>\n",
       "      <td>1320</td>\n",
       "      <td>0.740530</td>\n",
       "      <td>0.764818</td>\n",
       "      <td>0.742168</td>\n",
       "      <td>0.734311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Pero Like</td>\n",
       "      <td>184</td>\n",
       "      <td>0.666066</td>\n",
       "      <td>0.682928</td>\n",
       "      <td>0.664882</td>\n",
       "      <td>0.651492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Proper Tasty</td>\n",
       "      <td>34</td>\n",
       "      <td>0.676190</td>\n",
       "      <td>0.709167</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.636724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>BuzzFeed UK News</td>\n",
       "      <td>944</td>\n",
       "      <td>0.797155</td>\n",
       "      <td>0.808316</td>\n",
       "      <td>0.797796</td>\n",
       "      <td>0.794982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Nifty</td>\n",
       "      <td>37</td>\n",
       "      <td>0.694643</td>\n",
       "      <td>0.702262</td>\n",
       "      <td>0.727500</td>\n",
       "      <td>0.664701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>BuzzFeed Geeky</td>\n",
       "      <td>444</td>\n",
       "      <td>0.686977</td>\n",
       "      <td>0.726745</td>\n",
       "      <td>0.688462</td>\n",
       "      <td>0.671311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Cheeky</td>\n",
       "      <td>1863</td>\n",
       "      <td>0.810513</td>\n",
       "      <td>0.821800</td>\n",
       "      <td>0.810609</td>\n",
       "      <td>0.808498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>BuzzFeed Weddings</td>\n",
       "      <td>557</td>\n",
       "      <td>0.718106</td>\n",
       "      <td>0.772345</td>\n",
       "      <td>0.719140</td>\n",
       "      <td>0.702886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>BuzzFeed Politics</td>\n",
       "      <td>929</td>\n",
       "      <td>0.914964</td>\n",
       "      <td>0.918847</td>\n",
       "      <td>0.915218</td>\n",
       "      <td>0.914379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>BuzzFeed LGBT</td>\n",
       "      <td>535</td>\n",
       "      <td>0.759813</td>\n",
       "      <td>0.780890</td>\n",
       "      <td>0.761218</td>\n",
       "      <td>0.754587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>BuzzFeed World</td>\n",
       "      <td>625</td>\n",
       "      <td>0.801600</td>\n",
       "      <td>0.810783</td>\n",
       "      <td>0.801298</td>\n",
       "      <td>0.798871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>BuzzFeed Partner</td>\n",
       "      <td>137</td>\n",
       "      <td>0.664418</td>\n",
       "      <td>0.695175</td>\n",
       "      <td>0.666962</td>\n",
       "      <td>0.640130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>BuzzFeed France News</td>\n",
       "      <td>179</td>\n",
       "      <td>0.974841</td>\n",
       "      <td>0.971949</td>\n",
       "      <td>0.976569</td>\n",
       "      <td>0.973067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>BuzzFeed Tech</td>\n",
       "      <td>151</td>\n",
       "      <td>0.824086</td>\n",
       "      <td>0.837742</td>\n",
       "      <td>0.822743</td>\n",
       "      <td>0.819099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>BuzzFeed Japan News</td>\n",
       "      <td>199</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.872741</td>\n",
       "      <td>0.792331</td>\n",
       "      <td>0.736503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>BuzzFeed Oz News</td>\n",
       "      <td>337</td>\n",
       "      <td>0.746159</td>\n",
       "      <td>0.760536</td>\n",
       "      <td>0.748196</td>\n",
       "      <td>0.742226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>BuzzFeed News BR</td>\n",
       "      <td>336</td>\n",
       "      <td>0.968723</td>\n",
       "      <td>0.970079</td>\n",
       "      <td>0.968755</td>\n",
       "      <td>0.968190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>BuzzFeed Reader</td>\n",
       "      <td>36</td>\n",
       "      <td>0.753571</td>\n",
       "      <td>0.788929</td>\n",
       "      <td>0.786667</td>\n",
       "      <td>0.742976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      page  num_links  accuracy  precision    recall    fscore\n",
       "0          BuzzFeed Rewind       2106  0.769468   0.804743  0.770652  0.762497\n",
       "1              BuzzFeed UK       4096  0.670780   0.704175  0.671220  0.656821\n",
       "2                 BuzzFeed      10435  0.617729   0.663590  0.617901  0.588998\n",
       "3            BuzzFeed Food       2855  0.815061   0.837122  0.815317  0.811916\n",
       "4   BuzzFeed Entertainment       2791  0.792731   0.807137  0.792996  0.790220\n",
       "5           BuzzFeed Books       1978  0.747216   0.783619  0.747919  0.738632\n",
       "6         BuzzFeed Science        269  0.778826   0.781032  0.781168  0.776306\n",
       "7         BuzzFeed Parents        940  0.703191   0.764443  0.703677  0.684662\n",
       "8                    Tasty        972  0.774716   0.813309  0.774685  0.766898\n",
       "9       BuzzFeed Australia       5360  0.641978   0.679455  0.642291  0.622433\n",
       "10        BuzzFeed Animals       1758  0.805439   0.843739  0.805828  0.799731\n",
       "11    BuzzFeed UK Politics        514  0.883286   0.894788  0.885061  0.881146\n",
       "12           BuzzFeed Quiz       3581  0.920971   0.926374  0.921143  0.920640\n",
       "13         BuzzFeed France       1564  0.984977   0.985295  0.985109  0.984930\n",
       "14            BuzzFeed DIY       1553  0.809408   0.822333  0.809895  0.807355\n",
       "15         BuzzFeed Brasil       4089  0.988506   0.988652  0.988598  0.988500\n",
       "16            Tasty Demais        171  0.941513   0.947132  0.943001  0.940169\n",
       "17        BuzzFeed Español       2076  0.967727   0.969461  0.968003  0.967638\n",
       "18    BuzzFeed Philippines        946  0.675970   0.700548  0.676803  0.665959\n",
       "19         BuzzFeed Health       1732  0.734112   0.767647  0.735023  0.725676\n",
       "20             Buy Me That       1210  0.848347   0.865806  0.848477  0.846454\n",
       "21          BuzzFeed India       2635  0.797723   0.815729  0.797772  0.794714\n",
       "22       BuzzFeed Ladylike        304  0.651148   0.699049  0.652910  0.627205\n",
       "23      BuzzFeed Community       1174  0.722319   0.764617  0.722482  0.709405\n",
       "24    BuzzFeed Deutschland       1435  0.983624   0.983666  0.983859  0.983551\n",
       "25                    SOML        816  0.717526   0.752415  0.719081  0.706952\n",
       "26       BuzzFeed Scotland        253  0.697647   0.745131  0.696959  0.678102\n",
       "27           BuzzFeed News       4954  0.798447   0.804474  0.798428  0.797363\n",
       "28      Quizzes En Español        460  0.948913   0.953396  0.948926  0.947980\n",
       "29          BuzzFeed Video       2764  0.624458   0.673181  0.624973  0.596196\n",
       "30              Bien Tasty         63  0.960256   0.963750  0.960417  0.957325\n",
       "31            Cocoa Butter       1118  0.733911   0.758114  0.734424  0.726842\n",
       "32         BuzzFeed Canada        884  0.635198   0.664674  0.637118  0.617649\n",
       "33         BuzzFeed España        763  0.948886   0.953090  0.949270  0.948358\n",
       "34                    LOLA         86  0.935948   0.950216  0.927460  0.930758\n",
       "35       BuzzFeed Unsolved         45  0.744444   0.777262  0.724583  0.694624\n",
       "36         BuzzFeed México       2086  0.967877   0.969727  0.967890  0.967762\n",
       "37          BuzzFeed Celeb       1970  0.772335   0.789112  0.772822  0.768506\n",
       "38          BuzzFeed Japan        430  0.768605   0.863731  0.787370  0.736412\n",
       "39          BuzzFeed Style       2702  0.752592   0.796655  0.753148  0.742988\n",
       "40    BuzzFeed Oz Politics        647  0.883298   0.893605  0.884491  0.881595\n",
       "41    Obsessed by BuzzFeed       1320  0.740530   0.764818  0.742168  0.734311\n",
       "42               Pero Like        184  0.666066   0.682928  0.664882  0.651492\n",
       "43            Proper Tasty         34  0.676190   0.709167  0.700000  0.636724\n",
       "44        BuzzFeed UK News        944  0.797155   0.808316  0.797796  0.794982\n",
       "45                   Nifty         37  0.694643   0.702262  0.727500  0.664701\n",
       "46          BuzzFeed Geeky        444  0.686977   0.726745  0.688462  0.671311\n",
       "47                  Cheeky       1863  0.810513   0.821800  0.810609  0.808498\n",
       "48       BuzzFeed Weddings        557  0.718106   0.772345  0.719140  0.702886\n",
       "49       BuzzFeed Politics        929  0.914964   0.918847  0.915218  0.914379\n",
       "50           BuzzFeed LGBT        535  0.759813   0.780890  0.761218  0.754587\n",
       "51          BuzzFeed World        625  0.801600   0.810783  0.801298  0.798871\n",
       "52        BuzzFeed Partner        137  0.664418   0.695175  0.666962  0.640130\n",
       "53    BuzzFeed France News        179  0.974841   0.971949  0.976569  0.973067\n",
       "54           BuzzFeed Tech        151  0.824086   0.837742  0.822743  0.819099\n",
       "55     BuzzFeed Japan News        199  0.770000   0.872741  0.792331  0.736503\n",
       "56        BuzzFeed Oz News        337  0.746159   0.760536  0.748196  0.742226\n",
       "57        BuzzFeed News BR        336  0.968723   0.970079  0.968755  0.968190\n",
       "58         BuzzFeed Reader         36  0.753571   0.788929  0.786667  0.742976"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 links that are most relevant for FB  BuzzFeed Style\n",
      "--------------\n",
      "All The Best Deals On Amazon Today\n",
      "All The Best Deals On The Internet This Weekend\n",
      "29 Of The Best Beauty Products To Buy At Target\n",
      "29 Gorgeous And Cheap Dresses To Wear To An Outdoor Wedding\n",
      "How Would You Wear These Red High Heels?\n",
      "11 Beauty Products That Will Actually Take Off All Your Makeup\n",
      "25 Gorgeous Nail Polishes People Actually Swear By\n",
      "26 Awesome Beauty Products You Didn't Know You Could Get At Urban Outfitters\n",
      "26 Of The Best Places To Buy Last Minute Gifts\n",
      "Here's What Amazon Prom Dresses Look Like On Non-Models\n",
      "Here's What Forever 21's New Bridesmaid Dresses Look Like IRL\n",
      "Create A Sitcom And We'll Reveal Your Hair And Eye Color\n",
      "8 Life-Changing Tech Products You Need ASAP\n",
      "19 Photos That Prove Natural Hair Doesn't Need Twist-Outs To Look Beautiful\n",
      "21 Beautiful Ring Sets You'll Want On Your Fingers ASAP\n",
      "23 Easy Hacks That'll Make Every Lazy Girl's Life So Much Easier\n",
      "Kim Kardashian Is Launching Her Own Makeup Line And The World Is Not Ready\n",
      "Tell Us Your Boob Habits And We'll Tell You Which Bra You Should Own\n",
      "Can You Buy An Outfit From Lululemon For Under $350?\n",
      "Spend Some Dough At Bath And Body Works And We'll Tell You Which Body Part People Find Most Attractive\n"
     ]
    }
   ],
   "source": [
    "test_page = 'BuzzFeed Style'\n",
    "ind = pages.index(test_page)\n",
    "clf = page_clf[ind]\n",
    "\n",
    "temp = lf_meta[~(lf_meta.page == test_page)]\n",
    "temp = temp.sort_values('stats_date', ascending = False)[:1000]\n",
    "# temp = lf_meta[['title', 'page']]\n",
    "query_titles = temp['title'].unique()\n",
    "\n",
    "def get_title_fit(X, clf):\n",
    "     return clf.predict_proba(X)\n",
    "\n",
    "demo = pd.DataFrame()\n",
    "demo['title'] = query_titles\n",
    "temp = pd.DataFrame(clf.predict_proba(query_titles), columns=clf.classes_)\n",
    "demo['prob'] = temp[1]\n",
    "print(\"Top 10 links that are most relevant for FB \", test_page)\n",
    "print(\"--------------\")\n",
    "for nam in demo.sort_values('prob', ascending = False)[:20]['title']:\n",
    "    print(nam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 links that are most relevant for FB  BuzzFeed News\n",
      "--------------\n",
      "Police Officer Found Not Guilty In The Fatal Shooting Of Philando Castile\n",
      "Live Updates: Finsbury Park Terror Suspect Remains In Custody As Police Obtain Warrant For Further Detention\n",
      "Six People Have Resigned From Trump's HIV/AIDS Advisory Council Because He &quot;Doesn't Care&quot;\n",
      "Kochi Metro Normalises Trans People At Work With This Message To Passengers\n",
      "An Aussie Bloke Allegedly Tunnelled His Way Out Of A Bali Jail Just Months Before His Release Date\n",
      "Doctors Dispute North Korea's Reason For US Student Being Comatose\n",
      "Vice President Pence Has Hired His Own Lawyer In Connection With The Special Counsel Probe\n",
      "New Zealand Has The Highest Adolescent Suicide Rate Of All Wealthy Countries, Says New Report\n",
      "Trump Rolls Back Obama’s Cuba Policies On Travel And Trade\n",
      "Steve Harvey Gets Roasted On Twitter After Tasteless Flint Water Crisis Joke\n",
      "This Couple Turned 42 On The Same Day And Did The Cutest Birthday Photo Shoot\n",
      "When Survivors Of The Grenfell Tower Fire Broke Their Fasts Last Night, People Joined In Solidarity\n",
      "The Education Department Quietly Invited Anti-LGBT Groups To A Father's Day Event\n",
      "Canada Will Finally Apologize For Mistreatment Of LGBT People And Clear Old Criminal Records\n",
      "The Senate Just Voted Against Removing The Tampon Tax\n",
      "Bill Cosby Rape Case Ends In Mistrial After Jury Deadlocks\n",
      "Jake Tapper Is All Of Us Reacting To Trump's Lawyer\n",
      "We Spoke To The Man Behind The Viral Interview About The Grenfell Tower Fire\n",
      "Here's How A Teacher Showed His LGBT Pride In A Photo With Trump\n",
      "This Is What The LGBT Community In San Francisco Looked Like In The '70s\n"
     ]
    }
   ],
   "source": [
    "test_page = 'BuzzFeed News'\n",
    "ind = pages.index(test_page)\n",
    "clf = page_clf[ind]\n",
    "\n",
    "temp = lf_meta[~(lf_meta.page == test_page)]\n",
    "# temp = lf_meta[~(lf_meta.page == test_page)]\n",
    "temp = temp.sort_values('stats_date', ascending = False)[:1000]\n",
    "# temp = lf_meta[['title', 'page']]\n",
    "query_titles = temp['title'].unique()\n",
    "\n",
    "def get_title_fit(X, clf):\n",
    "     return clf.predict_proba(X)\n",
    "\n",
    "demo = pd.DataFrame()\n",
    "demo['title'] = query_titles\n",
    "temp = pd.DataFrame(clf.predict_proba(query_titles), columns=clf.classes_)\n",
    "demo['prob'] = temp[1]\n",
    "\n",
    "print(\"Top 10 links that are most relevant for FB \", test_page)\n",
    "print(\"--------------\")\n",
    "for nam in demo.sort_values('prob', ascending = False)[:20]['title']:\n",
    "    print(nam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>external_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>page</th>\n",
       "      <th>post_type</th>\n",
       "      <th>buzz_id</th>\n",
       "      <th>uri</th>\n",
       "      <th>title</th>\n",
       "      <th>stats_ts</th>\n",
       "      <th>stats_date</th>\n",
       "      <th>consumptions_by_type__link_clicks</th>\n",
       "      <th>...</th>\n",
       "      <th>stories_by_action_type__comment</th>\n",
       "      <th>stories_by_action_type__like</th>\n",
       "      <th>stories_by_action_type__share</th>\n",
       "      <th>engaged_fan</th>\n",
       "      <th>fan_reach</th>\n",
       "      <th>impressions</th>\n",
       "      <th>impressions_fan</th>\n",
       "      <th>twoday_stats_date</th>\n",
       "      <th>twoday_consumptions_by_type__link_clicks</th>\n",
       "      <th>twoday_stories_by_action_type__share</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [external_id, created_at, page, post_type, buzz_id, uri, title, stats_ts, stats_date, consumptions_by_type__link_clicks, consumptions_by_type__other_clicks, reactions_like_total, stories_by_action_type__comment, stories_by_action_type__like, stories_by_action_type__share, engaged_fan, fan_reach, impressions, impressions_fan, twoday_stats_date, twoday_consumptions_by_type__link_clicks, twoday_stories_by_action_type__share]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 22 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = lf_meta[~(lf_meta.page == test_page)]\n",
    "temp[temp.page == 'BuzzFeed News']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Build A Wardrobe From Forever 21 And We will Guess Your Signature Style;Which Game Of Thrones Character Are You Based On Your Taste In Food?;18 GIFs That Sum Up Your Relationship With Summer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded query 1...\n",
      "Loaded title sources...\n",
      "Loaded explicit double check...\n",
      "\n",
      "=> RUNNING RECOMMENDATIONS\n",
      "\n",
      "Page:  BuzzFeed Animals\n",
      "Until:  2017-01-01\n",
      "Max Suggestions:  20\n",
      "\n",
      "# of Data Points =>  46957\n",
      "Converted page infos into dummies...\n",
      "Ran pipeline on title features...\n",
      "Parsed hstack...\n",
      "Fit the regression model...\n",
      "Filtered already posted buzzes...\n",
      "Assigned next page...\n",
      "Converted pages into pandas dummies...\n",
      "Extracted title features for dataframe...\n",
      "\n",
      "Top  20  links that are most relevant for FB  BuzzFeed Animals ; (test sample size:  16907 )\n",
      "--------------\n",
      "                     page                                              title  \\\n",
      "39497     BuzzFeed Canada  11 Adorable Animals That Are Way Better At Sav...   \n",
      "44765            BuzzFeed  19 Wholesome Boyfriends Who Are So Damn Pure I...   \n",
      "22877            BuzzFeed   16 Times Tumblr Was Too Pure For This Good Earth   \n",
      "42868            BuzzFeed  21 Tweets That Are Really Weird But Will Make ...   \n",
      "35357         BuzzFeed UK  11 Happy Little Things To Make You Smile This ...   \n",
      "2778             BuzzFeed  11 Happy Little Things To Make You Smile This ...   \n",
      "12969         BuzzFeed UK  11 Happy Little Things To Make You Smile This ...   \n",
      "46953  BuzzFeed Australia       23 Glitters Even Non-Makeup Lovers Will Want   \n",
      "8437               Cheeky  These Adorable Pups Have All Been Rescued From...   \n",
      "28333  BuzzFeed Australia  22 Really Fucking Weird Tumblr Posts That Will...   \n",
      "43517      BuzzFeed Books  22 Weird Tumblr Posts That Are Also Really Fuc...   \n",
      "42883      BuzzFeed Style  22 Things All Typography And Lettering Lovers ...   \n",
      "9915               Cheeky  More Than 100 Adorable Puppies Were Rescued Fr...   \n",
      "46510         BuzzFeed UK  19 Very Pure And Wholesome Tumblr Posts About ...   \n",
      "45653            BuzzFeed  19 Tweets That'll Make You Think Hard And Laug...   \n",
      "1263             BuzzFeed  24 Reasons Why No One Should Ever Have A Pit B...   \n",
      "30980        BuzzFeed DIY              26 Delicate Tattoos For Nature Lovers   \n",
      "44760  BuzzFeed Community  Build Your Dream House And We'll Reveal Which ...   \n",
      "35652     BuzzFeed Canada  This Dog Owner Found Hot Dogs Stuffed With Raz...   \n",
      "45091                SOML  15 Tweets That Are Way Too Relatable To People...   \n",
      "\n",
      "       buzz_id  predicted_first_hour_shares  \\\n",
      "39497  4489394                            5   \n",
      "44765  4571195                           89   \n",
      "22877  4540830                           40   \n",
      "42868  4580532                           27   \n",
      "35357  4547588                           16   \n",
      "2778   4445577                           22   \n",
      "12969  4505097                           14   \n",
      "46953  4583860                           32   \n",
      "8437   4442935                            9   \n",
      "28333  4488979                           70   \n",
      "43517  4538239                           52   \n",
      "42883  4536081                           13   \n",
      "9915   4450931                           15   \n",
      "46510  4586083                           24   \n",
      "45653  4580058                           13   \n",
      "1263   3703513                          302   \n",
      "30980  4421609                          107   \n",
      "44760  4466500                            6   \n",
      "35652  4541660                          903   \n",
      "45091  4552287                           56   \n",
      "\n",
      "                                                     URL         next_page  \\\n",
      "39497  www.buzzfeed.com/saveonenergy/animals-saving-t...  BuzzFeed Animals   \n",
      "44765  www.buzzfeed.com/jasminnahar/wholesome-boyfriends  BuzzFeed Animals   \n",
      "22877  www.buzzfeed.com/farrahpenn/times-tumblr-was-t...  BuzzFeed Animals   \n",
      "42868  www.buzzfeed.com/natalyalobanova/hilarious-twe...  BuzzFeed Animals   \n",
      "35357  www.buzzfeed.com/emmacooke24/some-things-suck-...  BuzzFeed Animals   \n",
      "2778   www.buzzfeed.com/emmacooke24/well-get-through-...  BuzzFeed Animals   \n",
      "12969        www.buzzfeed.com/emmacooke24/some-good-news  BuzzFeed Animals   \n",
      "46953  www.buzzfeed.com/brookereilly/can-i-bathe-in-g...  BuzzFeed Animals   \n",
      "8437   www.buzzfeed.com/rosebuchanan/these-adorable-p...  BuzzFeed Animals   \n",
      "28333  www.buzzfeed.com/jennaguillaume/theres-a-lot-o...  BuzzFeed Animals   \n",
      "43517  www.buzzfeed.com/natalyalobanova/weird-tumblr-...  BuzzFeed Animals   \n",
      "42883     www.buzzfeed.com/sarahhan/you-are-just-my-type  BuzzFeed Animals   \n",
      "9915                www.buzzfeed.com/mbvd/cute-emergency  BuzzFeed Animals   \n",
      "46510  www.buzzfeed.com/natalyalobanova/tumblr-posts-...  BuzzFeed Animals   \n",
      "45653  www.buzzfeed.com/natalyalobanova/tweets-that-w...  BuzzFeed Animals   \n",
      "1263           www.buzzfeed.com/daves4/dont-pity-a-pitty  BuzzFeed Animals   \n",
      "30980  www.buzzfeed.com/emmacooke24/essential-tattoos...  BuzzFeed Animals   \n",
      "44760  www.buzzfeed.com/littleredhairedgirl16/create-...  BuzzFeed Animals   \n",
      "35652  www.buzzfeed.com/ishmaeldaro/regina-hot-dog-ra...  BuzzFeed Animals   \n",
      "45091  www.buzzfeed.com/norbertobriceno/15-tweets-abo...  BuzzFeed Animals   \n",
      "\n",
      "                created_at      prob  \\\n",
      "39497  2017-04-04 11:01:19  0.958464   \n",
      "44765  2017-07-02 14:53:41  0.953655   \n",
      "22877  2017-05-28 03:16:09  0.951642   \n",
      "42868  2017-07-16 19:31:09  0.950823   \n",
      "35357  2017-06-05 09:02:10  0.947562   \n",
      "2778   2017-01-22 10:16:12  0.947562   \n",
      "12969  2017-04-10 10:02:22  0.947562   \n",
      "46953  2017-07-31 11:11:13  0.946098   \n",
      "8437   2017-01-18 16:08:08  0.944829   \n",
      "28333  2017-03-17 11:15:20  0.938313   \n",
      "43517  2017-05-20 19:01:09  0.935368   \n",
      "42883  2017-06-06 10:31:19  0.928787   \n",
      "9915   2017-01-28 05:43:06  0.928024   \n",
      "46510  2017-07-22 15:03:14  0.926709   \n",
      "45653  2017-07-15 01:46:23  0.926480   \n",
      "1263   2017-01-16 21:16:19  0.925486   \n",
      "30980  2017-04-08 02:33:07  0.924787   \n",
      "44760  2017-06-13 19:21:06  0.922490   \n",
      "35652  2017-05-24 23:45:08  0.922230   \n",
      "45091  2017-06-17 19:00:09  0.919345   \n",
      "\n",
      "                                              identifier  \n",
      "39497  39497         BuzzFeed Canada\\n44765          ...  \n",
      "44765  39497         BuzzFeed Canada\\n44765          ...  \n",
      "22877  39497         BuzzFeed Canada\\n44765          ...  \n",
      "42868  39497         BuzzFeed Canada\\n44765          ...  \n",
      "35357  39497         BuzzFeed Canada\\n44765          ...  \n",
      "2778   39497         BuzzFeed Canada\\n44765          ...  \n",
      "12969  39497         BuzzFeed Canada\\n44765          ...  \n",
      "46953  39497         BuzzFeed Canada\\n44765          ...  \n",
      "8437   39497         BuzzFeed Canada\\n44765          ...  \n",
      "28333  39497         BuzzFeed Canada\\n44765          ...  \n",
      "43517  39497         BuzzFeed Canada\\n44765          ...  \n",
      "42883  39497         BuzzFeed Canada\\n44765          ...  \n",
      "9915   39497         BuzzFeed Canada\\n44765          ...  \n",
      "46510  39497         BuzzFeed Canada\\n44765          ...  \n",
      "45653  39497         BuzzFeed Canada\\n44765          ...  \n",
      "1263   39497         BuzzFeed Canada\\n44765          ...  \n",
      "30980  39497         BuzzFeed Canada\\n44765          ...  \n",
      "44760  39497         BuzzFeed Canada\\n44765          ...  \n",
      "35652  39497         BuzzFeed Canada\\n44765          ...  \n",
      "45091  39497         BuzzFeed Canada\\n44765          ...  \n"
     ]
    }
   ],
   "source": [
    "###manny code testing\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import scipy.sparse\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# add static path to nltk data\n",
    "nltk.data.path.append('/app/nltk_data')\n",
    "\n",
    "\"\"\"\n",
    "The links in this table are sorted based on the title fit (indicated by the probability or prob)\n",
    "to the target page under consideration, and also the predicted first hour shares (if they are\n",
    "posted on the target page). Out of the top (K * K) links, with respect to probability of fit to\n",
    "the target page (sort by prob), only the top K links are returned as the output based on\n",
    "predicted_first_hour_shares.\n",
    "\"\"\"\n",
    "def recommend_route(datasets=[], params={}):\n",
    "    prev_next_stats = datasets[0]   # cross-posting info: current_page and next_page details for each [external_id]\n",
    "    title_df = datasets[1]          # titles of BF links to train the title fit Bag-of-Words model\n",
    "    explicit_double_check = datasets[2] # external_ids and display_names for all versions of a buzz/link\n",
    "\n",
    "    test_page = params['test_page']\n",
    "    lookback_limit = params['lookback_limit']\n",
    "    max_suggestions = params['max_suggestions']\n",
    "\n",
    "    print('Page: ', test_page)\n",
    "    print('Until: ', lookback_limit)\n",
    "    print('Max Suggestions: ', max_suggestions)\n",
    "    print('')\n",
    "\n",
    "    train_df = prev_next_stats\n",
    "    train_df['page'] = train_df['display_name']\n",
    "    train_df['title'] = train_df['name']\n",
    "    train_df = train_df.fillna(0)\n",
    "    all_pages = train_df['page'].unique()\n",
    "    train_df['URL'] = \"www.buzzfeed.com/\" + train_df['user_name'] + \"/\" + train_df['uri']\n",
    "\n",
    "    \"\"\"\n",
    "    type cast the previous and next page columns to categorical (makes life easy when\n",
    "    doing pd.dummies later on)\n",
    "    \"\"\"\n",
    "    train_df['next_page'] = train_df['next_page'].astype('category', categories = all_pages)\n",
    "    train_df['page'] = train_df['page'].astype('category', categories = all_pages)\n",
    "\n",
    "    \"\"\"\n",
    "    input variables: FB stats for a post on current (or previous)\n",
    "    page in the 1st hour from posting\n",
    "    \"\"\"\n",
    "    one_hour_features = [\n",
    "        'consumptions_by_type__link_clicks', 'consumptions_by_type__other_clicks',\n",
    "        'reactions_like_total', 'stories_by_action_type__comment', 'stories_by_action_type__like',\n",
    "        'stories_by_action_type__share', 'engaged_fan', 'fan_reach', 'impressions',\n",
    "        'impressions_fan',\n",
    "    ]\n",
    "\n",
    "    # linear regression model\n",
    "    clf = linear_model.LinearRegression()\n",
    "    # set features to train on\n",
    "    page_infos = ['page', 'next_page']\n",
    "    train_features = one_hour_features + page_infos\n",
    "    for feat in one_hour_features:\n",
    "        train_df[feat] = np.log(train_df[feat] + 1)\n",
    "\n",
    "    train_df['clicks_bucket'] = np.log(train_df['next_clicks'] + 1)\n",
    "    train_df['share_bucket'] = np.log(train_df['next_shares']+1)\n",
    "    print('# of Data Points => ', len(train_df))\n",
    "\n",
    "    skip_title_fit = 1\n",
    "#     skip_title_fit = 0\n",
    "    if skip_title_fit == 0:\n",
    "        X = train_df[train_features  + ['title']]\n",
    "    else: \n",
    "        X = train_df[train_features]\n",
    "\n",
    "    y = train_df['share_bucket']\n",
    "\n",
    "    # convert page_infos into dummies\n",
    "    X = pd.get_dummies(data=X, columns=['page', 'next_page'])\n",
    "    print('Converted page infos into dummies...')\n",
    "\n",
    "    if skip_title_fit == 0:\n",
    "        # pipeline to fit on title features!\n",
    "        title_vect = Pipeline([\n",
    "            ('vect', CountVectorizer()),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "        ])\n",
    "        print('Ran pipeline on title features...')\n",
    "\n",
    "        # get the title features and \"hstack\" them along with \"one_hour_features\": input variables\n",
    "        X_tfidf = title_vect.fit_transform(X['title'])\n",
    "        X = X.drop(['title'], axis = 1)\n",
    "        X = scipy.sparse.hstack([X_tfidf, X])\n",
    "        print('Parsed hstack...')\n",
    "\n",
    "    # fit the regression model\n",
    "    clf = clf.fit(X, y)\n",
    "    print('Fit the regression model...')\n",
    "\n",
    "    \"\"\"\n",
    "    check 2 conditions:\n",
    "    1) creation date of link should be > lookback_limit (input parameter)\n",
    "    2) current page of the link shouldn't be the test_page itself\n",
    "    \"\"\"\n",
    "    test_df = train_df\n",
    "    test_df = test_df[test_df['created_at'] > lookback_limit]\n",
    "    # remove all buzz_ids which have atleast one external_id in the test_page\n",
    "    def check_if_on_testpage(X):\n",
    "      if test_page in explicit_double_check[explicit_double_check.buzz_id == X]['display_name'].unique():\n",
    "        return 1\n",
    "      else:\n",
    "        return 0\n",
    "\n",
    "    test_df['already_posted'] = test_df['buzz_id'].apply(check_if_on_testpage)\n",
    "    # keep only buzzes that've not yet been posted on test_page\n",
    "    test_df = test_df[test_df.already_posted == 0]\n",
    "    test_df = test_df[~(test_df['page'] == test_page)]\n",
    "    test_df = test_df[~(test_df['next_page'] == test_page)]\n",
    "    print('Filtered already posted buzzes...')\n",
    "\n",
    "    # manually assign next_page = test_page, since we are gonna use this dataframe to make predictions; convert the column to type \"categorical\"\n",
    "    test_df['next_page'] = test_page\n",
    "    test_df['next_page'] = test_df['next_page'].astype('category', categories = all_pages)\n",
    "\n",
    "    X_test = test_df[train_features + ['title']]\n",
    "    print('Assigned next page...')\n",
    "\n",
    "    # convert current and next page names into Pandas dummy variables/columns (pd.get_dummies)\n",
    "    X_test = pd.get_dummies(data = X_test, columns=['page', 'next_page'])\n",
    "    print('Converted pages into pandas dummies...')\n",
    "    \n",
    "    if skip_title_fit == 0:\n",
    "        # extract title features for the test dataframe\n",
    "        X_tfidf = title_vect.transform(X_test['title'])\n",
    "        X_test = X_test.drop(['title'], axis = 1)\n",
    "        X_test = scipy.sparse.hstack([X_tfidf, X_test])\n",
    "        print('Extracted title features for dataframe...')\n",
    "    else:\n",
    "        print('Skipping title fit...')\n",
    "        \n",
    "    # make predictions on a link's performance, if it were to be posted on the target page, given the perfoprmance in the previous page!\n",
    "    test_df['predicted_shares'] = clf.predict(X_test)\n",
    "\n",
    "    # convert the log-scaled predictions back to exponents for computing accuracy and errors!\n",
    "    test_df['predicted_shares'] = (np.exp(test_df['predicted_shares'])-1).astype(int)\n",
    "    test_df['predicted_first_hour_shares'] = test_df['predicted_shares']\n",
    "\n",
    "    # list of columns to include in the output dataframe\n",
    "    result_cols = ['page', 'title', 'buzz_id', 'predicted_first_hour_shares', 'URL', 'next_page', 'created_at']\n",
    "    test_df = test_df[result_cols].drop_duplicates(subset=['buzz_id'], keep = 'last')\n",
    "\n",
    "    df = title_df[['name', 'display_name']].dropna()\n",
    "    df.columns = ['title', 'page']\n",
    "\n",
    "    # training samples: class 1 = links from test_page; class 0 = random links that are from else where\n",
    "    X_1 = df[df.page == test_page]\n",
    "    X_0 = df[~(df.page == test_page)].sample(n=len(X_1))\n",
    "    X_all = pd.concat([X_1, X_0])\n",
    "\n",
    "    def get_class(X, page):\n",
    "        return 1 if X == page else 0\n",
    "\n",
    "    X_all['class'] = X_all['page'].apply(get_class, page=test_page)\n",
    "\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=word_tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultinomialNB()),\n",
    "    ])\n",
    "\n",
    "    X = X_all['title']\n",
    "    y = X_all['class']\n",
    "\n",
    "    title_clf = text_clf.fit(X, y)\n",
    "    temp = test_df\n",
    "\n",
    "    pred = pd.DataFrame(title_clf.predict_proba(temp['title']), columns=title_clf.classes_)\n",
    "    temp['prob'] = pred[1].values\n",
    "\n",
    "    print('')\n",
    "    print('Top ', max_suggestions, ' links that are most relevant for FB ', test_page, '; (test sample size: ', len(temp), ')')\n",
    "    print('--------------')\n",
    "    temp = temp.sort_values('prob', ascending = False)[:max_suggestions*10]\n",
    "    temp['identifier'] = '{page}_{buzz_id}_{next_page}_{shares}'.format(\n",
    "        page=temp['page'].astype(str),\n",
    "        buzz_id=temp['buzz_id'].map(str),\n",
    "        next_page=temp['next_page'].astype(str),\n",
    "        shares=temp['predicted_first_hour_shares'].map(str)\n",
    "    )\n",
    "\n",
    "#     temp = temp.sort_values('predicted_first_hour_shares', ascending = False)[:max_suggestions]\n",
    "    # The links in this table are sorted only based on the title fit (indicated by the probability or prob) to the target page under consideration.\n",
    "    temp = temp.sort_values('prob', ascending = False)[:max_suggestions]\n",
    "    print(temp)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filepat = \"../../mono/proto_recommendation_api/\"\n",
    "    query_1 = pd.read_csv(filepat + 'fixtures/routing_query_1.csv', encoding='utf-8', engine='python')\n",
    "    print('Loaded query 1...')\n",
    "    title_sources = pd.read_csv(filepat + 'fixtures/routing_title_sources.csv', encoding='utf-8', engine='python')\n",
    "    print('Loaded title sources...')\n",
    "    explicit_dc = pd.read_csv(filepat + 'fixtures/routing_explicit_double_check.csv', encoding='utf-8', engine='python')\n",
    "    print('Loaded explicit double check...')\n",
    "    print('')\n",
    "    print('=> RUNNING RECOMMENDATIONS')\n",
    "    print('')\n",
    "\n",
    "    fake_data_sets = [query_1, title_sources, explicit_dc]\n",
    "    fake_params = {\n",
    "        'test_page': 'BuzzFeed Animals',\n",
    "        'lookback_limit': '2017-01-01',\n",
    "        'max_suggestions': 20,\n",
    "    }\n",
    "    recommend_route(datasets=fake_data_sets, params=fake_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded query 1...\n",
      "Loaded title sources...\n",
      "Loaded explicit double check...\n",
      "\n",
      "=> RUNNING RECOMMENDATIONS\n",
      "\n",
      "Page:  BuzzFeed Food\n",
      "Until:  2017-01-01\n",
      "Max Suggestions:  20\n",
      "\n",
      "# of Data Points =>  46957\n",
      "Converted page infos into dummies...\n",
      "Fit the regression model...\n",
      "Filtered already posted buzzes...\n",
      "Assigned next page...\n",
      "Converted pages into pandas dummies...\n",
      "Skipping title fit...\n",
      "\n",
      "Top  20  links that are most relevant for FB  BuzzFeed Food ; (test sample size:  15950 )\n",
      "--------------\n",
      "                       page  \\\n",
      "28271         BuzzFeed Quiz   \n",
      "10332         BuzzFeed Quiz   \n",
      "13412        BuzzFeed India   \n",
      "5114          BuzzFeed Quiz   \n",
      "33091         BuzzFeed Quiz   \n",
      "18833      BuzzFeed Parents   \n",
      "19706         BuzzFeed Quiz   \n",
      "11161         BuzzFeed Quiz   \n",
      "17873              BuzzFeed   \n",
      "2201       BuzzFeed Parents   \n",
      "45374  BuzzFeed Philippines   \n",
      "35998              BuzzFeed   \n",
      "37644       BuzzFeed Health   \n",
      "43360         BuzzFeed Quiz   \n",
      "45050           BuzzFeed UK   \n",
      "33810       BuzzFeed Canada   \n",
      "30243    BuzzFeed Community   \n",
      "44897                 Nifty   \n",
      "43768      BuzzFeed Parents   \n",
      "39655    BuzzFeed Community   \n",
      "\n",
      "                                                   title  buzz_id  \\\n",
      "28271  Pick Your Favorite Ice Cream Flavor And We'll ...  4486625   \n",
      "10332  Can We Guess Your Age And Location Based On Yo...  4438174   \n",
      "13412  13 Drool-Worthy Ways You Should Actually Be Ea...  4482214   \n",
      "5114   Tell Us Your Chipotle Order And We'll Guess Yo...  4464031   \n",
      "33091  Create Your Perfect Home And We'll Tell You Wh...  4546097   \n",
      "18833  7 Easy Organizing Tricks You'll Actually Want ...  4521505   \n",
      "19706  Choose Either Salty Or Sweet Foods And We'll G...  4540223   \n",
      "11161  Make Your Own Cheese Pasta And We’ll Tell You ...  4485541   \n",
      "17873  Order A Meal From Wetherspoon's And We'll Gues...  4505856   \n",
      "2201    15 No-Mess Parenting Hacks That Are Sheer Genius  4439141   \n",
      "45374  21 Filipino Food Items We'll Probably Never Ge...  4513353   \n",
      "35998  23 Easy Hacks That'll Make Every Lazy Girl's L...  4545897   \n",
      "37644      7 Easy Organizing Ideas That Cost $15 Or Less  4567766   \n",
      "43360       Would You Eat These Weird Food Combinations?  4553729   \n",
      "45050  This British Ice Cream Quiz Will Reveal Your A...  4561606   \n",
      "33810  11 Grilled Cheese Sandwiches That Will Make Yo...  4496491   \n",
      "30243  Pick All Three Meals, Two Snacks, And A Desser...  4521829   \n",
      "44897  16 DIY Hacks To Prep Before Your Next Camping ...  4555587   \n",
      "43768  7 Small Ways To Actually Get Your Home In Orde...  4550488   \n",
      "39655  Answer 8 Starbucks Questions And We'll Tell Yo...  4563947   \n",
      "\n",
      "       predicted_first_hour_shares  \\\n",
      "28271                           10   \n",
      "10332                           16   \n",
      "13412                           18   \n",
      "5114                            12   \n",
      "33091                           17   \n",
      "18833                           41   \n",
      "19706                           29   \n",
      "11161                           13   \n",
      "17873                           25   \n",
      "2201                           360   \n",
      "45374                          113   \n",
      "35998                           33   \n",
      "37644                           19   \n",
      "43360                           21   \n",
      "45050                           44   \n",
      "33810                           28   \n",
      "30243                           20   \n",
      "44897                           42   \n",
      "43768                           38   \n",
      "39655                           11   \n",
      "\n",
      "                                                     URL      next_page  \\\n",
      "28271  www.buzzfeed.com/annas451019d1e/choose-your-fa...  BuzzFeed Food   \n",
      "10332  www.buzzfeed.com/georgerizz/can-we-guess-your-...  BuzzFeed Food   \n",
      "13412                www.buzzfeed.com/andreborges/flunch  BuzzFeed Food   \n",
      "5114   www.buzzfeed.com/masonnelson2/can-we-guess-you...  BuzzFeed Food   \n",
      "33091  www.buzzfeed.com/alexiskap/create-your-perfect...  BuzzFeed Food   \n",
      "18833  www.buzzfeed.com/nataliebrown/easy-organizing-...  BuzzFeed Food   \n",
      "19706  www.buzzfeed.com/floperry/choose-between-these...  BuzzFeed Food   \n",
      "11161  www.buzzfeed.com/brettvergara/make-your-own-ch...  BuzzFeed Food   \n",
      "17873  www.buzzfeed.com/jamiejones/order-a-meal-from-...  BuzzFeed Food   \n",
      "2201   www.buzzfeed.com/kristatorres/truly-amazing-pa...  BuzzFeed Food   \n",
      "45374     www.buzzfeed.com/isabellelaureta/rip-childhood  BuzzFeed Food   \n",
      "35998  www.buzzfeed.com/juliegerstein/lazy-cleaning-h...  BuzzFeed Food   \n",
      "37644  www.buzzfeed.com/nataliebrown/clean-grill-with...  BuzzFeed Food   \n",
      "43360  www.buzzfeed.com/jasminnahar/would-you-eat-the...  BuzzFeed Food   \n",
      "45050  www.buzzfeed.com/beckybarnicoat/british-ice-cr...  BuzzFeed Food   \n",
      "33810  www.buzzfeed.com/dempsters/grilled-cheese-sand...  BuzzFeed Food   \n",
      "30243  www.buzzfeed.com/edithkiprono/we-know-your-bes...  BuzzFeed Food   \n",
      "44897  www.buzzfeed.com/cieravelarde/roughing-it-has-...  BuzzFeed Food   \n",
      "43768  www.buzzfeed.com/nataliebrown/summer-organizin...  BuzzFeed Food   \n",
      "39655  www.buzzfeed.com/kaylahunttt/pick-6-of-your-fa...  BuzzFeed Food   \n",
      "\n",
      "                created_at      prob  \\\n",
      "28271  2017-03-13 18:42:09  0.969374   \n",
      "10332  2017-01-10 20:51:10  0.968234   \n",
      "13412  2017-03-14 06:29:25  0.967494   \n",
      "5114   2017-02-14 20:17:13  0.959818   \n",
      "33091  2017-06-03 17:09:16  0.957459   \n",
      "18833  2017-05-07 15:27:17  0.956709   \n",
      "19706  2017-05-30 20:30:11  0.952629   \n",
      "11161  2017-03-11 22:39:41  0.951934   \n",
      "17873  2017-04-10 13:48:58  0.950502   \n",
      "2201   2017-01-22 15:32:09  0.948639   \n",
      "45374  2017-06-28 08:37:24  0.945388   \n",
      "35998  2017-06-12 12:01:10  0.944673   \n",
      "37644  2017-07-03 14:29:41  0.944143   \n",
      "43360  2017-06-12 03:30:08  0.943846   \n",
      "45050  2017-06-21 15:44:08  0.943488   \n",
      "33810  2017-04-13 11:00:58  0.942507   \n",
      "30243  2017-04-30 20:40:10  0.941689   \n",
      "44897  2017-06-26 23:00:06  0.941376   \n",
      "43768  2017-06-11 20:40:09  0.941245   \n",
      "39655  2017-06-24 16:41:27  0.940852   \n",
      "\n",
      "                                              identifier  \n",
      "28271  28271           BuzzFeed Quiz\\n10332          ...  \n",
      "10332  28271           BuzzFeed Quiz\\n10332          ...  \n",
      "13412  28271           BuzzFeed Quiz\\n10332          ...  \n",
      "5114   28271           BuzzFeed Quiz\\n10332          ...  \n",
      "33091  28271           BuzzFeed Quiz\\n10332          ...  \n",
      "18833  28271           BuzzFeed Quiz\\n10332          ...  \n",
      "19706  28271           BuzzFeed Quiz\\n10332          ...  \n",
      "11161  28271           BuzzFeed Quiz\\n10332          ...  \n",
      "17873  28271           BuzzFeed Quiz\\n10332          ...  \n",
      "2201   28271           BuzzFeed Quiz\\n10332          ...  \n",
      "45374  28271           BuzzFeed Quiz\\n10332          ...  \n",
      "35998  28271           BuzzFeed Quiz\\n10332          ...  \n",
      "37644  28271           BuzzFeed Quiz\\n10332          ...  \n",
      "43360  28271           BuzzFeed Quiz\\n10332          ...  \n",
      "45050  28271           BuzzFeed Quiz\\n10332          ...  \n",
      "33810  28271           BuzzFeed Quiz\\n10332          ...  \n",
      "30243  28271           BuzzFeed Quiz\\n10332          ...  \n",
      "44897  28271           BuzzFeed Quiz\\n10332          ...  \n",
      "43768  28271           BuzzFeed Quiz\\n10332          ...  \n",
      "39655  28271           BuzzFeed Quiz\\n10332          ...  \n"
     ]
    }
   ],
   "source": [
    "###manny code testing\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import scipy.sparse\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# add static path to nltk data\n",
    "nltk.data.path.append('/app/nltk_data')\n",
    "\n",
    "\"\"\"\n",
    "The links in this table are sorted based on the title fit (indicated by the probability or prob)\n",
    "to the target page under consideration, and also the predicted first hour shares (if they are\n",
    "posted on the target page). Out of the top (K * K) links, with respect to probability of fit to\n",
    "the target page (sort by prob), only the top K links are returned as the output based on\n",
    "predicted_first_hour_shares.\n",
    "\"\"\"\n",
    "def recommend_route(datasets=[], params={}):\n",
    "    prev_next_stats = datasets[0]   # cross-posting info: current_page and next_page details for each [external_id]\n",
    "    title_df = datasets[1]          # titles of BF links to train the title fit Bag-of-Words model\n",
    "    explicit_double_check = datasets[2] # external_ids and display_names for all versions of a buzz/link\n",
    "\n",
    "    test_page = params['test_page']\n",
    "    lookback_limit = params['lookback_limit']\n",
    "    max_suggestions = params['max_suggestions']\n",
    "\n",
    "    print('Page: ', test_page)\n",
    "    print('Until: ', lookback_limit)\n",
    "    print('Max Suggestions: ', max_suggestions)\n",
    "    print('')\n",
    "\n",
    "    train_df = prev_next_stats\n",
    "    train_df['page'] = train_df['display_name']\n",
    "    train_df['title'] = train_df['name']\n",
    "    train_df = train_df.fillna(0)\n",
    "    all_pages = train_df['page'].unique()\n",
    "    train_df['URL'] = \"www.buzzfeed.com/\" + train_df['user_name'] + \"/\" + train_df['uri']\n",
    "\n",
    "    \"\"\"\n",
    "    type cast the previous and next page columns to categorical (makes life easy when\n",
    "    doing pd.dummies later on)\n",
    "    \"\"\"\n",
    "    train_df['next_page'] = train_df['next_page'].astype('category', categories = all_pages)\n",
    "    train_df['page'] = train_df['page'].astype('category', categories = all_pages)\n",
    "\n",
    "    \"\"\"\n",
    "    input variables: FB stats for a post on current (or previous)\n",
    "    page in the 1st hour from posting\n",
    "    \"\"\"\n",
    "    one_hour_features = [\n",
    "        'consumptions_by_type__link_clicks', 'consumptions_by_type__other_clicks',\n",
    "        'reactions_like_total', 'stories_by_action_type__comment', 'stories_by_action_type__like',\n",
    "        'stories_by_action_type__share', 'engaged_fan', 'fan_reach', 'impressions',\n",
    "        'impressions_fan',\n",
    "    ]\n",
    "\n",
    "    # linear regression model\n",
    "    clf = linear_model.LinearRegression()\n",
    "    # set features to train on\n",
    "    page_infos = ['page', 'next_page']\n",
    "    train_features = one_hour_features + page_infos\n",
    "    for feat in one_hour_features:\n",
    "        train_df[feat] = np.log(train_df[feat] + 1)\n",
    "\n",
    "    train_df['clicks_bucket'] = np.log(train_df['next_clicks'] + 1)\n",
    "    train_df['share_bucket'] = np.log(train_df['next_shares']+1)\n",
    "    print('# of Data Points => ', len(train_df))\n",
    "\n",
    "#     skip_title_fit = 1\n",
    "    skip_title_fit = 0\n",
    "    if skip_title_fit == 0:\n",
    "        X = train_df[train_features  + ['title']]\n",
    "    else: \n",
    "        X = train_df[train_features]\n",
    "\n",
    "    y = train_df['share_bucket']\n",
    "\n",
    "    # convert page_infos into dummies\n",
    "    X = pd.get_dummies(data=X, columns=['page', 'next_page'])\n",
    "    print('Converted page infos into dummies...')\n",
    "\n",
    "    if skip_title_fit == 0:\n",
    "        # pipeline to fit on title features!\n",
    "        title_vect = Pipeline([\n",
    "            ('vect', CountVectorizer()),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "        ])\n",
    "        print('Ran pipeline on title features...')\n",
    "\n",
    "        # get the title features and \"hstack\" them along with \"one_hour_features\": input variables\n",
    "        X_tfidf = title_vect.fit_transform(X['title'])\n",
    "        X = X.drop(['title'], axis = 1)\n",
    "        X = scipy.sparse.hstack([X_tfidf, X])\n",
    "        print('Parsed hstack...')\n",
    "\n",
    "    # fit the regression model\n",
    "    clf = clf.fit(X, y)\n",
    "    print('Fit the regression model...')\n",
    "\n",
    "    \"\"\"\n",
    "    check 2 conditions:\n",
    "    1) creation date of link should be > lookback_limit (input parameter)\n",
    "    2) current page of the link shouldn't be the test_page itself\n",
    "    \"\"\"\n",
    "    test_df = train_df\n",
    "    test_df = test_df[test_df['created_at'] > lookback_limit]\n",
    "    # remove all buzz_ids which have atleast one external_id in the test_page\n",
    "    def check_if_on_testpage(X):\n",
    "      if test_page in explicit_double_check[explicit_double_check.buzz_id == X]['display_name'].unique():\n",
    "        return 1\n",
    "      else:\n",
    "        return 0\n",
    "\n",
    "    test_df['already_posted'] = test_df['buzz_id'].apply(check_if_on_testpage)\n",
    "    # keep only buzzes that've not yet been posted on test_page\n",
    "#     test_df = test_df[test_df.already_posted == 0]\n",
    "    test_df = test_df[~(test_df['page'] == test_page)]\n",
    "    test_df = test_df[~(test_df['next_page'] == test_page)]\n",
    "    print('Filtered already posted buzzes...')\n",
    "\n",
    "    # manually assign next_page = test_page, since we are gonna use this dataframe to make predictions; convert the column to type \"categorical\"\n",
    "    test_df['next_page'] = test_page\n",
    "    test_df['next_page'] = test_df['next_page'].astype('category', categories = all_pages)\n",
    "\n",
    "    if skip_title_fit == 0:\n",
    "        X_test = test_df[train_features + ['title']]\n",
    "    else:\n",
    "        X_test = test_df[train_features]\n",
    "\n",
    "    print('Assigned next page...')\n",
    "\n",
    "    # convert current and next page names into Pandas dummy variables/columns (pd.get_dummies)\n",
    "    X_test = pd.get_dummies(data = X_test, columns=['page', 'next_page'])\n",
    "    print('Converted pages into pandas dummies...')\n",
    "    \n",
    "    if skip_title_fit == 0:\n",
    "        # extract title features for the test dataframe\n",
    "        X_tfidf = title_vect.transform(X_test['title'])\n",
    "        X_test = X_test.drop(['title'], axis = 1)\n",
    "        X_test = scipy.sparse.hstack([X_tfidf, X_test])\n",
    "        print('Extracted title features for dataframe...')\n",
    "    else:\n",
    "        print('Skipping title fit...')\n",
    "        \n",
    "    # make predictions on a link's performance, if it were to be posted on the target page, given the perfoprmance in the previous page!\n",
    "    test_df['predicted_shares'] = clf.predict(X_test)\n",
    "\n",
    "    # convert the log-scaled predictions back to exponents for computing accuracy and errors!\n",
    "    test_df['predicted_shares'] = (np.exp(test_df['predicted_shares'])-1).astype(int)\n",
    "    test_df['predicted_first_hour_shares'] = test_df['predicted_shares']\n",
    "\n",
    "    # list of columns to include in the output dataframe\n",
    "    result_cols = ['page', 'title', 'buzz_id', 'predicted_first_hour_shares', 'URL', 'next_page', 'created_at']\n",
    "    test_df = test_df[result_cols].drop_duplicates(subset=['buzz_id'], keep = 'last')\n",
    "\n",
    "    df = title_df[['name', 'display_name']].dropna()\n",
    "    df.columns = ['title', 'page']\n",
    "\n",
    "    # training samples: class 1 = links from test_page; class 0 = random links that are from else where\n",
    "    X_1 = df[df.page == test_page]\n",
    "    X_0 = df[~(df.page == test_page)].sample(n=len(X_1))\n",
    "    X_all = pd.concat([X_1, X_0])\n",
    "\n",
    "    def get_class(X, page):\n",
    "        return 1 if X == page else 0\n",
    "\n",
    "    X_all['class'] = X_all['page'].apply(get_class, page=test_page)\n",
    "\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=word_tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultinomialNB()),\n",
    "    ])\n",
    "\n",
    "    X = X_all['title']\n",
    "    y = X_all['class']\n",
    "\n",
    "    title_clf = text_clf.fit(X, y)\n",
    "    temp = test_df\n",
    "\n",
    "    pred = pd.DataFrame(title_clf.predict_proba(temp['title']), columns=title_clf.classes_)\n",
    "    temp['prob'] = pred[1].values\n",
    "\n",
    "    print('')\n",
    "    print('Top ', max_suggestions, ' links that are most relevant for FB ', test_page, '; (test sample size: ', len(temp), ')')\n",
    "    print('--------------')\n",
    "    temp = temp.sort_values('prob', ascending = False)[:max_suggestions*10]\n",
    "    temp['identifier'] = '{page}_{buzz_id}_{next_page}_{shares}'.format(\n",
    "        page=temp['page'].astype(str),\n",
    "        buzz_id=temp['buzz_id'].map(str),\n",
    "        next_page=temp['next_page'].astype(str),\n",
    "        shares=temp['predicted_first_hour_shares'].map(str)\n",
    "    )\n",
    "\n",
    "#     temp = temp.sort_values('predicted_first_hour_shares', ascending = False)[:max_suggestions]\n",
    "    # The links in this table are sorted only based on the title fit (indicated by the probability or prob) to the target page under consideration.\n",
    "    temp = temp.sort_values('prob', ascending = False)[:max_suggestions]\n",
    "    print(temp)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filepat = \"../../mono/proto_recommendation_api/\"\n",
    "    query_1 = pd.read_csv(filepat + 'fixtures/routing_query_1.csv', encoding='utf-8', engine='python')\n",
    "    print('Loaded query 1...')\n",
    "    title_sources = pd.read_csv(filepat + 'fixtures/routing_title_sources.csv', encoding='utf-8', engine='python')\n",
    "    print('Loaded title sources...')\n",
    "    explicit_dc = pd.read_csv(filepat + 'fixtures/routing_explicit_double_check.csv', encoding='utf-8', engine='python')\n",
    "    print('Loaded explicit double check...')\n",
    "    print('')\n",
    "    print('=> RUNNING RECOMMENDATIONS')\n",
    "    print('')\n",
    "\n",
    "    fake_data_sets = [query_1, title_sources, explicit_dc]\n",
    "    fake_params = {\n",
    "        'test_page': 'BuzzFeed Food',\n",
    "        'lookback_limit': '2017-01-01',\n",
    "        'max_suggestions': 20,\n",
    "    }\n",
    "    recommend_route(datasets=fake_data_sets, params=fake_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded query 1...\n",
      "Loaded title sources...\n",
      "Loaded explicit double check...\n",
      "\n",
      "=> RUNNING RECOMMENDATIONS\n",
      "\n",
      "Page:  Nifty\n",
      "Until:  2017-01-01\n",
      "Max Suggestions:  20\n",
      "\n",
      "# of Data Points =>  46957\n",
      "Converted page infos into dummies...\n",
      "Ran pipeline on title features...\n",
      "Parsed hstack...\n",
      "Fit the regression model...\n",
      "Filtered already posted buzzes...\n",
      "Assigned next page...\n",
      "Converted pages into pandas dummies...\n",
      "Extracted title features for dataframe...\n",
      "\n",
      "Top  20  links that are most relevant for FB  Nifty ; (test sample size:  17579 )\n",
      "--------------\n",
      "                     page                                              title  \\\n",
      "31273     BuzzFeed Health  23 Birth Control Alarms That Will Make You Lau...   \n",
      "39773        BuzzFeed DIY  30 Cleaning Tips That'll Make Your Life So Muc...   \n",
      "35913        BuzzFeed DIY   23 Things That'll Make Your House So Much Homier   \n",
      "28884            BuzzFeed  23 Tweets About Amazon Prime Day That'll Make ...   \n",
      "5976   BuzzFeed Australia  21 Things That Will Make Australians Feel Damn...   \n",
      "30059      BuzzFeed Video  35 Tweets About NYC That Will Make You Laugh H...   \n",
      "12313      BuzzFeed Geeky  21 Nihilistic Memes That Will Make You Questio...   \n",
      "44592        BuzzFeed DIY   23 Clever Ways To Make Parenting So Much Cheaper   \n",
      "11380         BuzzFeed UK  23 Weird Tweets That Will Make You Laugh Despi...   \n",
      "6779             BuzzFeed  21 Mansplaining Jokes That Will Make You Piss ...   \n",
      "22525     BuzzFeed Health  11 Self Defense Tips That Will Make You Feel S...   \n",
      "37631        BuzzFeed DIY  23 Things That'll Make Your Home So Much Safer...   \n",
      "40753        BuzzFeed DIY  15 Things That'll Make Your Home Smell So Much...   \n",
      "1824          BuzzFeed UK  21 Science Jokes That Will Make You Laugh Desp...   \n",
      "42714      BuzzFeed Books  21 Ridiculously Clever Mugs That Are 100% Funn...   \n",
      "6493      BuzzFeed Health  23 Surprising Weight-Loss Tips That Are Actual...   \n",
      "28521     BuzzFeed Canada  21 Clever Mnemonic Devices That Will Help You ...   \n",
      "44321        Cocoa Butter  31 Tweets About Atlanta That Will Make You Lau...   \n",
      "13819        BuzzFeed DIY  17 Clever Products That May Actually Make You ...   \n",
      "7693          BuzzFeed UK  23 Jokes That Will Make Overthinkers Laugh The...   \n",
      "\n",
      "       buzz_id  predicted_first_hour_shares  \\\n",
      "31273  4497511                         1361   \n",
      "39773  4532619                          115   \n",
      "35913  4565443                           60   \n",
      "28884  4577637                          214   \n",
      "5976   4434391                          554   \n",
      "30059  4543566                          404   \n",
      "12313  4501813                         1247   \n",
      "44592  4554427                          143   \n",
      "11380  4442920                          169   \n",
      "6779   4451075                         1374   \n",
      "22525  4516977                          160   \n",
      "37631  4549553                          138   \n",
      "40753  4492493                           69   \n",
      "1824   4439528                          427   \n",
      "42714  4549728                          385   \n",
      "6493   4419053                          732   \n",
      "28521  4486573                          921   \n",
      "44321  4554545                          718   \n",
      "13819  4447895                          278   \n",
      "7693   4462906                         5812   \n",
      "\n",
      "                                                     URL next_page  \\\n",
      "31273  www.buzzfeed.com/carolinekee/omg-take-your-pil...     Nifty   \n",
      "39773  www.buzzfeed.com/emmamcanaw/cleaning-hacks-to-...     Nifty   \n",
      "35913  www.buzzfeed.com/treyegreen/things-thatll-make...     Nifty   \n",
      "28884  www.buzzfeed.com/jonmichaelpoff/amazon-prime-d...     Nifty   \n",
      "5976          www.buzzfeed.com/jennaguillaume/i-feel-old     Nifty   \n",
      "30059    www.buzzfeed.com/mjs538/pole-dancing-subway-rat     Nifty   \n",
      "12313  www.buzzfeed.com/lorynbrantz/21-nihilist-memes...     Nifty   \n",
      "44592  www.buzzfeed.com/elenamgarcia/ways-to-make-par...     Nifty   \n",
      "11380  www.buzzfeed.com/floperry/weird-tweets-that-wi...     Nifty   \n",
      "6779   www.buzzfeed.com/hilarywardle/actually-i-think...     Nifty   \n",
      "22525  www.buzzfeed.com/skycowans/11-self-defense-tip...     Nifty   \n",
      "37631  www.buzzfeed.com/elizabethlilly/childproof-you...     Nifty   \n",
      "40753  www.buzzfeed.com/elizabethlilly/cover-up-that-...     Nifty   \n",
      "1824           www.buzzfeed.com/kellyoakes/science-jokes     Nifty   \n",
      "42714  www.buzzfeed.com/anamariaglavan/the-cleverest-...     Nifty   \n",
      "6493   www.buzzfeed.com/sallytamarkin/actually-do-abl...     Nifty   \n",
      "28521  www.buzzfeed.com/sarahaspler/mnemonic-devices-...     Nifty   \n",
      "44321  www.buzzfeed.com/delaneystrunk/if-you-like-fis...     Nifty   \n",
      "13819  www.buzzfeed.com/jeffbarron/ways-to-make-winte...     Nifty   \n",
      "7693   www.buzzfeed.com/annaborges/overthink-in-my-ca...     Nifty   \n",
      "\n",
      "                created_at      prob  \\\n",
      "31273  2017-03-31 01:08:09  0.879762   \n",
      "39773  2017-05-30 11:01:14  0.870639   \n",
      "35913  2017-07-18 12:01:10  0.870440   \n",
      "28884  2017-07-11 19:31:13  0.865815   \n",
      "5976   2017-01-06 10:34:10  0.864009   \n",
      "30059  2017-06-06 16:30:09  0.863753   \n",
      "12313  2017-04-05 23:39:43  0.863660   \n",
      "44592  2017-06-24 11:31:05  0.863069   \n",
      "11380  2017-02-04 23:03:06  0.862942   \n",
      "6779   2017-01-29 03:31:15  0.861691   \n",
      "22525  2017-05-10 22:06:15  0.861127   \n",
      "37631  2017-06-23 12:31:12  0.860860   \n",
      "40753  2017-04-18 14:19:15  0.860308   \n",
      "1824   2017-01-14 11:01:08  0.859759   \n",
      "42714  2017-06-15 19:01:09  0.856886   \n",
      "6493   2017-01-08 15:30:06  0.856635   \n",
      "28521  2017-03-15 23:31:10  0.856274   \n",
      "44321  2017-06-13 00:30:08  0.854826   \n",
      "13819  2017-02-05 17:04:01  0.853498   \n",
      "7693   2017-02-15 23:01:07  0.852817   \n",
      "\n",
      "                                              identifier  \n",
      "31273  31273                BuzzFeed Health\\n39773   ...  \n",
      "39773  31273                BuzzFeed Health\\n39773   ...  \n",
      "35913  31273                BuzzFeed Health\\n39773   ...  \n",
      "28884  31273                BuzzFeed Health\\n39773   ...  \n",
      "5976   31273                BuzzFeed Health\\n39773   ...  \n",
      "30059  31273                BuzzFeed Health\\n39773   ...  \n",
      "12313  31273                BuzzFeed Health\\n39773   ...  \n",
      "44592  31273                BuzzFeed Health\\n39773   ...  \n",
      "11380  31273                BuzzFeed Health\\n39773   ...  \n",
      "6779   31273                BuzzFeed Health\\n39773   ...  \n",
      "22525  31273                BuzzFeed Health\\n39773   ...  \n",
      "37631  31273                BuzzFeed Health\\n39773   ...  \n",
      "40753  31273                BuzzFeed Health\\n39773   ...  \n",
      "1824   31273                BuzzFeed Health\\n39773   ...  \n",
      "42714  31273                BuzzFeed Health\\n39773   ...  \n",
      "6493   31273                BuzzFeed Health\\n39773   ...  \n",
      "28521  31273                BuzzFeed Health\\n39773   ...  \n",
      "44321  31273                BuzzFeed Health\\n39773   ...  \n",
      "13819  31273                BuzzFeed Health\\n39773   ...  \n",
      "7693   31273                BuzzFeed Health\\n39773   ...  \n"
     ]
    }
   ],
   "source": [
    "###manny code testing\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import scipy.sparse\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# add static path to nltk data\n",
    "nltk.data.path.append('/app/nltk_data')\n",
    "\n",
    "\"\"\"\n",
    "The links in this table are sorted based on the title fit (indicated by the probability or prob)\n",
    "to the target page under consideration, and also the predicted first hour shares (if they are\n",
    "posted on the target page). Out of the top (K * K) links, with respect to probability of fit to\n",
    "the target page (sort by prob), only the top K links are returned as the output based on\n",
    "predicted_first_hour_shares.\n",
    "\"\"\"\n",
    "def recommend_route(datasets=[], params={}):\n",
    "    prev_next_stats = datasets[0]   # cross-posting info: current_page and next_page details for each [external_id]\n",
    "    title_df = datasets[1]          # titles of BF links to train the title fit Bag-of-Words model\n",
    "    explicit_double_check = datasets[2] # external_ids and display_names for all versions of a buzz/link\n",
    "\n",
    "    test_page = params['test_page']\n",
    "    lookback_limit = params['lookback_limit']\n",
    "    max_suggestions = params['max_suggestions']\n",
    "\n",
    "    print('Page: ', test_page)\n",
    "    print('Until: ', lookback_limit)\n",
    "    print('Max Suggestions: ', max_suggestions)\n",
    "    print('')\n",
    "\n",
    "    train_df = prev_next_stats\n",
    "    train_df['page'] = train_df['display_name']\n",
    "    train_df['title'] = train_df['name']\n",
    "    train_df = train_df.fillna(0)\n",
    "    all_pages = train_df['page'].unique()\n",
    "    train_df['URL'] = \"www.buzzfeed.com/\" + train_df['user_name'] + \"/\" + train_df['uri']\n",
    "\n",
    "    \"\"\"\n",
    "    type cast the previous and next page columns to categorical (makes life easy when\n",
    "    doing pd.dummies later on)\n",
    "    \"\"\"\n",
    "    train_df['next_page'] = train_df['next_page'].astype('category', categories = all_pages)\n",
    "    train_df['page'] = train_df['page'].astype('category', categories = all_pages)\n",
    "\n",
    "    \"\"\"\n",
    "    input variables: FB stats for a post on current (or previous)\n",
    "    page in the 1st hour from posting\n",
    "    \"\"\"\n",
    "    one_hour_features = [\n",
    "        'consumptions_by_type__link_clicks', 'consumptions_by_type__other_clicks',\n",
    "        'reactions_like_total', 'stories_by_action_type__comment', 'stories_by_action_type__like',\n",
    "        'stories_by_action_type__share', 'engaged_fan', 'fan_reach', 'impressions',\n",
    "        'impressions_fan',\n",
    "    ]\n",
    "\n",
    "    # linear regression model\n",
    "    clf = linear_model.LinearRegression()\n",
    "    # set features to train on\n",
    "    page_infos = ['page', 'next_page']\n",
    "    train_features = one_hour_features + page_infos\n",
    "    for feat in one_hour_features:\n",
    "        train_df[feat] = np.log(train_df[feat] + 1)\n",
    "\n",
    "    train_df['clicks_bucket'] = np.log(train_df['next_clicks'] + 1)\n",
    "    train_df['share_bucket'] = np.log(train_df['next_shares']+1)\n",
    "    print('# of Data Points => ', len(train_df))\n",
    "\n",
    "#     skip_title_fit = 1\n",
    "    skip_title_fit = 0\n",
    "    if skip_title_fit == 0:\n",
    "        X = train_df[train_features  + ['title']]\n",
    "    else: \n",
    "        X = train_df[train_features]\n",
    "\n",
    "    y = train_df['share_bucket']\n",
    "\n",
    "    # convert page_infos into dummies\n",
    "    X = pd.get_dummies(data=X, columns=['page', 'next_page'])\n",
    "    print('Converted page infos into dummies...')\n",
    "\n",
    "    if skip_title_fit == 0:\n",
    "        # pipeline to fit on title features!\n",
    "        title_vect = Pipeline([\n",
    "            ('vect', CountVectorizer()),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "        ])\n",
    "        print('Ran pipeline on title features...')\n",
    "\n",
    "        # get the title features and \"hstack\" them along with \"one_hour_features\": input variables\n",
    "        X_tfidf = title_vect.fit_transform(X['title'])\n",
    "        X = X.drop(['title'], axis = 1)\n",
    "        X = scipy.sparse.hstack([X_tfidf, X])\n",
    "        print('Parsed hstack...')\n",
    "\n",
    "    # fit the regression model\n",
    "    clf = clf.fit(X, y)\n",
    "    print('Fit the regression model...')\n",
    "\n",
    "    \"\"\"\n",
    "    check 2 conditions:\n",
    "    1) creation date of link should be > lookback_limit (input parameter)\n",
    "    2) current page of the link shouldn't be the test_page itself\n",
    "    \"\"\"\n",
    "    test_df = train_df\n",
    "    test_df = test_df[test_df['created_at'] > lookback_limit]\n",
    "    # remove all buzz_ids which have atleast one external_id in the test_page\n",
    "    def check_if_on_testpage(X):\n",
    "      if test_page in explicit_double_check[explicit_double_check.buzz_id == X]['display_name'].unique():\n",
    "        return 1\n",
    "      else:\n",
    "        return 0\n",
    "\n",
    "    test_df['already_posted'] = test_df['buzz_id'].apply(check_if_on_testpage)\n",
    "    # keep only buzzes that've not yet been posted on test_page\n",
    "    test_df = test_df[test_df.already_posted == 0]\n",
    "    test_df = test_df[~(test_df['page'] == test_page)]\n",
    "    test_df = test_df[~(test_df['next_page'] == test_page)]\n",
    "    print('Filtered already posted buzzes...')\n",
    "\n",
    "    # manually assign next_page = test_page, since we are gonna use this dataframe to make predictions; convert the column to type \"categorical\"\n",
    "    test_df['next_page'] = test_page\n",
    "    test_df['next_page'] = test_df['next_page'].astype('category', categories = all_pages)\n",
    "\n",
    "    if skip_title_fit == 0:\n",
    "        X_test = test_df[train_features + ['title']]\n",
    "    else:\n",
    "        X_test = test_df[train_features]\n",
    "\n",
    "    print('Assigned next page...')\n",
    "\n",
    "    # convert current and next page names into Pandas dummy variables/columns (pd.get_dummies)\n",
    "    X_test = pd.get_dummies(data = X_test, columns=['page', 'next_page'])\n",
    "    print('Converted pages into pandas dummies...')\n",
    "    \n",
    "    if skip_title_fit == 0:\n",
    "        # extract title features for the test dataframe\n",
    "        X_tfidf = title_vect.transform(X_test['title'])\n",
    "        X_test = X_test.drop(['title'], axis = 1)\n",
    "        X_test = scipy.sparse.hstack([X_tfidf, X_test])\n",
    "        print('Extracted title features for dataframe...')\n",
    "    else:\n",
    "        print('Skipping title fit...')\n",
    "        \n",
    "    # make predictions on a link's performance, if it were to be posted on the target page, given the perfoprmance in the previous page!\n",
    "    test_df['predicted_shares'] = clf.predict(X_test)\n",
    "\n",
    "    # convert the log-scaled predictions back to exponents for computing accuracy and errors!\n",
    "    test_df['predicted_shares'] = (np.exp(test_df['predicted_shares'])-1).astype(int)\n",
    "    test_df['predicted_first_hour_shares'] = test_df['predicted_shares']\n",
    "\n",
    "    # list of columns to include in the output dataframe\n",
    "    result_cols = ['page', 'title', 'buzz_id', 'predicted_first_hour_shares', 'URL', 'next_page', 'created_at']\n",
    "    test_df = test_df[result_cols].drop_duplicates(subset=['buzz_id'], keep = 'last')\n",
    "\n",
    "    df = title_df[['name', 'display_name']].dropna()\n",
    "    df.columns = ['title', 'page']\n",
    "\n",
    "    # training samples: class 1 = links from test_page; class 0 = random links that are from else where\n",
    "    X_1 = df[df.page == test_page]\n",
    "    X_0 = df[~(df.page == test_page)].sample(n=len(X_1))\n",
    "    X_all = pd.concat([X_1, X_0])\n",
    "\n",
    "    def get_class(X, page):\n",
    "        return 1 if X == page else 0\n",
    "\n",
    "    X_all['class'] = X_all['page'].apply(get_class, page=test_page)\n",
    "\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=word_tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultinomialNB()),\n",
    "    ])\n",
    "\n",
    "    X = X_all['title']\n",
    "    y = X_all['class']\n",
    "\n",
    "    title_clf = text_clf.fit(X, y)\n",
    "    temp = test_df\n",
    "\n",
    "    pred = pd.DataFrame(title_clf.predict_proba(temp['title']), columns=title_clf.classes_)\n",
    "    temp['prob'] = pred[1].values\n",
    "\n",
    "    print('')\n",
    "    print('Top ', max_suggestions, ' links that are most relevant for FB ', test_page, '; (test sample size: ', len(temp), ')')\n",
    "    print('--------------')\n",
    "    temp = temp.sort_values('prob', ascending = False)[:max_suggestions*10]\n",
    "    temp['identifier'] = '{page}_{buzz_id}_{next_page}_{shares}'.format(\n",
    "        page=temp['page'].astype(str),\n",
    "        buzz_id=temp['buzz_id'].map(str),\n",
    "        next_page=temp['next_page'].astype(str),\n",
    "        shares=temp['predicted_first_hour_shares'].map(str)\n",
    "    )\n",
    "\n",
    "#     temp = temp.sort_values('predicted_first_hour_shares', ascending = False)[:max_suggestions]\n",
    "    # The links in this table are sorted only based on the title fit (indicated by the probability or prob) to the target page under consideration.\n",
    "    temp = temp.sort_values('prob', ascending = False)[:max_suggestions]\n",
    "    print(temp)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filepat = \"../../mono/proto_recommendation_api/\"\n",
    "    query_1 = pd.read_csv(filepat + 'fixtures/routing_query_1.csv', encoding='utf-8', engine='python')\n",
    "    print('Loaded query 1...')\n",
    "    title_sources = pd.read_csv(filepat + 'fixtures/routing_title_sources.csv', encoding='utf-8', engine='python')\n",
    "    print('Loaded title sources...')\n",
    "    explicit_dc = pd.read_csv(filepat + 'fixtures/routing_explicit_double_check.csv', encoding='utf-8', engine='python')\n",
    "    print('Loaded explicit double check...')\n",
    "    print('')\n",
    "    print('=> RUNNING RECOMMENDATIONS')\n",
    "    print('')\n",
    "\n",
    "    fake_data_sets = [query_1, title_sources, explicit_dc]\n",
    "    fake_params = {4\n",
    "        'test_page': 'Nifty',\n",
    "        'lookback_limit': '2017-01-01',\n",
    "        'max_suggestions': 20,\n",
    "    }\n",
    "    recommend_route(datasets=fake_data_sets, params=fake_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# external_id = 1318800798260799_2282584891882380: actual 1st hour shares = 325,   Identifier: BuzzFeed_4461993_BuzzFeed Video_389\n",
    "# external_id = 1318800798260799_2282590138548522: actual 1st hour shares = 248,   Identifier: BuzzFeed Australia_4494602_BuzzFeed Video_408\n",
    "y_test = [325, 248]\n",
    "y_pred = [389, 408]\n",
    "from sklearn import metrics\n",
    "accuracy = metrics.r2_score(y_test, y_pred)\n",
    "#     print(\"Cross-Predicted Accuracy (R2):\", accuracy)\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# print(\"Mean Absolute Error: \", mean_absolute_error(y, predictions))\n",
    "error_percent = mean_absolute_error(y_test, y_pred)/np.mean(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-9.0172035756451336, 0.39092495636998253)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy, error_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286.5"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
