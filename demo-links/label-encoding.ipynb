{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINEAR REGRESSION!\n",
      "Performance stats with page names, and also cross-promotion order included!\n",
      "No. of data-points =  38769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######FIT THE MODEL ON HISTORICAL DATA!!!\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.stats.api import ols\n",
    "from sklearn import datasets, linear_model\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "import math\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import scipy.sparse\n",
    "\n",
    "filepath = '/Users/eshwarchandrasekharan/Desktop/repo/predict-pop/models-links/'\n",
    "train_df = pd.read_csv(filepath + 'radshift-links-previous-to-next-page-info.csv')\n",
    "# train_df = build_df\n",
    "train_df = train_df.fillna(0)\n",
    "all_pages = train_df['page'].unique()\n",
    "\n",
    "###encode page info - labels\n",
    "from sklearn import preprocessing\n",
    "curr_le = preprocessing.LabelEncoder()\n",
    "curr_le = curr_le.fit(train_df['page'])\n",
    "train_df['current_page_label'] = curr_le.transform(train_df['page'])\n",
    "next_le = preprocessing.LabelEncoder()\n",
    "next_le = next_le.fit(train_df['next_page'])\n",
    "train_df['next_page_label'] = next_le.transform(train_df['next_page'])\n",
    "\n",
    "one_hour_features = [\n",
    "       'consumptions_by_type__link_clicks',\n",
    "      'consumptions_by_type__other_clicks',\n",
    "      'reactions_like_total',\n",
    "      'stories_by_action_type__comment',\n",
    "      'stories_by_action_type__like',\n",
    "       'stories_by_action_type__share', \n",
    "        'engaged_fan',\n",
    "      'fan_reach', \n",
    "    'impressions',\n",
    "      'impressions_fan',\n",
    "                ]\n",
    "\n",
    "# cross_promotion_features = all_pages\n",
    "\n",
    "from sklearn import linear_model\n",
    "clf = linear_model.LinearRegression()\n",
    "print(\"LINEAR REGRESSION!\")\n",
    "\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# clf = DecisionTreeRegressor(max_depth = 5)\n",
    "# print(\"TREE REGRESSION!\")\n",
    "\n",
    "print(\"Performance stats with page names, and also cross-promotion order included!\")\n",
    "# print(\"WITH cross-promotion features\")\n",
    "\n",
    "page_infos = ['current_page_label', 'next_page_label']\n",
    "# page_infos = ['page', 'next_page']\n",
    "# page_infos = ['display_name', 'next_page']   ###mode report variables\n",
    "\n",
    "train_features = one_hour_features\n",
    "\n",
    "# include_page_info = 0\n",
    "include_page_info = 1\n",
    "if include_page_info == 1:\n",
    "  train_features = one_hour_features + page_infos\n",
    "# train_features = cross_promotion_features + one_hour_features\n",
    "\n",
    "log_scale = 1\n",
    "# log_scale = 0\n",
    "\n",
    "if log_scale == 1:\n",
    "    for feat in one_hour_features:\n",
    "        train_df[feat] = np.log(train_df[feat] + 1)\n",
    "\n",
    "cross_promote = 0\n",
    "# cross_promote = 1\n",
    "\n",
    "if cross_promote == 1:\n",
    "    for feat in cross_promotion_features:\n",
    "        train_features.append(feat)\n",
    "\n",
    "train_df['clicks_bucket'] = np.log(train_df['next_clicks'] + 1)\n",
    "train_df['share_bucket'] = np.log(train_df['next_shares']+1)\n",
    "\n",
    "print(\"No. of data-points = \", len(train_df))\n",
    "\n",
    "###generate DFs for analysis - X and Y\n",
    "X = train_df[train_features + ['title']]\n",
    "y = train_df['share_bucket']\n",
    "# y = train_df['next_shares']\n",
    "####convert page_infos into dummies\n",
    "# if include_page_info == 1:\n",
    "#   X = pd.get_dummies(data = X, columns=['display_name', 'next_page']) ###mode report variables\n",
    "#   X = pd.get_dummies(data = X, columns=['page', 'next_page']) \n",
    "\n",
    "####include title features!\n",
    "title_vect = Pipeline([\n",
    "                     ('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ])\n",
    "\n",
    "##\n",
    "X_tfidf = title_vect.fit_transform(X['title'])\n",
    "X = X.drop(['title'], axis = 1)\n",
    "X = scipy.sparse.hstack([X_tfidf, X])\n",
    "\n",
    "clf = clf.fit(X, y)\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>title</th>\n",
       "      <th>buzz_id</th>\n",
       "      <th>predicted_shares</th>\n",
       "      <th>next_page</th>\n",
       "      <th>test_page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26826</th>\n",
       "      <td>BuzzFeed News</td>\n",
       "      <td>Dakota Access Pipeline Will Be Rerouted In A V...</td>\n",
       "      <td>4412034.0</td>\n",
       "      <td>3037</td>\n",
       "      <td>BuzzFeed</td>\n",
       "      <td>BuzzFeed Animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35383</th>\n",
       "      <td>BuzzFeed</td>\n",
       "      <td>Accidental Censorship Of Olympic Divers Makes ...</td>\n",
       "      <td>4330863.0</td>\n",
       "      <td>2946</td>\n",
       "      <td>BuzzFeed Video</td>\n",
       "      <td>BuzzFeed Animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29878</th>\n",
       "      <td>BuzzFeed News</td>\n",
       "      <td>A Zoo Is Streaming Red Pandas On Tuesday To Di...</td>\n",
       "      <td>4392765.0</td>\n",
       "      <td>2859</td>\n",
       "      <td>BuzzFeed</td>\n",
       "      <td>BuzzFeed Animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37519</th>\n",
       "      <td>BuzzFeed News</td>\n",
       "      <td>Stop Trying To Rescue Baby Animals, Wildlife O...</td>\n",
       "      <td>4300956.0</td>\n",
       "      <td>2822</td>\n",
       "      <td>BuzzFeed</td>\n",
       "      <td>BuzzFeed Animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3602</th>\n",
       "      <td>BuzzFeed News</td>\n",
       "      <td>These Are The Victims Of The Portland Train St...</td>\n",
       "      <td>4543535.0</td>\n",
       "      <td>2490</td>\n",
       "      <td>BuzzFeed</td>\n",
       "      <td>BuzzFeed Animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29785</th>\n",
       "      <td>Cheeky</td>\n",
       "      <td>Thousands Of Americans Crashed Canada's Immigr...</td>\n",
       "      <td>4393851.0</td>\n",
       "      <td>2456</td>\n",
       "      <td>BuzzFeed Philippines</td>\n",
       "      <td>BuzzFeed Animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38322</th>\n",
       "      <td>BuzzFeed</td>\n",
       "      <td>In Their Words: The Swedish Heroes Who Caught ...</td>\n",
       "      <td>4271370.0</td>\n",
       "      <td>2340</td>\n",
       "      <td>BuzzFeed Video</td>\n",
       "      <td>BuzzFeed Animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33405</th>\n",
       "      <td>BuzzFeed</td>\n",
       "      <td>Ohio Police Share Graphic Photos Of Couple Ove...</td>\n",
       "      <td>4352719.0</td>\n",
       "      <td>1982</td>\n",
       "      <td>BuzzFeed Video</td>\n",
       "      <td>BuzzFeed Animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29590</th>\n",
       "      <td>BuzzFeed News</td>\n",
       "      <td>Middle Schoolers Chant \"Build The Wall\" In Caf...</td>\n",
       "      <td>4395258.0</td>\n",
       "      <td>1942</td>\n",
       "      <td>BuzzFeed Video</td>\n",
       "      <td>BuzzFeed Animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37069</th>\n",
       "      <td>BuzzFeed News</td>\n",
       "      <td>You Should Probably Check Your Pok√©mon Go Priv...</td>\n",
       "      <td>4311070.0</td>\n",
       "      <td>1928</td>\n",
       "      <td>BuzzFeed Video</td>\n",
       "      <td>BuzzFeed Animals</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                page                                              title  \\\n",
       "26826  BuzzFeed News  Dakota Access Pipeline Will Be Rerouted In A V...   \n",
       "35383       BuzzFeed  Accidental Censorship Of Olympic Divers Makes ...   \n",
       "29878  BuzzFeed News  A Zoo Is Streaming Red Pandas On Tuesday To Di...   \n",
       "37519  BuzzFeed News  Stop Trying To Rescue Baby Animals, Wildlife O...   \n",
       "3602   BuzzFeed News  These Are The Victims Of The Portland Train St...   \n",
       "29785         Cheeky  Thousands Of Americans Crashed Canada's Immigr...   \n",
       "38322       BuzzFeed  In Their Words: The Swedish Heroes Who Caught ...   \n",
       "33405       BuzzFeed  Ohio Police Share Graphic Photos Of Couple Ove...   \n",
       "29590  BuzzFeed News  Middle Schoolers Chant \"Build The Wall\" In Caf...   \n",
       "37069  BuzzFeed News  You Should Probably Check Your Pok√©mon Go Priv...   \n",
       "\n",
       "         buzz_id  predicted_shares             next_page         test_page  \n",
       "26826  4412034.0              3037              BuzzFeed  BuzzFeed Animals  \n",
       "35383  4330863.0              2946        BuzzFeed Video  BuzzFeed Animals  \n",
       "29878  4392765.0              2859              BuzzFeed  BuzzFeed Animals  \n",
       "37519  4300956.0              2822              BuzzFeed  BuzzFeed Animals  \n",
       "3602   4543535.0              2490              BuzzFeed  BuzzFeed Animals  \n",
       "29785  4393851.0              2456  BuzzFeed Philippines  BuzzFeed Animals  \n",
       "38322  4271370.0              2340        BuzzFeed Video  BuzzFeed Animals  \n",
       "33405  4352719.0              1982        BuzzFeed Video  BuzzFeed Animals  \n",
       "29590  4395258.0              1942        BuzzFeed Video  BuzzFeed Animals  \n",
       "37069  4311070.0              1928        BuzzFeed Video  BuzzFeed Animals  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####make predictions on performance if posted on the target page, given the perfoprmance in the previous page!\n",
    "test_page = 'BuzzFeed Animals'\n",
    "max_suggestions = 10\n",
    "test_df = train_df\n",
    "\n",
    "test_df['test_page'] = test_page\n",
    "test_df['next_page_label'] = next_le.transform(test_df['test_page'])\n",
    "\n",
    "X_test = test_df[train_features + ['title']]\n",
    "\n",
    "###save for later!\n",
    "# curr_page = X_test['page']\n",
    "\n",
    "# if include_page_info == 1:\n",
    "# #   X_test = pd.get_dummies(data = X_test, columns=['display_name', 'next_page'])\n",
    "#   X_test = pd.get_dummies(data = X_test, columns=['page', 'next_page'])\n",
    "    \n",
    "# for page in test_df['next_page'].unique():\n",
    "#   X_test['next_page_' + page] = 0\n",
    "\n",
    "# X_test['next_page_' + test_page] = 1\n",
    "\n",
    "##title features\n",
    "X_tfidf = title_vect.transform(X_test['title'])\n",
    "X_test = X_test.drop(['title'], axis = 1)\n",
    "X_test = scipy.sparse.hstack([X_tfidf, X_test])\n",
    "###\n",
    "\n",
    "test_df['predicted_shares'] = clf.predict(X_test)\n",
    "test_df['predicted_shares'] = (np.exp(test_df['predicted_shares'])-1).astype(int)\n",
    "\n",
    "result_cols = ['page', 'title', 'buzz_id', 'predicted_shares', 'next_page', 'test_page']\n",
    "# result_cols = ['display_name', 'title', 'buzz_id', 'predicted_shares']\n",
    "# test_df = test_df[~(test_df['page'] == test_page)]\n",
    "# test_df = test_df[~(test_df['display_name'] == test_page)]\n",
    "# ###only keep candidates that are from pages which have sourced test_page in the past!\n",
    "top_k_similar = 10\n",
    "candidates = test_df[test_df.next_page == test_page][['page', 'next_page']]\n",
    "candidates['freq'] = candidates.groupby('page')['page'].transform('count')\n",
    "candidates = candidates.drop_duplicates()\n",
    "candidate_pages = candidates.sort_values('freq', ascending = False)['page'][:top_k_similar]\n",
    "# candidate_pages = test_df[test_df.next_page == test_page]['page'].unique()\n",
    "test_df = test_df[test_df['page'].isin(candidate_pages)]\n",
    "###\n",
    "test_df = test_df[result_cols].drop_duplicates(subset=['buzz_id'], keep = 'last')\n",
    "test_df.sort_values('predicted_shares', ascending = False)[:max_suggestions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(\"Label encoding!!!\")\n",
    "# from sklearn import preprocessing\n",
    "# le = preprocessing.LabelEncoder()\n",
    "\n",
    "# le = le.fit(all_pages)\n",
    "# le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# candidates = test_df[test_df.next_page == test_page][['page', 'next_page']]\n",
    "# candidates['freq'] = candidates.groupby('page')['page'].transform('count')\n",
    "# candidates = candidates.drop_duplicates()\n",
    "# candidates.sort_values('freq', ascending = False)['page'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Buy Me That', 'BuzzFeed Style', 'BuzzFeed', 'BuzzFeed Video',\n",
       "       'BuzzFeed Philippines', 'BuzzFeed Food', 'Tasty',\n",
       "       'BuzzFeed Parents', 'BuzzFeed UK News', 'BuzzFeed Australia',\n",
       "       'BuzzFeed IRL', 'BuzzFeed Animals', 'BuzzFeed UK',\n",
       "       'Obsessed by BuzzFeed', 'SOML', 'BuzzFeed News',\n",
       "       'BuzzFeed Entertainment', 'Quizzes En Espa√±ol', 'BuzzFeed M√©xico',\n",
       "       'BuzzFeed DIY', 'BuzzFeed Books', 'BuzzFeed Quiz', 'BuzzFeed World',\n",
       "       'BuzzFeed Rewind', 'BuzzFeed LGBT', 'BuzzFeed Celeb',\n",
       "       'BuzzFeed Geeky', 'BuzzFeed Ladylike', 'BuzzFeed Espa√±ol',\n",
       "       'BuzzFeed Science', 'Cheeky', 'BuzzFeed Health',\n",
       "       'BuzzFeed Community', 'BuzzFeed Brasil', 'BuzzFeed India',\n",
       "       'Cocoa Butter', 'BuzzFeed Politics', 'BuzzFeed Weddings',\n",
       "       'BuzzFeed Japan News', 'LOLA', 'BuzzFeed Scotland',\n",
       "       'BuzzFeed Oz Politics', 'BuzzFeed Canada', 'BuzzFeed Oz News',\n",
       "       'BuzzFeed Espa√±a', 'BuzzFeed UK Politics', 'Pero Like',\n",
       "       'BuzzFeed France', 'Reasons to Smile', 'Tasty Miam',\n",
       "       'BuzzFeed Tech', 'Tasty Demais', 'BuzzFeed Japan',\n",
       "       'BuzzFeed Deutschland', 'BuzzFeed Unsolved', 'Nifty',\n",
       "       'BuzzFeed Partner', 'Bien Tasty', 'BuzzFeed Reader',\n",
       "       'Another Round', 'BuzzFeed News BR', 'Einfach Tasty',\n",
       "       'BuzzFeed France News', 'BuzzFeed San Francisco', 'BuzzFeed Pink',\n",
       "       'Ohmygod Yaaa', 'Proper Tasty', 'Kristin Chirico', 'Adam Ellis',\n",
       "       'Top Knot', 'BuzzFeed BFF', 'Buzzed Feed', 'BuzzFeed Sweaty',\n",
       "       'BuzzFeed Steven L.', 'The Try Guys', 'BuzzReads'], dtype=object)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
