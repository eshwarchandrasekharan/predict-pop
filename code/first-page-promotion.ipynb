{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eshwarchandrasekharan/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "###predict where the link will be posted given just the title!\n",
    "import pandas as pd\n",
    "from pandas.stats.api import ols\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import math\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "\n",
    "train = pd.read_csv('jan_may_2017_links_cross_posting_info.csv')\n",
    "all_pages = list(train.buzz_account_display_name.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['buzz_external_id', 'buzz_post_parent_external_id',\n",
       "       'buzz_post_created_at', 'buzz_account_display_name',\n",
       "       'buzz_post_type_name', 'buzz_post_buzz_id', 'buzz_campaign_uri',\n",
       "       'buzz_campaign_name', 'ext_table1_stats_date', 'ext_external_id',\n",
       "       ...\n",
       "       'Tasty One-Pot', 'BuzzFeed Sweaty', 'Tasty Junior',\n",
       "       'Oh Great, More Politics', 'Einfach Tasty', 'BuzzFeed Steven L.',\n",
       "       'The Try Guys', 'Ohmygod Yaaa', 'BuzzFeed Chloe', 'Nifty Science'],\n",
       "      dtype='object', length=113)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "starting_features = ['buzz_external_id', 'buzz_account_display_name',\n",
    "       'buzz_post_type_name', 'buzz_post_buzz_id', 'buzz_campaign_uri',\n",
    "       'buzz_campaign_name',]\n",
    "df = train[starting_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###for each buzz_id, get the external ID which was posted FIRST!!!\n",
    "train_grouped = train.groupby(['buzz_post_buzz_id']).agg({'buzz_post_created_at':'min'})\n",
    "\n",
    "train_grouped = train_grouped.reset_index()\n",
    "\n",
    "train_grouped = train_grouped.rename(columns={'buzz_post_created_at':'first_post'})\n",
    "\n",
    "df = pd.merge(train, train_grouped, how='left', on=['buzz_post_buzz_id'])\n",
    "\n",
    "df = df[df['buzz_post_created_at'] == df['first_post']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###predict where the link will be posted given just the title!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page:  BuzzFeed Quiz #links:  1316 Accuracy:  0.881848427238 Precision:  0.902321037665 Recall:  0.883009788952 Fscore: 0.880307450512\n",
      "Page:  BuzzFeed Japan #links:  966 Accuracy:  0.783133913787 Precision:  0.865833765726 Recall:  0.790795198814 Fscore: 0.751409797241\n",
      "Page:  BuzzFeed UK #links:  560 Accuracy:  0.683035714286 Precision:  0.767226560248 Recall:  0.68376647489 Fscore: 0.654709584089\n",
      "Page:  BuzzFeed Brasil #links:  1022 Accuracy:  0.943220946915 Precision:  0.94886025067 Recall:  0.944271353994 Fscore: 0.94295810928\n",
      "Page:  BuzzFeed Japan News #links:  616 Accuracy:  0.721800419617 Precision:  0.833584901167 Recall:  0.737795488652 Fscore: 0.679828366954\n",
      "Page:  BuzzFeed Deutschland #links:  581 Accuracy:  0.973320070734 Precision:  0.972841210883 Recall:  0.974329853259 Fscore: 0.973129669579\n",
      "Page:  BuzzFeed News #links:  649 Accuracy:  0.81824090638 Precision:  0.834572991192 Recall:  0.820735206815 Fscore: 0.81590332916\n",
      "Page:  BuzzFeed #links:  978 Accuracy:  0.638532182104 Precision:  0.71634592794 Recall:  0.640663523629 Fscore: 0.603391372181\n",
      "Page:  BuzzFeed Canada #links:  251 Accuracy:  0.655647058824 Precision:  0.73361781677 Recall:  0.664958597053 Fscore: 0.626807967318\n",
      "Page:  BuzzFeed Video #links:  121 Accuracy:  0.702666666667 Precision:  0.779103457172 Recall:  0.706388333888 Fscore: 0.681517721394\n",
      "Page:  Cocoa Butter #links:  177 Accuracy:  0.70626984127 Precision:  0.778933823201 Recall:  0.709496689026 Fscore: 0.685850312158\n",
      "Page:  BuzzFeed Oz Politics #links:  212 Accuracy:  0.815836101883 Precision:  0.852237136741 Recall:  0.81638640161 Fscore: 0.808775844702\n",
      "Page:  Quizzes En Español #links:  195 Accuracy:  0.920512820513 Precision:  0.931601201557 Recall:  0.922675058859 Fscore: 0.919217256383\n",
      "Page:  Obsessed by BuzzFeed #links:  292 Accuracy:  0.715663354763 Precision:  0.772625634501 Recall:  0.715872310887 Fscore: 0.696968817821\n",
      "Page:  BuzzFeed News BR #links:  278 Accuracy:  0.953116883117 Precision:  0.956108304495 Recall:  0.954793008715 Fscore: 0.952961528101\n",
      "Page:  Buy Me That #links:  362 Accuracy:  0.812195585997 Precision:  0.850460150356 Recall:  0.815436007494 Fscore: 0.80673398042\n",
      "Page:  BuzzFeed UK Politics #links:  256 Accuracy:  0.832088989442 Precision:  0.867560936428 Recall:  0.833762740362 Fscore: 0.825175539833\n",
      "Page:  BuzzFeed Australia #links:  718 Accuracy:  0.671255827506 Precision:  0.741237572297 Recall:  0.675736237255 Fscore: 0.645971967507\n",
      "Page:  BuzzFeed Rewind #links:  220 Accuracy:  0.713636363636 Precision:  0.790404103549 Recall:  0.718208519265 Fscore: 0.693261570856\n",
      "Page:  BuzzFeed Books #links:  210 Accuracy:  0.671428571429 Precision:  0.756510415383 Recall:  0.676606630139 Fscore: 0.644653769983\n",
      "Page:  BuzzFeed DIY #links:  260 Accuracy:  0.725 Precision:  0.771248364006 Recall:  0.729518830845 Fscore: 0.712014493998\n",
      "Page:  BuzzFeed México #links:  665 Accuracy:  0.939097744361 Precision:  0.945606041981 Recall:  0.940452669827 Fscore: 0.938548844899\n",
      "Page:  BuzzFeed Food #links:  545 Accuracy:  0.75504587156 Precision:  0.826744226978 Recall:  0.757011024254 Fscore: 0.739954597363\n",
      "Page:  BuzzFeed Celeb #links:  480 Accuracy:  0.73125 Precision:  0.7719026555 Recall:  0.735382548309 Fscore: 0.720771797202\n",
      "Page:  BuzzFeed Animals #links:  268 Accuracy:  0.754192872117 Precision:  0.832143482255 Recall:  0.75820410281 Fscore: 0.737531324841\n",
      "Page:  BuzzFeed France #links:  546 Accuracy:  0.953286071726 Precision:  0.957389959717 Recall:  0.953888954211 Fscore: 0.95303045692\n",
      "Page:  BuzzFeed Español #links:  386 Accuracy:  0.903979353979 Precision:  0.918347337661 Recall:  0.909183166699 Fscore: 0.902775478712\n",
      "Page:  BuzzFeed Style #links:  409 Accuracy:  0.691824751581 Precision:  0.787504518682 Recall:  0.691948046593 Fscore: 0.662100726219\n",
      "Page:  Cheeky #links:  364 Accuracy:  0.786834094368 Precision:  0.829685481602 Recall:  0.790723741513 Fscore: 0.779918116654\n",
      "Page:  BuzzFeed Community #links:  348 Accuracy:  0.708592132505 Precision:  0.782502093881 Recall:  0.713411373512 Fscore: 0.686712734994\n",
      "Page:  BuzzFeed Weddings #links:  112 Accuracy:  0.709288537549 Precision:  0.801703738513 Recall:  0.718270618271 Fscore: 0.684151586438\n",
      "Page:  BuzzFeed World #links:  243 Accuracy:  0.837670068027 Precision:  0.859731087385 Recall:  0.837543327822 Fscore: 0.832991183149\n",
      "Page:  BuzzFeed Entertainment #links:  344 Accuracy:  0.75300511509 Precision:  0.794858013211 Recall:  0.759091002741 Fscore: 0.743666427341\n",
      "Page:  BuzzFeed Geeky #links:  142 Accuracy:  0.66539408867 Precision:  0.771185447085 Recall:  0.673842199467 Fscore: 0.619698441212\n",
      "Page:  BuzzFeed Parents #links:  189 Accuracy:  0.687908961593 Precision:  0.764333441864 Recall:  0.690626272183 Fscore: 0.663227920264\n",
      "Page:  BuzzFeed Health #links:  298 Accuracy:  0.716355932203 Precision:  0.775280801437 Recall:  0.720229431548 Fscore: 0.697888381444\n",
      "Page:  BuzzFeed España #links:  357 Accuracy:  0.911561032864 Precision:  0.925418380473 Recall:  0.911945200665 Fscore: 0.910004950789\n",
      "Page:  BuzzFeed Politics #links:  378 Accuracy:  0.850280701754 Precision:  0.869870403812 Recall:  0.855387577056 Fscore: 0.847829565287\n",
      "Page:  Pero Like #links:  45 Accuracy:  0.533333333333 Precision:  0.587083333333 Recall:  0.56125 Fscore: 0.465446497946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eshwarchandrasekharan/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page:  BuzzFeed UK News #links:  366 Accuracy:  0.811162532395 Precision:  0.836445874121 Recall:  0.816218254474 Fscore: 0.807751165153\n",
      "Page:  BuzzFeed India #links:  620 Accuracy:  0.749193548387 Precision:  0.80937220307 Recall:  0.750905144316 Fscore: 0.735298544006\n",
      "Page:  BuzzFeed France News #links:  217 Accuracy:  0.926109936575 Precision:  0.934052937248 Recall:  0.930020827413 Fscore: 0.924961706114\n",
      "Page:  BuzzFeed Scotland #links:  96 Accuracy:  0.708684210526 Precision:  0.775806946322 Recall:  0.708423798424 Fscore: 0.67591983064\n",
      "Page:  LOLA #links:  37 Accuracy:  0.839285714286 Precision:  0.882738095238 Recall:  0.846666666667 Fscore: 0.828246753247\n",
      "Page:  BuzzFeed Philippines #links:  161 Accuracy:  0.723674242424 Precision:  0.770755583788 Recall:  0.72806048634 Fscore: 0.709172874601\n",
      "Page:  BuzzFeed Oz News #links:  179 Accuracy:  0.737380952381 Precision:  0.783808723067 Recall:  0.738766772331 Fscore: 0.723283771125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eshwarchandrasekharan/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/eshwarchandrasekharan/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page:  BuzzFeed Reader #links:  46 Accuracy:  0.727777777778 Precision:  0.738293650794 Recall:  0.760595238095 Fscore: 0.684736929737\n",
      "Page:  BuzzFeed Pink #links:  25 Accuracy:  0.84 Precision:  0.835833333333 Recall:  0.833333333333 Fscore: 0.797301587302\n",
      "Page:  BuzzFeed LGBT #links:  114 Accuracy:  0.723913043478 Precision:  0.770454107081 Recall:  0.722161380286 Fscore: 0.699222646466\n",
      "Page:  BuzzFeed Partner #links:  89 Accuracy:  0.68660130719 Precision:  0.704788479494 Recall:  0.693926767677 Fscore: 0.658856502832\n",
      "Page:  BuzzFeed Tech #links:  211 Accuracy:  0.82430786268 Precision:  0.845696737931 Recall:  0.840815338945 Fscore: 0.821566118461\n",
      "Page:  BuzzFeed Science #links:  73 Accuracy:  0.705714285714 Precision:  0.735265151515 Recall:  0.72125 Fscore: 0.694241183653\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pages = []\n",
    "num_links = []\n",
    "average_acc = []\n",
    "average_pre = []\n",
    "average_rec = []\n",
    "average_f1 = []\n",
    "cv_folds = 10\n",
    "results = pd.DataFrame()\n",
    "page_clf = []\n",
    "\n",
    "for test_page in all_pages:\n",
    "#     test_page = \"BuzzFeed\"\n",
    "    X_1 = df[df.buzz_account_display_name == test_page]\n",
    "    X_0 = df[~(df.buzz_account_display_name == test_page)].sample(n=len(X_1))\n",
    "    if len(X_1) < 2.5*cv_folds:\n",
    "        continue\n",
    "    X_1 = X_1[['buzz_campaign_name', 'buzz_account_display_name']]\n",
    "    X_0 = X_0[['buzz_campaign_name', 'buzz_account_display_name']]\n",
    "\n",
    "    X_all = pd.concat([X_1, X_0])\n",
    "\n",
    "    def get_class(X, page):\n",
    "        if X == page:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    X_all['class'] = X_all['buzz_account_display_name'].apply(get_class, page = test_page)\n",
    "\n",
    "    X_all = X_all.dropna()\n",
    "\n",
    "    text_clf = Pipeline([\n",
    "                         ('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', MultinomialNB()),\n",
    "                        ])\n",
    "\n",
    "    fold = 0\n",
    "    kf = KFold(n_splits = cv_folds, shuffle = True)\n",
    "\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    fscore = []\n",
    "\n",
    "    X = X_all['buzz_campaign_name']\n",
    "    y = X_all['class']\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "#         print(\"Fold = \", fold)\n",
    "        fold+= 1\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        text_clf.fit(X_train, y_train)\n",
    "        y_pred = text_clf.predict(X_test)\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        prec, rec, fs, sup = precision_recall_fscore_support(y_test, y_pred)\n",
    "        accuracy.append(acc)\n",
    "        precision.append(prec)\n",
    "        recall.append(rec)\n",
    "        fscore.append(fs)\n",
    "    print(\"Page: \", test_page, \"#links: \", len(X_1), \"Accuracy: \", np.mean(accuracy), \"Precision: \", np.mean(precision), \"Recall: \", np.mean(recall), \"Fscore:\", np.mean(fscore))\n",
    "    num_links.append(len(X_1))\n",
    "    average_acc.append(np.mean(accuracy))\n",
    "    average_pre.append(np.mean(precision))\n",
    "    average_rec.append(np.mean(recall))\n",
    "    average_f1.append(np.mean(fscore))\n",
    "    pages.append(test_page)\n",
    "    page_clf.append(text_clf)\n",
    "results['page'] = pages\n",
    "results['num_links'] = num_links\n",
    "results['accuracy'] = average_acc\n",
    "results['precision'] = average_pre\n",
    "results['recall'] = average_rec\n",
    "results['fscore'] = average_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>num_links</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>fscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BuzzFeed Quiz</td>\n",
       "      <td>1316</td>\n",
       "      <td>0.881848</td>\n",
       "      <td>0.902321</td>\n",
       "      <td>0.883010</td>\n",
       "      <td>0.880307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BuzzFeed Japan</td>\n",
       "      <td>966</td>\n",
       "      <td>0.783134</td>\n",
       "      <td>0.865834</td>\n",
       "      <td>0.790795</td>\n",
       "      <td>0.751410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BuzzFeed UK</td>\n",
       "      <td>560</td>\n",
       "      <td>0.683036</td>\n",
       "      <td>0.767227</td>\n",
       "      <td>0.683766</td>\n",
       "      <td>0.654710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BuzzFeed Brasil</td>\n",
       "      <td>1022</td>\n",
       "      <td>0.943221</td>\n",
       "      <td>0.948860</td>\n",
       "      <td>0.944271</td>\n",
       "      <td>0.942958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BuzzFeed Japan News</td>\n",
       "      <td>616</td>\n",
       "      <td>0.721800</td>\n",
       "      <td>0.833585</td>\n",
       "      <td>0.737795</td>\n",
       "      <td>0.679828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BuzzFeed Deutschland</td>\n",
       "      <td>581</td>\n",
       "      <td>0.973320</td>\n",
       "      <td>0.972841</td>\n",
       "      <td>0.974330</td>\n",
       "      <td>0.973130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BuzzFeed News</td>\n",
       "      <td>649</td>\n",
       "      <td>0.818241</td>\n",
       "      <td>0.834573</td>\n",
       "      <td>0.820735</td>\n",
       "      <td>0.815903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BuzzFeed</td>\n",
       "      <td>978</td>\n",
       "      <td>0.638532</td>\n",
       "      <td>0.716346</td>\n",
       "      <td>0.640664</td>\n",
       "      <td>0.603391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BuzzFeed Canada</td>\n",
       "      <td>251</td>\n",
       "      <td>0.655647</td>\n",
       "      <td>0.733618</td>\n",
       "      <td>0.664959</td>\n",
       "      <td>0.626808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BuzzFeed Video</td>\n",
       "      <td>121</td>\n",
       "      <td>0.702667</td>\n",
       "      <td>0.779103</td>\n",
       "      <td>0.706388</td>\n",
       "      <td>0.681518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Cocoa Butter</td>\n",
       "      <td>177</td>\n",
       "      <td>0.706270</td>\n",
       "      <td>0.778934</td>\n",
       "      <td>0.709497</td>\n",
       "      <td>0.685850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BuzzFeed Oz Politics</td>\n",
       "      <td>212</td>\n",
       "      <td>0.815836</td>\n",
       "      <td>0.852237</td>\n",
       "      <td>0.816386</td>\n",
       "      <td>0.808776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Quizzes En Español</td>\n",
       "      <td>195</td>\n",
       "      <td>0.920513</td>\n",
       "      <td>0.931601</td>\n",
       "      <td>0.922675</td>\n",
       "      <td>0.919217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Obsessed by BuzzFeed</td>\n",
       "      <td>292</td>\n",
       "      <td>0.715663</td>\n",
       "      <td>0.772626</td>\n",
       "      <td>0.715872</td>\n",
       "      <td>0.696969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>BuzzFeed News BR</td>\n",
       "      <td>278</td>\n",
       "      <td>0.953117</td>\n",
       "      <td>0.956108</td>\n",
       "      <td>0.954793</td>\n",
       "      <td>0.952962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Buy Me That</td>\n",
       "      <td>362</td>\n",
       "      <td>0.812196</td>\n",
       "      <td>0.850460</td>\n",
       "      <td>0.815436</td>\n",
       "      <td>0.806734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>BuzzFeed UK Politics</td>\n",
       "      <td>256</td>\n",
       "      <td>0.832089</td>\n",
       "      <td>0.867561</td>\n",
       "      <td>0.833763</td>\n",
       "      <td>0.825176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BuzzFeed Australia</td>\n",
       "      <td>718</td>\n",
       "      <td>0.671256</td>\n",
       "      <td>0.741238</td>\n",
       "      <td>0.675736</td>\n",
       "      <td>0.645972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BuzzFeed Rewind</td>\n",
       "      <td>220</td>\n",
       "      <td>0.713636</td>\n",
       "      <td>0.790404</td>\n",
       "      <td>0.718209</td>\n",
       "      <td>0.693262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>BuzzFeed Books</td>\n",
       "      <td>210</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>0.756510</td>\n",
       "      <td>0.676607</td>\n",
       "      <td>0.644654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>BuzzFeed DIY</td>\n",
       "      <td>260</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.771248</td>\n",
       "      <td>0.729519</td>\n",
       "      <td>0.712014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>BuzzFeed México</td>\n",
       "      <td>665</td>\n",
       "      <td>0.939098</td>\n",
       "      <td>0.945606</td>\n",
       "      <td>0.940453</td>\n",
       "      <td>0.938549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>BuzzFeed Food</td>\n",
       "      <td>545</td>\n",
       "      <td>0.755046</td>\n",
       "      <td>0.826744</td>\n",
       "      <td>0.757011</td>\n",
       "      <td>0.739955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>BuzzFeed Celeb</td>\n",
       "      <td>480</td>\n",
       "      <td>0.731250</td>\n",
       "      <td>0.771903</td>\n",
       "      <td>0.735383</td>\n",
       "      <td>0.720772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>BuzzFeed Animals</td>\n",
       "      <td>268</td>\n",
       "      <td>0.754193</td>\n",
       "      <td>0.832143</td>\n",
       "      <td>0.758204</td>\n",
       "      <td>0.737531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>BuzzFeed France</td>\n",
       "      <td>546</td>\n",
       "      <td>0.953286</td>\n",
       "      <td>0.957390</td>\n",
       "      <td>0.953889</td>\n",
       "      <td>0.953030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>BuzzFeed Español</td>\n",
       "      <td>386</td>\n",
       "      <td>0.903979</td>\n",
       "      <td>0.918347</td>\n",
       "      <td>0.909183</td>\n",
       "      <td>0.902775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>BuzzFeed Style</td>\n",
       "      <td>409</td>\n",
       "      <td>0.691825</td>\n",
       "      <td>0.787505</td>\n",
       "      <td>0.691948</td>\n",
       "      <td>0.662101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Cheeky</td>\n",
       "      <td>364</td>\n",
       "      <td>0.786834</td>\n",
       "      <td>0.829685</td>\n",
       "      <td>0.790724</td>\n",
       "      <td>0.779918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>BuzzFeed Community</td>\n",
       "      <td>348</td>\n",
       "      <td>0.708592</td>\n",
       "      <td>0.782502</td>\n",
       "      <td>0.713411</td>\n",
       "      <td>0.686713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>BuzzFeed Weddings</td>\n",
       "      <td>112</td>\n",
       "      <td>0.709289</td>\n",
       "      <td>0.801704</td>\n",
       "      <td>0.718271</td>\n",
       "      <td>0.684152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>BuzzFeed World</td>\n",
       "      <td>243</td>\n",
       "      <td>0.837670</td>\n",
       "      <td>0.859731</td>\n",
       "      <td>0.837543</td>\n",
       "      <td>0.832991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>BuzzFeed Entertainment</td>\n",
       "      <td>344</td>\n",
       "      <td>0.753005</td>\n",
       "      <td>0.794858</td>\n",
       "      <td>0.759091</td>\n",
       "      <td>0.743666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>BuzzFeed Geeky</td>\n",
       "      <td>142</td>\n",
       "      <td>0.665394</td>\n",
       "      <td>0.771185</td>\n",
       "      <td>0.673842</td>\n",
       "      <td>0.619698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>BuzzFeed Parents</td>\n",
       "      <td>189</td>\n",
       "      <td>0.687909</td>\n",
       "      <td>0.764333</td>\n",
       "      <td>0.690626</td>\n",
       "      <td>0.663228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>BuzzFeed Health</td>\n",
       "      <td>298</td>\n",
       "      <td>0.716356</td>\n",
       "      <td>0.775281</td>\n",
       "      <td>0.720229</td>\n",
       "      <td>0.697888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>BuzzFeed España</td>\n",
       "      <td>357</td>\n",
       "      <td>0.911561</td>\n",
       "      <td>0.925418</td>\n",
       "      <td>0.911945</td>\n",
       "      <td>0.910005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>BuzzFeed Politics</td>\n",
       "      <td>378</td>\n",
       "      <td>0.850281</td>\n",
       "      <td>0.869870</td>\n",
       "      <td>0.855388</td>\n",
       "      <td>0.847830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Pero Like</td>\n",
       "      <td>45</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.587083</td>\n",
       "      <td>0.561250</td>\n",
       "      <td>0.465446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>BuzzFeed UK News</td>\n",
       "      <td>366</td>\n",
       "      <td>0.811163</td>\n",
       "      <td>0.836446</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.807751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>BuzzFeed India</td>\n",
       "      <td>620</td>\n",
       "      <td>0.749194</td>\n",
       "      <td>0.809372</td>\n",
       "      <td>0.750905</td>\n",
       "      <td>0.735299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>BuzzFeed France News</td>\n",
       "      <td>217</td>\n",
       "      <td>0.926110</td>\n",
       "      <td>0.934053</td>\n",
       "      <td>0.930021</td>\n",
       "      <td>0.924962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>BuzzFeed Scotland</td>\n",
       "      <td>96</td>\n",
       "      <td>0.708684</td>\n",
       "      <td>0.775807</td>\n",
       "      <td>0.708424</td>\n",
       "      <td>0.675920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>LOLA</td>\n",
       "      <td>37</td>\n",
       "      <td>0.839286</td>\n",
       "      <td>0.882738</td>\n",
       "      <td>0.846667</td>\n",
       "      <td>0.828247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>BuzzFeed Philippines</td>\n",
       "      <td>161</td>\n",
       "      <td>0.723674</td>\n",
       "      <td>0.770756</td>\n",
       "      <td>0.728060</td>\n",
       "      <td>0.709173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>BuzzFeed Oz News</td>\n",
       "      <td>179</td>\n",
       "      <td>0.737381</td>\n",
       "      <td>0.783809</td>\n",
       "      <td>0.738767</td>\n",
       "      <td>0.723284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>BuzzFeed Reader</td>\n",
       "      <td>46</td>\n",
       "      <td>0.727778</td>\n",
       "      <td>0.738294</td>\n",
       "      <td>0.760595</td>\n",
       "      <td>0.684737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>BuzzFeed Pink</td>\n",
       "      <td>25</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.835833</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.797302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>BuzzFeed LGBT</td>\n",
       "      <td>114</td>\n",
       "      <td>0.723913</td>\n",
       "      <td>0.770454</td>\n",
       "      <td>0.722161</td>\n",
       "      <td>0.699223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>BuzzFeed Partner</td>\n",
       "      <td>89</td>\n",
       "      <td>0.686601</td>\n",
       "      <td>0.704788</td>\n",
       "      <td>0.693927</td>\n",
       "      <td>0.658857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>BuzzFeed Tech</td>\n",
       "      <td>211</td>\n",
       "      <td>0.824308</td>\n",
       "      <td>0.845697</td>\n",
       "      <td>0.840815</td>\n",
       "      <td>0.821566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>BuzzFeed Science</td>\n",
       "      <td>73</td>\n",
       "      <td>0.705714</td>\n",
       "      <td>0.735265</td>\n",
       "      <td>0.721250</td>\n",
       "      <td>0.694241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      page  num_links  accuracy  precision    recall    fscore\n",
       "0            BuzzFeed Quiz       1316  0.881848   0.902321  0.883010  0.880307\n",
       "1           BuzzFeed Japan        966  0.783134   0.865834  0.790795  0.751410\n",
       "2              BuzzFeed UK        560  0.683036   0.767227  0.683766  0.654710\n",
       "3          BuzzFeed Brasil       1022  0.943221   0.948860  0.944271  0.942958\n",
       "4      BuzzFeed Japan News        616  0.721800   0.833585  0.737795  0.679828\n",
       "5     BuzzFeed Deutschland        581  0.973320   0.972841  0.974330  0.973130\n",
       "6            BuzzFeed News        649  0.818241   0.834573  0.820735  0.815903\n",
       "7                 BuzzFeed        978  0.638532   0.716346  0.640664  0.603391\n",
       "8          BuzzFeed Canada        251  0.655647   0.733618  0.664959  0.626808\n",
       "9           BuzzFeed Video        121  0.702667   0.779103  0.706388  0.681518\n",
       "10            Cocoa Butter        177  0.706270   0.778934  0.709497  0.685850\n",
       "11    BuzzFeed Oz Politics        212  0.815836   0.852237  0.816386  0.808776\n",
       "12      Quizzes En Español        195  0.920513   0.931601  0.922675  0.919217\n",
       "13    Obsessed by BuzzFeed        292  0.715663   0.772626  0.715872  0.696969\n",
       "14        BuzzFeed News BR        278  0.953117   0.956108  0.954793  0.952962\n",
       "15             Buy Me That        362  0.812196   0.850460  0.815436  0.806734\n",
       "16    BuzzFeed UK Politics        256  0.832089   0.867561  0.833763  0.825176\n",
       "17      BuzzFeed Australia        718  0.671256   0.741238  0.675736  0.645972\n",
       "18         BuzzFeed Rewind        220  0.713636   0.790404  0.718209  0.693262\n",
       "19          BuzzFeed Books        210  0.671429   0.756510  0.676607  0.644654\n",
       "20            BuzzFeed DIY        260  0.725000   0.771248  0.729519  0.712014\n",
       "21         BuzzFeed México        665  0.939098   0.945606  0.940453  0.938549\n",
       "22           BuzzFeed Food        545  0.755046   0.826744  0.757011  0.739955\n",
       "23          BuzzFeed Celeb        480  0.731250   0.771903  0.735383  0.720772\n",
       "24        BuzzFeed Animals        268  0.754193   0.832143  0.758204  0.737531\n",
       "25         BuzzFeed France        546  0.953286   0.957390  0.953889  0.953030\n",
       "26        BuzzFeed Español        386  0.903979   0.918347  0.909183  0.902775\n",
       "27          BuzzFeed Style        409  0.691825   0.787505  0.691948  0.662101\n",
       "28                  Cheeky        364  0.786834   0.829685  0.790724  0.779918\n",
       "29      BuzzFeed Community        348  0.708592   0.782502  0.713411  0.686713\n",
       "30       BuzzFeed Weddings        112  0.709289   0.801704  0.718271  0.684152\n",
       "31          BuzzFeed World        243  0.837670   0.859731  0.837543  0.832991\n",
       "32  BuzzFeed Entertainment        344  0.753005   0.794858  0.759091  0.743666\n",
       "33          BuzzFeed Geeky        142  0.665394   0.771185  0.673842  0.619698\n",
       "34        BuzzFeed Parents        189  0.687909   0.764333  0.690626  0.663228\n",
       "35         BuzzFeed Health        298  0.716356   0.775281  0.720229  0.697888\n",
       "36         BuzzFeed España        357  0.911561   0.925418  0.911945  0.910005\n",
       "37       BuzzFeed Politics        378  0.850281   0.869870  0.855388  0.847830\n",
       "38               Pero Like         45  0.533333   0.587083  0.561250  0.465446\n",
       "39        BuzzFeed UK News        366  0.811163   0.836446  0.816218  0.807751\n",
       "40          BuzzFeed India        620  0.749194   0.809372  0.750905  0.735299\n",
       "41    BuzzFeed France News        217  0.926110   0.934053  0.930021  0.924962\n",
       "42       BuzzFeed Scotland         96  0.708684   0.775807  0.708424  0.675920\n",
       "43                    LOLA         37  0.839286   0.882738  0.846667  0.828247\n",
       "44    BuzzFeed Philippines        161  0.723674   0.770756  0.728060  0.709173\n",
       "45        BuzzFeed Oz News        179  0.737381   0.783809  0.738767  0.723284\n",
       "46         BuzzFeed Reader         46  0.727778   0.738294  0.760595  0.684737\n",
       "47           BuzzFeed Pink         25  0.840000   0.835833  0.833333  0.797302\n",
       "48           BuzzFeed LGBT        114  0.723913   0.770454  0.722161  0.699223\n",
       "49        BuzzFeed Partner         89  0.686601   0.704788  0.693927  0.658857\n",
       "50           BuzzFeed Tech        211  0.824308   0.845697  0.840815  0.821566\n",
       "51        BuzzFeed Science         73  0.705714   0.735265  0.721250  0.694241"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BuzzFeed Quiz 0.822006133087\n",
      "BuzzFeed Japan 0.0504418176062\n",
      "BuzzFeed UK 0.649453967268\n",
      "BuzzFeed Brasil 0.0237510229628\n",
      "BuzzFeed Japan News 0.0377321096068\n",
      "BuzzFeed Deutschland 0.0372165433724\n",
      "BuzzFeed News 0.415163135694\n",
      "BuzzFeed 0.623760096086\n",
      "BuzzFeed Canada 0.597919688812\n",
      "BuzzFeed Video 0.469984381006\n",
      "Cocoa Butter 0.443208450578\n",
      "BuzzFeed Oz Politics 0.309079002749\n",
      "Quizzes En Español 0.193981406629\n",
      "Obsessed by BuzzFeed 0.480483461722\n",
      "BuzzFeed News BR 0.072696859011\n",
      "Buy Me That 0.38192244223\n",
      "BuzzFeed UK Politics 0.376333492243\n",
      "BuzzFeed Australia 0.614498944707\n",
      "BuzzFeed Rewind 0.412539128523\n",
      "Woops, the model for page  BuzzFeed Ladylike  is still in the works!\n",
      "BuzzFeed Books 0.618130931117\n",
      "BuzzFeed DIY 0.526890877113\n",
      "BuzzFeed México 0.0291228070385\n",
      "BuzzFeed Food 0.592997062626\n",
      "BuzzFeed Celeb 0.674910157553\n",
      "BuzzFeed Animals 0.693397325216\n",
      "BuzzFeed France 0.0359120191369\n",
      "BuzzFeed Español 0.0696082033988\n",
      "BuzzFeed Style 0.63979040659\n",
      "Woops, the model for page  Tasty  is still in the works!\n",
      "Cheeky 0.455828417832\n",
      "BuzzFeed Community 0.505030576443\n",
      "BuzzFeed Weddings 0.55718023009\n",
      "Woops, the model for page  Tasty Demais  is still in the works!\n",
      "BuzzFeed World 0.348429496688\n",
      "Woops, the model for page  SOML  is still in the works!\n",
      "BuzzFeed Entertainment 0.424723545287\n",
      "BuzzFeed Geeky 0.540336033629\n",
      "BuzzFeed Parents 0.630569798087\n",
      "BuzzFeed Health 0.705367073709\n",
      "BuzzFeed España 0.111566093203\n",
      "BuzzFeed Politics 0.273441463939\n",
      "Pero Like 0.638245097043\n",
      "BuzzFeed UK News 0.408457532364\n",
      "BuzzFeed India 0.379905099204\n",
      "BuzzFeed France News 0.074070794161\n",
      "BuzzFeed Scotland 0.535412496572\n",
      "LOLA 0.324031949171\n",
      "BuzzFeed Philippines 0.552603637518\n",
      "BuzzFeed Oz News 0.497464961678\n",
      "Woops, the model for page  BuzzFeed Unsolved  is still in the works!\n",
      "Woops, the model for page  BuzzFeed IRL  is still in the works!\n",
      "Woops, the model for page  Bien Tasty  is still in the works!\n",
      "BuzzFeed Reader 0.542095724053\n",
      "BuzzFeed Pink 0.362572831581\n",
      "BuzzFeed LGBT 0.667659852906\n",
      "BuzzFeed Partner 0.690884728965\n",
      "Woops, the model for page  So Relatable  is still in the works!\n",
      "Woops, the model for page  BuzzFeed Open Lab  is still in the works!\n",
      "BuzzFeed Tech 0.542710420609\n",
      "BuzzFeed Science 0.529749977013\n",
      "Woops, the model for page  Reasons to Smile  is still in the works!\n",
      "Woops, the model for page  Tasty Miam  is still in the works!\n",
      "Woops, the model for page  Proper Tasty  is still in the works!\n",
      "Woops, the model for page  BuzzFeed San Francisco  is still in the works!\n",
      "Woops, the model for page  BuzzFeed Shane  is still in the works!\n",
      "Woops, the model for page  BuzzFeed Ryan  is still in the works!\n",
      "Woops, the model for page  Nifty  is still in the works!\n",
      "Woops, the model for page  See Something Say Something  is still in the works!\n",
      "Woops, the model for page  Tasty Japan  is still in the works!\n",
      "Woops, the model for page  BuzzFeed Zack  is still in the works!\n",
      "Woops, the model for page  Kristin Chirico  is still in the works!\n",
      "Woops, the model for page  BuzzFeed Austin  is still in the works!\n",
      "Woops, the model for page  Adam Ellis  is still in the works!\n",
      "Woops, the model for page  Another Round  is still in the works!\n",
      "Woops, the model for page  Top Knot  is still in the works!\n",
      "Woops, the model for page  Buzzed Feed  is still in the works!\n",
      "Woops, the model for page  BuzzFeed BFF  is still in the works!\n",
      "Woops, the model for page  Tasty One-Pot  is still in the works!\n",
      "Woops, the model for page  BuzzFeed Sweaty  is still in the works!\n",
      "Woops, the model for page  Tasty Junior  is still in the works!\n",
      "Woops, the model for page  Oh Great, More Politics  is still in the works!\n",
      "Woops, the model for page  Einfach Tasty  is still in the works!\n",
      "Woops, the model for page  BuzzFeed Steven L.  is still in the works!\n",
      "Woops, the model for page  The Try Guys  is still in the works!\n",
      "Woops, the model for page  Ohmygod Yaaa  is still in the works!\n",
      "Woops, the model for page  BuzzFeed Chloe  is still in the works!\n",
      "Woops, the model for page  Nifty Science  is still in the works!\n"
     ]
    }
   ],
   "source": [
    "#####Query by page and title of link - Should a link with this title be posted on this BF page?\n",
    "# q_page = \"LOLA\"\n",
    "q_page = \"BuzzFeed Quiz\"\n",
    "# q_page = \"BuzzFeed Japan\"\n",
    "q_test = [\"Find Out What your spirit animal is?\"]  ###list of titles you wanna query\n",
    "\n",
    "max_prob = 0\n",
    "max_page = \"\"\n",
    "\n",
    "for pg in all_pages:\n",
    "    try:\n",
    "        clf = page_clf[pages.index(pg)]\n",
    "    except:\n",
    "        print(\"Woops, the model for page \", pg, \" is still in the works!\")\n",
    "        continue\n",
    "        \n",
    "    prob = clf.predict_proba(q_test)[0][1]\n",
    "    print(pg, prob)\n",
    "    if prob > max_prob:\n",
    "        max_prob = prob\n",
    "        max_page = page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Nifty Science', 0.82200613308712345)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_page, max_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
